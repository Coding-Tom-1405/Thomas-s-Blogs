[
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "",
    "text": "In this blog post, we will go how to create more tensorflow models. However, unlike the last blog post where we utilized a dataframe of images to do image classification, we are going to use a dataframe of articles and performing text classification to determine if an article would be considered “fake news”."
  },
  {
    "objectID": "posts/HW6/index.html#acquire-training-data",
    "href": "posts/HW6/index.html#acquire-training-data",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Acquire Training Data",
    "text": "Acquire Training Data\nFor this task we are going to use something called stopwords. These are widely used words such as “the”, “a”, “an”, etc. that we want to configure our search engine to disregard while indexing. Thus, we can do that by the following:\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nNow let’s import the remaining libraries along with the data.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow import keras\nfrom keras import utils\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\n\n# Embedding visualizations\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Importing data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\nimported_data = pd.read_csv(train_url)\n\n# Importing English stopwords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nWith our data imported and read in, let’s see what we are dealing with.\n\nimported_data\n\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n22444\n10709\nALARMING: NSA Refuses to Release Clinton-Lynch...\nIf Clinton and Lynch just talked about grandki...\n1\n\n\n22445\n8731\nCan Pence's vow not to sling mud survive a Tru...\n() - In 1990, during a close and bitter congre...\n0\n\n\n22446\n4733\nWatch Trump Campaign Try To Spin Their Way Ou...\nA new ad by the Hillary Clinton SuperPac Prior...\n1\n\n\n22447\n3993\nTrump celebrates first 100 days as president, ...\nHARRISBURG, Pa.U.S. President Donald Trump hit...\n0\n\n\n22448\n12896\nTRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...\nMELBOURNE, FL is a town with a population of 7...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nFor this dataset, we can see that each row represents an article. Within each row, we are given the title and the full text of each article as well as a boolean value called fake that denotes whether the article contains fake news, as determined by the authors of the paper above."
  },
  {
    "objectID": "posts/HW6/index.html#make-a-dataset",
    "href": "posts/HW6/index.html#make-a-dataset",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Make a Dataset",
    "text": "Make a Dataset\nOnce we have imported everything we needed, we can move on to creating the dataset, which we will accomplish by creating a function that takes in the dataframe as the parameter. In order to do that, there are a three things we want our function to execute: 1) making all the texts lowercase to prevent misinterpretation, 2) removing all the stopwords to improve efficiency, 3) constructing and returning a tf.data.Dataset that has two inputs (title, text) and one output fake. Thus, we can accomplish all of these with the following code:\n\ndef make_dataset(df):\n    \"\"\"\n    Creating a prepped tensorflow dataset from a dataframe\n\n    Input:\n      df = dataframe\n    Return:\n      tensorflow dataset with two inputs and one outputs\n    \"\"\"\n    # Removing stopwords\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    # Creating dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n      {\"title\" : df[[\"title\"]], \"text\" : df[[\"text\"]]},\n      {\"fake\" : df[\"fake\"]}\n      ))\n\n    # Batching dataset (improve efficiency)\n    ds = ds.batch(100)\n    return ds\n\nWith this function, let’s create our dataset using our imported dataframe. In addition, because the dataframe is in a specific order, let’s randomize the order before we split it into training and validation sets.\n\ndataset = make_dataset(imported_data)\ndataset = dataset.shuffle(buffer_size = len(dataset)) # Shuffling order of dataset\n\ntrain_size = int(0.8*len(dataset)) # Indicating size: 80% for training\nval_size   = int(0.2*len(dataset)) # Indicating size: 20% for validation\n\ntrain = dataset.take(train_size) # Training set\nval = dataset.skip(train_size).take(val_size) # Validation Set\n\nBefore we move on to our model components, let’s establish what our initial comparing condition is.\n\nimported_data['fake'].value_counts() # display counts of fake/real news\n\n1    11740\n0    10709\nName: fake, dtype: int64\n\n\nAs we mentioned in the last blog post, the base rate refers to the accuracy of a baseline model that always predicts the most occurring output. Thus, our baseline model would always predict 1, or fake news, making our base rate 11740/(11740+10709)%, or 52.3%.\nNext, let’s also implement text vectorization for our models later.\n\n# Preprating a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\ntext_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))"
  },
  {
    "objectID": "posts/HW6/index.html#create-models",
    "href": "posts/HW6/index.html#create-models",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Create Models",
    "text": "Create Models\nAs mentioned above, the purpose of these models is to use the inputs title and text within the imported data and classify whether each article is considered either real or fake news, outputting a fake boolean of 0 or 1.\nSo first, we want to specify our model’s input shapes.\n\n# Title input shape\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\",\n    dtype = \"string\"\n)\n\n#Text input shape\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\",\n    dtype = \"string\"\n)"
  },
  {
    "objectID": "posts/HW6/index.html#first-model",
    "href": "posts/HW6/index.html#first-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "First Model",
    "text": "First Model\nNow, we can start constructing the tensorflow models.\nFor our first model, we will use only the article title as an input.\nWe will begin by constructing the layers. For all of these models, we will be incorporating what’s called an embedding layer, which allows us to convert input info into a dense vector. In addition, because we aredealing with strings, we will want to incorporate a TextVectorization layer in order to map/transform text features into integer sequences. Thus, our code should look something like this\n\n# TextVectorization layer to input_title\ntitle_features = title_vectorize_layer(title_input)\n\n# Embeddings\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Binary classifications\ntitle_features = layers.Dense(2, activation='relu', name=\"fake\")(title_features)\n\n\n# only using title\nmodel1 = keras.Model(\n    inputs = [title_input],\n    outputs = title_features\n)\n\nLet’s visualize what our model looks like.\n\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 10ms/step - accuracy: 0.5083 - loss: 0.6927 - val_accuracy: 0.4991 - val_loss: 0.6931\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5156 - loss: 0.6920 - val_accuracy: 0.5200 - val_loss: 0.6861\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5633 - loss: 0.6845 - val_accuracy: 0.5287 - val_loss: 0.6765\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6161 - loss: 0.6768 - val_accuracy: 0.5356 - val_loss: 0.6657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6592 - loss: 0.6653 - val_accuracy: 0.6278 - val_loss: 0.6515\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.7231 - loss: 0.6502 - val_accuracy: 0.8740 - val_loss: 0.6368\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.7261 - loss: 0.6352 - val_accuracy: 0.8244 - val_loss: 0.6180\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - accuracy: 0.7695 - loss: 0.6139 - val_accuracy: 0.8791 - val_loss: 0.5950\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.7824 - loss: 0.6004 - val_accuracy: 0.8829 - val_loss: 0.5719\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8152 - loss: 0.5767 - val_accuracy: 0.8789 - val_loss: 0.5545\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8255 - loss: 0.5575 - val_accuracy: 0.8964 - val_loss: 0.5303\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8436 - loss: 0.5371 - val_accuracy: 0.8962 - val_loss: 0.5128\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8425 - loss: 0.5216 - val_accuracy: 0.8860 - val_loss: 0.4936\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.8469 - loss: 0.5032 - val_accuracy: 0.8873 - val_loss: 0.4769\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8682 - loss: 0.4827 - val_accuracy: 0.9018 - val_loss: 0.4556\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8638 - loss: 0.4675 - val_accuracy: 0.8582 - val_loss: 0.4510\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8583 - loss: 0.4544 - val_accuracy: 0.9124 - val_loss: 0.4233\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8596 - loss: 0.4434 - val_accuracy: 0.9058 - val_loss: 0.4086\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8759 - loss: 0.4213 - val_accuracy: 0.9128 - val_loss: 0.3888\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8823 - loss: 0.4100 - val_accuracy: 0.9247 - val_loss: 0.3757\n\n\n\n# we visualize our training history\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy of model1 started at 49.9%% but hit peak of 92.5%, better than the baseline model by 40.2%."
  },
  {
    "objectID": "posts/HW6/index.html#second-model",
    "href": "posts/HW6/index.html#second-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Second Model",
    "text": "Second Model\nFor our second model, we will use only the article text as an input this time around.\nThe structure of our model will look the exact same as model1, except this time, for out text_vectorize_layer input parameter, we are going to use text_input rather than title_input. Thus our code will look like so:\n\n# We begin construct our model's layers\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(2, activation='relu', name=\"fake\")(text_features)\n\nmodel2 = keras.Model(\n    # only using text\n    inputs = [text_input],\n    outputs = text_features\n)\n\nOnce again, let’s visualize and train our model.\n\n# we visualize our model\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\nhistory2 = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.5218 - loss: 0.6885 - val_accuracy: 0.5418 - val_loss: 0.6711\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.5843 - loss: 0.6687 - val_accuracy: 0.6849 - val_loss: 0.6439\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.6715 - loss: 0.6363 - val_accuracy: 0.8587 - val_loss: 0.5951\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.7953 - loss: 0.5863 - val_accuracy: 0.9193 - val_loss: 0.5277\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8478 - loss: 0.5275 - val_accuracy: 0.9253 - val_loss: 0.4646\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 17ms/step - accuracy: 0.8666 - loss: 0.4667 - val_accuracy: 0.9398 - val_loss: 0.4078\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.8877 - loss: 0.4186 - val_accuracy: 0.9302 - val_loss: 0.3664\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8967 - loss: 0.3782 - val_accuracy: 0.9461 - val_loss: 0.3276\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9098 - loss: 0.3468 - val_accuracy: 0.9480 - val_loss: 0.3012\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9219 - loss: 0.3175 - val_accuracy: 0.9269 - val_loss: 0.2823\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9278 - loss: 0.2921 - val_accuracy: 0.9522 - val_loss: 0.2574\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9315 - loss: 0.2717 - val_accuracy: 0.9576 - val_loss: 0.2328\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9386 - loss: 0.2576 - val_accuracy: 0.9616 - val_loss: 0.2175\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9387 - loss: 0.2415 - val_accuracy: 0.9577 - val_loss: 0.2089\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9412 - loss: 0.2297 - val_accuracy: 0.9500 - val_loss: 0.2087\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9473 - loss: 0.2211 - val_accuracy: 0.9536 - val_loss: 0.1980\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9483 - loss: 0.2103 - val_accuracy: 0.9593 - val_loss: 0.1839\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9545 - loss: 0.1974 - val_accuracy: 0.9664 - val_loss: 0.1704\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9555 - loss: 0.1873 - val_accuracy: 0.9420 - val_loss: 0.1790\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9511 - loss: 0.1885 - val_accuracy: 0.9669 - val_loss: 0.1576\n\n\n\n# we visualize our training history\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy of model2 started at 54.2% but hit peak of 96.6%, which was better than the baseline model by 44.3%. In addition, model2 stabalized around 91% to 96% by the 4th epoch."
  },
  {
    "objectID": "posts/HW6/index.html#third-model",
    "href": "posts/HW6/index.html#third-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Third Model",
    "text": "Third Model\nFor our final model, we will use both the article title and the article text as input.\nFor this model, because we have two components, we are going to want to perform vectorization and embedding on them individually and then concatenating the output of the article title pipeline with the output of the article text pipeline. Once concatenation, we can perform use the same Dropout,FlobalAveragePooling1D, and Dense layers as the previous models, making our code look like so:\n\n# Vectorization\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\n# Embedding\ntitle_embedding = layers.Embedding(size_vocabulary, 10)\ntext_embedding = layers.Embedding(size_vocabulary, 10)\ntitle_features = title_embedding(title_features)\ntext_features = text_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# main layers\nmain = layers.concatenate([title_features, text_features], axis = 1)\nmain = layers.Dropout(0.2)(main)\nmain = layers.GlobalAveragePooling1D()(main)\nmain = layers.Dropout(0.2)(main)\nmain = layers.Dense(2, activation='relu', name = 'fake')(main)\n\nFor the last time, let’s visualize and train:\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = main\n)\n\n\n# we visualize our model\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory3 = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9344 - loss: 0.2223 - val_accuracy: 0.9618 - val_loss: 0.1554\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9498 - loss: 0.1773 - val_accuracy: 0.9736 - val_loss: 0.1209\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9571 - loss: 0.1462 - val_accuracy: 0.9778 - val_loss: 0.1043\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9675 - loss: 0.1208 - val_accuracy: 0.9789 - val_loss: 0.0886\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.9712 - loss: 0.1055 - val_accuracy: 0.9873 - val_loss: 0.0623\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9762 - loss: 0.0910 - val_accuracy: 0.9881 - val_loss: 0.0552\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9782 - loss: 0.0767 - val_accuracy: 0.9903 - val_loss: 0.0462\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9831 - loss: 0.0634 - val_accuracy: 0.9920 - val_loss: 0.0414\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9859 - loss: 0.0560 - val_accuracy: 0.9929 - val_loss: 0.0350\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9872 - loss: 0.0497 - val_accuracy: 0.9924 - val_loss: 0.0355\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9861 - loss: 0.0464 - val_accuracy: 0.9933 - val_loss: 0.0247\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9875 - loss: 0.0432 - val_accuracy: 0.9940 - val_loss: 0.0259\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9910 - loss: 0.0351 - val_accuracy: 0.9958 - val_loss: 0.0202\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9913 - loss: 0.0326 - val_accuracy: 0.9953 - val_loss: 0.0207\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9908 - loss: 0.0308 - val_accuracy: 0.9964 - val_loss: 0.0190\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9929 - loss: 0.0308 - val_accuracy: 0.9964 - val_loss: 0.0162\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9935 - loss: 0.0257 - val_accuracy: 0.9976 - val_loss: 0.0150\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 17ms/step - accuracy: 0.9931 - loss: 0.0268 - val_accuracy: 0.9960 - val_loss: 0.0147\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9948 - loss: 0.0215 - val_accuracy: 0.9973 - val_loss: 0.0138\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9936 - loss: 0.0225 - val_accuracy: 0.9967 - val_loss: 0.0136\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nUnlike the previous model which started off low and then climbed into the 90s value, the accuracy of model3 stabilized between 96.2% and 99.7% during training right from the first epoch, meaning it was better than the baseline model by 43.5% to 47.3% for all epochs."
  },
  {
    "objectID": "posts/HW6/index.html#model-evaluation",
    "href": "posts/HW6/index.html#model-evaluation",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nOut of all three of our models, it is obvious that model3 using both article title and text was the best performing. Thus, we will now use model3 to see how well it performs on our unseen test data. Similar to before, we will use make_dataset() to import and read in the test data, creating a new dataset, which we will then evaluate our model with.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\nimported_test_data = pd.read_csv(test_url)\ntest = make_dataset(imported_test_data)\n\nmodel3.evaluate(test, verbose=1)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9946 - loss: 0.0186\n\n\n[0.017387792468070984, 0.9952336549758911]\n\n\nGreat! model3 was able to get 99.4% accuracy on the new test dataset, meaning eat would be correct at detecting fake news above 99% of the time."
  },
  {
    "objectID": "posts/HW6/index.html#embedding-visualization",
    "href": "posts/HW6/index.html#embedding-visualization",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Embedding Visualization",
    "text": "Embedding Visualization\nBecause we used embedding layers in our models, we can visualize the embedding our model learned to see if there are any patterns in the words that our model found useful when distinguising news. To do this we will use a 2d embedding plot and then principal component analysis (PCA) to reduce dimensionality.\n\nweights = model3.get_layer('embedding_1').get_weights()[0] # get weights from embedding layer\nvocab = title_vectorize_layer.get_vocabulary() # get vocabulary from data prep\n\npca = PCA(n_components=2) # Initializing PCA\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = [2]*len(embedding_df),\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nAs you can see, “funds”, “concerns”, “financial”, and “debt” are clustered close by, which make sense in the topic of economics. We also we words like “socialism” and “corruption” next to each other on the right side of the scatterplot, which could be highlighting more right-leaning articles."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Homework 4: Numpy & Matrix-Vectors (Heat Diffusion)",
    "section": "",
    "text": "In this blog post, we will learn how to use numpy and other tools to work with linear algebra in Python as well as increase computational speed. For our example, we will conduct a simulation of two-dimensional heat diffusion.\n\nMatrix Multiplication\nFor this example, we will need the following block of code.\n\nN = 101\nepsilon = 0.2\n\nHere, N is representative of the number of x,y grid values while epsilon is representative of how “heavy” we want each update to be.\nFirst, we want to create our initial condition: an empty grid with one unit of heat at the midpoint called u0. To do that, we will need the following block of code:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\nimport jax\n\nModuleNotFoundError: No module named 'jax'\n\n\nTesting"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Homework 2: Web Scraping (Movie Recs)",
    "section": "",
    "text": "In this blog, we will learn how to extract data from HTML sites using webscraping. For our example, we will use movie sites.\n\nCreating Scrapy Project\nWe will first start by running these two commands in the terminal:\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nRunning these lines will allow us to create a folder labeled “TBDB_scraper” which we will then for this webscraping tutorial.\nInside the inner “TMBD_scraper” will exists a python file called settings.py which we will want to add these two lines:\nUSER_AGENT = \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"\nDOWNLOAD_DELAY = 2\nAdding this line will be crucial for the 403 Forbidden errors, which commonly arises since these websites have detectors that see that we are scraping the website, so they want to block us from this action.\nThe first line USER_AGENT helps implement a fake user agent for all the requests that we will send, which makes it harder for the website to know if the requests are from a scraper (which it will want to block) or an actual user.\nThe second line DOWNLOAD_DELAY helps implement a randomization of request delays, which spaces out the requests over longer, patternless intervals to make the website flag us less frequently for scraping.\n\n\nImplementing Scraping Methods\nNow that we have our scrapy project, we want to create a tmdb_spider.py file in the spiders folder.\nWe first start by importing scrapy, which is the webscraping library which will allow us to do all of this.\n\nimport scrapy\n\nNext, we need to create the spider class TmbdSpider which contains its name (tmbd_spider) that we will need to perform the webscraping along with the url for the movie from the website we are working with and its three parse methods. For this example, we will use The Dark Knight with the data from The Movie Dtabase. So, our code will look something like this:\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir = None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nAnd for the three parsing methods:\ndef parse(self, response):\n      \"\"\"\n      This method starts with the start_url which is a movie page on the website,\n      and proceeds by sending a request to the cast page of the movie from the start_url.\n      This has no outputs but sets up the request to be used in the parse_full_credits method.\n      \"\"\"\n        page = response.url + \"/cast\" # Adding on /cast to get the cast page of movie's url\n        yield scrapy.Request(page, callback = self.parse_full_credits)\nThe parse method starts with the start_url and then sends a request for the movie’s cast page from the start_url, which will be used in the next defined method, parse_full_credits\ndef parse_full_credits(self, response):\n      \"\"\"\n      This method starts with the cast page of a movie given through the start_url \n      from parse function. It then iterates through all the actors's, actresses's links \n      on this page and sends a request for each actor's, actress's page. This\n      produces no outputs but sets up the requests to be used in the parse_actor_page method.\n      \"\"\"\n        for entry in response.css(\"ol\")[0].css(\"li\"):\n            actor_tag = entry.css(\"a::attr(href)\").get() # Gets unique tag for each actor, actress\n            actor_link = \"https://www.themoviedb.org\" + actor_tag # Adding on tag to main website \n                                                                  # address to get the link to\n                                                                  # each actor's, actress's page  \n            yield scrapy.Request(actor_link, callback = self.parse_actor_page)\n                                           \nSimilar to the last method, this method starts with the cast page of the desired movie and then iterates through all the actor’s, actress’s links on this page and sends a request for each actor’s actress’s page, which will be used in the last defined method, parse_actor_page.\ndef parse_actor_page(self, response):\n        \"\"\"\n        This method starts with the actor/actress's page given by the \n        parse_full_credits function. It then iterates through all the movies that the \n        actor/actress has made an appearance. For each movie, the output will be\n        a dictionary where the key is thename of the actor, actress, and the value\n        is the name of the movie.\n        \"\"\" \n        actor_name = response.css(\"h2\").css(\"::text\").get() # Gets the name of actor,actress \n        for entry in response.css(\"div.credits_list bdi::text\"):\n            movie_or_TV_name = entry.get() # Iterates through each movie appearance\n            yield {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name} # Returns dictionary\nThis method starts with the page of a certain actor, actress and iterates through all movies the this actor, actress has made an appearance and for each one movie we will return a dictionary with the their name and the movie’s name.\nGreat, now we can use our defined spider! So in terminal, navigate to the TMDB_scraper folder by running\nscrapy crawl tmdb_spider -o results.csv\nwhich will return us a csv file with the completed list of all the movies that all the actors, actresses in our chosen movie has made an appearance in.\n\n\nCreating Movie Recommender\nNow that we have our completed data set, we will want to create an algorith to help us with movie recommendations.\nWe first start off by reading in our data, so we need to import the pandas and numpy libraries.\n\nimport pandas as pd\nimport numpy as np\nresults = pd.read_csv(\"results.csv\")\nresults\n\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nHeath Ledger\nHeath Ledger: A Tragic Tale\n\n\n1\nHeath Ledger\nJoker: Put on a Happy Face\n\n\n2\nHeath Ledger\nI Am Heath Ledger\n\n\n3\nHeath Ledger\nThe Fire Rises: The Creation and Impact of The...\n\n\n4\nHeath Ledger\nToo Young to Die\n\n\n...\n...\n...\n\n\n4144\nMichael Caine\nThe Double\n\n\n4145\nMichael Caine\nBlue Ice\n\n\n4146\nMichael Caine\nThe Fourth Protocol\n\n\n4147\nMichael Caine\nPulp\n\n\n4148\nMichael Caine\nGet Carter\n\n\n\n\n4149 rows × 2 columns\n\n\n\n\nAs verification, we want to make sure that the number of actors and actresses listed in our data equals the size of the movie cast. So, we would run the code below:\n\nlen(results[\"actor\"].unique())\n\n136\n\n\nNow, we want to create a 2-dimensional list with one column consists of all the movie names the actors or actresses have appeared in while the other column consists of the number of actor and actresses from The Dark Knight that was also in a certain movie.\n\nmovies_or_TV_shows = results[\"movie_or_TV_name\"].unique() # Creating df containing all \n                                                          # movie names with 0 repeats\n\nrows, cols = (len(movies_or_TV_shows), 2) \nrec_list = [[0 for i in range(cols)] for j in range(rows)] # Creating 2D lists, each unique movie \n                                                           # gets in own row\n\nindex = 0\nfor movie_or_TV_show in movies_or_TV_shows:\n    \"\"\"\n    Iterating through each movie, creating a dataframe of all actors and actreses from \n    The Dark Knight movie that also appeared in the specific movie, and then counting\n    the number of actors and actresses\n    \"\"\"\n    \n    panda = results[results[\"movie_or_TV_name\"] == movie_or_TV_show]\n    num_shared_actors = len(panda)\n    \n    rec_list[index][0] = movie_or_TV_show   # Movie name list\n    rec_list[index][1] = num_shared_actors  # Counting list\n    \n    index += 1 # Updating list\n\nWith this list, we now need to sort it, which we can do using the sorted() function and lambda functions. Because we want the top movie recommendations first, we will use the reverse=True statement. Lastly, we want to convert this to a pandas dataframe to use later.\n\nrec_list_sorted = sorted(rec_list,key=lambda l:l[1], reverse=True)\nrec_list_sorted = pd.DataFrame(rec_list_sorted)\nrec_list_panda = rec_list_sorted.rename(columns = {0: \"Movie\", \n                                                   1: \"No. Actors Appearing in Dark Knight\"})\nrec_list_panda\n\n\n\n\n\n\n\n\n\nMovie\nNo. Actors Appearing in Dark Knight\n\n\n\n\n0\nThe Dark Knight\n138\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n...\n...\n...\n\n\n3383\nTony Awards\n1\n\n\n3384\nNavy Log\n1\n\n\n3385\nMark Saber\n1\n\n\n3386\nWhat's My Line?\n1\n\n\n3387\nMorning Departure\n1\n\n\n\n\n3388 rows × 2 columns\n\n\n\n\nNote that for the first entry, “The Dark Knight” as a count of 138 when we were expecting to only get 136. This is because two actors, Heath Ledger and Tom McComas, are actually listed twice on their cast page, which accounts for the increase of two counts.\nGreat! Now we have our ordered list! Ignoring “The Dark Knight” with the highest count (which is a given), we can see that the next highest is “The Fire Rises: The Creation and Impact of the Dark Knight Trilogy,” a documentary about “The Dark Knight” trilogy (which also makes sense), so we shold ignore that one as well. The third highest is “The Dark Knight Rises,” followed by “CSI: Crime Scene Investigation” and then “Doctor Who.” These are our top three recommendations\n\n\nCreating Data Visualization\nTo end, let’s create a data visualization showing a bar chart of number of shared actors for the movies.\nBecause there over 3,000 movies, let’s restrict the amount we want to include by imposing a minimum count. For this example, we will make it 6. Also, because we only want recommendations, we will exclude The Dark Knight in the visualization as well.\nFirst we want to create our considered movie rec dataframe.\n\nimport matplotlib.pyplot as plt \nbar_data = pd.DataFrame(rec_list_sorted)             # Creating dataframe of 2D sorted list\nbar_data = bar_data[bar_data[1] &lt; max(bar_data[1])]  # Removing The Dark Knight entry\nbar_data = bar_data[bar_data[1] &gt;= 6]                # Restricting minimum score to be 6\nbar_data\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n5\nGotham Tonight\n8\n\n\n6\nPrison Break\n7\n\n\n7\nWaking the Dead\n7\n\n\n8\nHeath Ledger: A Tribute\n6\n\n\n9\nE! True Hollywood Story\n6\n\n\n10\nEnding the Knight\n6\n\n\n11\nBatman Begins\n6\n\n\n12\nCasualty\n6\n\n\n13\nThe View\n6\n\n\n14\nLIVE with Kelly and Mark\n6\n\n\n\n\n\n\n\n\nFinally, we can create our bar chart figure. Because we have titles as our independent variable, which can be long, we want to make a horizontal bar chart instead of a vertical one.\n\nfig = plt.figure(figsize = (10, 5))\n \n# creating the bar plot\nplt.barh(bar_data[0], bar_data[1], color ='Blue')\n \nplt.xlabel(\"Movie\")\nplt.ylabel(\"No. of Actor/Actress Appearances Who Were Also in The Dark Knight\")\nplt.title(\"Bar Plot of the Top Recs for The Dark Knight\")\nplt.show()"
  },
  {
    "objectID": "posts/Group Project/index.html",
    "href": "posts/Group Project/index.html",
    "title": "Group Project: GUI Pygame and Web Browser",
    "section": "",
    "text": "In this blog post, we will show how to use the graphical user interface (also known as GUI) Pygame to create a game. In addition, we will incorporating Flask which helps facilitate modern web browsers.\nIn our example, we will be creating a rendition of the Human Benchmark Test’s Chimp Test (https://humanbenchmark.com/tests/chimp). If you are unfamiliar with this game, essentially, the game starts with a grid of randomly positioned and labeled squares and the goal of the user is to memorize the position of each square. Once fully memorized, the user is tasked with clicking all the squares in chronological order without the numbers appearing anymore after making the first move. The user has three lives to pass through as many levels as possible, each getting increasingly more difficult with having to memorize more squares. Thus, in this blog post, we will be going over how we used Flask to create the web browser to allow the user to choose game mode settings and view general performance statistics with Plotly as well as Pygame to create the game’s memory logic.\nAll the necessary codes to follow this blog post is in this repository: https://github.com/Coding-Tom-1405/PIC16B-Group-Project\nLet’s get started!\n\nFlask (Website Aspect)\nThe first thing we will talk about is the Flask usage to create our webapp.\nFirst, because we are dealing with Flask, we needed to import its package and the neccesary functions. In addition, since we will use data usage, we will also need to import csv.\nfrom flask import Flask, render_template, request, redirect, url_for, send_from_directory, jsonify\nimport csv\nBecause this type of game already exists, we wanted to come up with our own twist by incorporating a difficulty and challenge mode settings that the players can choose. For difficulty, we made it so that after each new level, the player will get to choose how many new squares they wanted to introduce in the next round. For additional challenge, we decided to incorporate a time aspect to limit how long they have to remember the number’s position. Because the game mode settings will change constantly, and we needed to create a function to save these settings into a data file so that our Pygame can access it later. For this function, which we called write_to_csv, we needed the two settings as our parameters and then our code will essentially open the CSV and append new row entries with the game settings chosen.\ndef write_to_csv(level, challenge):\n    \"\"\"\n    Save the selected game level and timer option to a CSV fil\n\n    Args:\n        level (integer): the difficulty mode chosen \n        challenge (string): the challenge mode chosen\n\n    Returns:\n        None.\n    \"\"\"\n    with open('game_setting.csv', 'a', newline='') as file:     # Accessing CSV file on appending mode\n        writer = csv.writer(file)                               # Initializing CSV file writer object\n        writer.writerow([level, challenge])                     # Writing single row to CSV file\nAfter this, we want to establish our HTML template files which acts as the structure of each page on the website.\nThe first one is the menu.html (the base html) which will represent a navigation bar with two links: “Home” which will take you back to the homepage of the website and a direct link to our GitHub repository in case the user wants to take a look at our codes.\n&lt;nav&gt;\n    &lt;ul&gt;\n        &lt;li&gt;&lt;a href=\"{{ url_for('homepage') }}\" class=\"menu-link logo\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"https://github.com/Coding-Tom-1405/PIC16B-Group-Project\" class=\"menu-link\"&gt;GitHub&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/nav&gt;\nWith this establishing a hyperlink to the homepage, the next HTML we need is homepage.html. For this one The main thing that we need is two create to redirections, one for the game setting selection and one for the game data visualization, which we accomplish through a href. In addition, for aesthetics reason, we also created a static folder which contained a style CSS file to describe the fonts and structures of our website as well as different image PNGs to display interesting images in the website.\n{% include 'menu.html' %}\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Welcome to Chimp Test Game&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"welcome-container\"&gt;  &lt;!-- Container for the welcome section --&gt;\n        &lt;div class=\"title\"&gt;\n            &lt;!-- Animated title with delayed effect --&gt;\n            &lt;span style=\"--delay:0s;\"&gt;Welcome to&lt;/span&gt;\n            &lt;br/&gt;\n            &lt;span style=\"--delay:0.2s;\"&gt;Chimp Test Game&lt;/span&gt;\n        &lt;/div&gt;\n        &lt;p class=\"game-description\"&gt;The chimpanzee memory test is a cognitive task designed to assess the memory capabilities of chimpanzees. In this test, participants are presented with a sequence number, which they must remember and recall in the correct order. The test aims to evaluate the capacity of the participants' short term memory.&lt;/p&gt;\n        \n        &lt;div class=\"tab-container\"&gt;  &lt;!-- Container for navigation tabs/cards --&gt;\n            &lt;a href=\"{{ url_for('select') }}\" class=\"card\"&gt; &lt;!-- Redirecting to stats page --&gt;\n                &lt;div class=\"image-box\"&gt; &lt;!-- Container for displaying images --&gt;\n                    &lt;img src=\"{{ url_for('static', filename='images/image1.png') }}\" alt=\"Play Game\"&gt;\n                &lt;/div&gt;\n                &lt;div class=\"content\"&gt;\n                    &lt;h2&gt;Play Game&lt;/h2&gt;\n                &lt;/div&gt;\n            &lt;/a&gt;\n            &lt;a href=\"{{ url_for('stats') }}\" class=\"card\"&gt; &lt;!-- Redirecting to stats page --&gt;\n                &lt;div class=\"image-box\"&gt;\n                    &lt;img src=\"{{ url_for('static', filename='images/image2.png') }}\" alt=\"Game Stats\"&gt;\n                &lt;/div&gt;\n                &lt;div class=\"content\"&gt;\n                    &lt;h2&gt;Game Stats&lt;/h2&gt;\n                &lt;/div&gt;\n            &lt;/a&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThrough this HTML, the homepage looked like this\n\n\n\nHomepage-2.png\n\n\nFrom the hompage.html, we now need to define select.html which acted as the settings page where the user can choose their game modes. The main focus that we needed for this file is creating the option for section for both modes, and for this, we needed to make it so that they can only choose one option for each section. Thus, we used the input type = \"radio\".\n{% include 'menu.html' %}\n\n&lt;!doctype html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Select Game Options&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n    &lt;!-- Same but smaller version of the cards in the main page --&gt;\n    &lt;div class=\"index-container\"&gt;  \n        &lt;div class=\"tab-container\"&gt;  \n            &lt;a href=\"{{ url_for('select') }}\" class=\"card-index\"&gt;\n                &lt;div class=\"image-box\"&gt;\n                    &lt;img src=\"{{ url_for('static', filename='images/image1.png') }}\" alt=\"Play Game\"&gt;\n                &lt;/div&gt;\n                &lt;div class=\"content\"&gt;\n                    &lt;h2&gt;Play Game&lt;/h2&gt;\n                &lt;/div&gt;\n            &lt;/a&gt;\n            &lt;a href=\"{{ url_for('stats') }}\" class=\"card-index\"&gt;\n                &lt;div class=\"image-box\"&gt;\n                    &lt;img src=\"{{ url_for('static', filename='images/image2.png') }}\" alt=\"Game Stats\"&gt;\n                &lt;/div&gt;\n                &lt;div class=\"content\"&gt;\n                    &lt;h2&gt;Game Stats&lt;/h2&gt;\n                &lt;/div&gt;\n            &lt;/a&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n\n&lt;body&gt;\n&lt;form id=\"selectionForm\" action=\"/select\" method=\"post\"&gt;  &lt;!-- Form for submitting game selection options, posts data to /select --&gt;\n    &lt;div class=\"main center\"&gt;\n        &lt;div class=\"box\"&gt;\n            &lt;h2&gt;Select Level&lt;/h2&gt;  &lt;!-- Section game level --&gt;\n            &lt;ul&gt;\n                &lt;input type=\"radio\" name=\"level\" value=\"1\" checked&gt; Level 1&lt;br&gt;\n                &lt;input type=\"radio\" name=\"level\" value=\"2\"&gt; Level 2&lt;br&gt;\n                &lt;input type=\"radio\" name=\"level\" value=\"3\"&gt; Level 3&lt;br&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n        &lt;div class=\"box\"&gt;\n            &lt;h2&gt;Challenges&lt;/h2&gt;  &lt;!-- Section game challenges --&gt;\n            &lt;ul&gt;\n                &lt;input type=\"radio\" name=\"challenge\" value=\"timer\"&gt; Timer&lt;br&gt;\n                &lt;input type=\"radio\" name=\"challenge\" value=\"none\" checked&gt; None&lt;br&gt;\n            &lt;/ul&gt;\n        &lt;/div&gt;\n\n        &lt;div class=\"input-and-button-container\"&gt;  &lt;!-- Container for the submit button --&gt;\n            &lt;div class=\"start-button-container\"&gt;  \n                &lt;button type=\"submit\" class=\"btn\"&gt;Confirm&lt;/button&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/form&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n    \nThrough this HTML, the game setting selection page looked like this\n\n\n\nGame-Settings.png\n\n\nWith this HTML, we can receive the user’s setting choices. However, for the user’s convenience, we also wanted to give the user a chance to confirm their choices or go back to make changes. Thus, we also wrote a confirm.html which displays their previous choices and also has a redirecting button to go back to the select page. In order to access the user’s settings to get level and challenge for this HTML file, however, we also needed to create another function read_from_csv to retrieve the data. Since this list is continuously appended by the write_to_csv function each time a player changes the settings, we want our function to only read the last line of the file, which we can do using indexing -1. Also, because there are two components for each row entry, we decided to use the strip() function to remove any whitespaces and then applied split() function on the ‘,’ delimiter to split the string into a list, assigning two variables for the level and challenge.\nThus, for our accessing data function:\ndef read_from_csv():\n    \"\"\"\n    Read the most recent game setting (level and timer) from the CSV fil\n\n    Returns:\n        The difficulty converted from string into integer form as well as the time (string)e.\n    \"\"\"\n    with o\"game_setting.csv\"file, 'r') as f             # Accessing CSV file with reading modeile:\n        last_line = file.readlines(                     # Getting only the last row entry of file)[-1]\n        difficulty, time = last_line.strip().split(',') # Assigning two variables to the split string\n        return int(difficulty), time\nAnd our HTML file:\n{% include 'menu.html' %}\n\n{% include 'menu.html' %}\n\n&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Game Stats&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n    &lt;style&gt;\n        /* Inline styles - consider moving these to your style.css for production */\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n            background-color: #f0f0f0; /* Light grey background */\n        }\n        .header {\n            text-align: center;\n            padding: 20px;\n        }\n        .plots-container {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            gap: 20px; /* Adds space between the rows */\n        }\n        .plot-row {\n            display: flex;\n            justify-content: center;\n            gap: 20px; /* Adds space between the images */\n        }\n        .plot-row img {\n            width: 100%;\n            max-width: 400px; /* Increased from 300px to make images larger */\n            border-radius: 8px; /* Adds rounded corners to the images */\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1); /* Adds a subtle shadow to the images */\n        }\n        .plot-top {\n            flex-direction: column;\n            align-items: center;\n        }\n        .decoration {\n            margin-top: 20px;\n            text-align: center;\n            color: #333; /* Darker text for contrast */\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"header\"&gt;\n        &lt;h1&gt;Game Statistics&lt;/h1&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"plots-container\"&gt;\n        &lt;div class=\"plot-row plot-top\"&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot1_plotly.png') }}\" alt=\"Plot 1\"&gt;\n        &lt;/div&gt;\n        &lt;div class=\"plot-row\"&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot2_plotly.png') }}\" alt=\"Plot 2\"&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot3_plotly.png') }}\" alt=\"Plot 3\"&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"decoration\"&gt;\n        &lt;p&gt;Explore your game's performance and trends!&lt;/p&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nThrough this HTML file, our confirmation page looks like this\n\n\n\nConfirm.png\n\n\nThe last HTML file we needed to define was the stats.html which established the structure for our data visualization. For this, we want to access different PNG files established by our data visualization codes. So we used img src to access the static folder and then look for the specific plotly.png to display the plot. This is to get all three plots in one website, but we also made them appear as individual interactive plot tabs which we will discuss later.\n{% include 'menu.html' %}\n\n&lt;!doctype html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Game Stats&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n    &lt;style&gt;\n        /* Inline styles */\n        body {\n            font-family: Arial, sans-serif;\n            margin: 0;\n            padding: 20px;\n            background-color: #f0f0f0; /* Light grey background */\n        }\n        .header {\n            text-align: center;\n            padding: 20px;\n        }\n        .plots-container {\n            display: flex;\n            flex-direction: column;\n            align-items: center;\n            gap: 20px; /* Adds space between the rows */\n        }\n        .plot-row {\n            display: flex;\n            justify-content: center;\n            gap: 20px; /* Adds space between the images */\n        }\n        .plot-row img {\n            width: 100%;\n            max-width: 400px; /* Increased from 300px to make images larger */\n            border-radius: 8px; /* Adds rounded corners to the images */\n            box-shadow: 0 4px 8px rgba(0,0,0,0.1); /* Adds a subtle shadow to the images */\n        }\n        .plot-top {\n            flex-direction: column;\n            align-items: center;\n        }\n        .decoration {\n            margin-top: 20px;\n            text-align: center;\n            color: #333; /* Darker text for contrast */\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;div class=\"header\"&gt;\n        &lt;h1&gt;Game Statistics&lt;/h1&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"plots-container\"&gt; &lt;!-- Container for plots --&gt;\n        &lt;div class=\"plot-row plot-top\"&gt; &lt;!-- Container for top plot row --&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot1_plotly.png') }}\" alt=\"Plot 1\"&gt;\n        &lt;/div&gt;\n        &lt;div class=\"plot-row\"&gt; &lt;!-- Container for top plot row --&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot2_plotly.png') }}\" alt=\"Plot 2\"&gt;\n            &lt;img src=\"{{ url_for('static', filename='plots/plot3_plotly.png') }}\" alt=\"Plot 3\"&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;div class=\"decoration\"&gt;\n        &lt;p&gt;Explore your game's performance and trends!&lt;/p&gt;\n    &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nWith all HTML files established, we started with our app.py to include the app routing definitions.\nSo after creating a Flask application object with app = Flask(__name__), the first route definition we needed was for the homepage. To specify the routing of different pages, we used @app.route(), and inside the () will be the unique tag to identify the page. So for this homepage, we start with '/' inside. Next since we have the HTML file specifying the formatting of the homepage, we we created a function to call that file using render_template().\n@app.route(\"/\")\ndef homepage():\n    \"\"\"\n    Render the homepage of the web.\n    \"\"\"\n    return render_template('homepage.html')\nNext, after clicking the “Play Game” button on the homepage, we wanted to be directed to the game setting selection page, which will utilize the select.html. Thus we said @app.route(\"/select\"). However, since this HTML uses form method, we needed to make sure to also include the parameter methods = ['GET', 'POST']) in the () to let the application know how to handle different requests. Because of this, we needed to use an if statement in our function to check if the request is specifically a 'POST' request, and if so, the form on the select.html was submitted, which means we will need to call the write_to_csv function to send the content to the game setting CSV file.\n@app.route(\"/select\", methods=['GET', 'POST'])\ndef select():\n    \"\"\"\n    Get and store the game level and challenge options in the game setting CSV file.\n    \"\"\"\n    if request.method == 'POST':\n        selected_level = request.form.get(\"level\")            # Getting the chosen level\n        selected_challenge = request.form.get(\"challenge\")    # Getting the chosen challenge\n        write_to_csv(selected_level, selected_challenge)      # Appending settings into CSV file\n        return redirect('/confirm')                           # Rendering confirm.html template\n    return render_template('select.html')\nOnce the player chooses their settings, we want to get to the confirmation page, so we will do @app.route(\"/confirm\"). Here is where we will access read_from_csv() to access the last row entry which we will then assign to two variables to then send to confirm.html\n@app.route(\"/confirm\")\ndef confirm():\n    \"\"\"\n    Render the confirm page with the last selected game settings.\n    \"\"\" \n    selected_level, selected_challenge = read_from_csv()   # Retrieving the last settings\n    return render_template(\"confirm.html\", level=selected_level, challenge=selected_challenge)\nThe last app routing definition is the stats page we we will save the body for the third section of the project discussion but we did @app.route(\"/stats\") and return render_template('stats.html') like any of the other routings.\n\n\nPygame Code (Game Aspect)\nThe next thing that we will talk about is the code to create the game as this is the main focus of our project.\nFirst, because we need the pygame package, we will need to install it. Thus, in Anaconda Prompt, after switching to the conda environment you want to work in, we will then run pip install pygame to install it in our environment.\nNext, after we install pygame, we will create our main.py Python file (we will explain later why we want to call it that). In this file, let’s import the necessary packages: pygame (for creating video games), random (since we want to randomize the square’s position), os and csv (since we want to access or create directory CSV files)\nimport pygame     \nimport random\nimport os\nimport csv\nHere, pygame and random will be solely used for our game code. os and csv we will see needed for other aspects of the project.\nWith these packages, lets start writing our game code. We will begin with creating our initial global variables that we will use throughout our codes:\nThe first thing we need is to create our coordinate grid to get the positions of the squares. We did this by creating a list of ordered pairs and using a nested for loop definition to define the available range of both x and y.\ncoord = [(x, y) for x in range(1, 10) for y in range(1, 10)]\noriginal_coord = coord[:]                                   # Replica to update coord \nNote that we had an original_coord variable as well. The reason being is because during a level, once one corrdinate has been assigned to a square, we removed it from the coord list to prevent any duplicate. However, during debugging. The game would terminate itself after level 4 or 5, getting an indexing error because since coord wasn’t being constantly replenished. Thus, we made sure to incorporate a replica to make sure to update it to being its old version again.\nAfter the grid was established, we needed to establish our collection of squares and numbers. After researching further about Pygame, we realized that the package had a sprite element which acted as a 2D image of a larger graphical scene, and these could be grouped in a container using sprite’s Group() function, so we assigned a variable to pygame.sprite.Group() so that we can constantly modify/update and access it during different levels. We also figured that because these squares will have numbers associated to them, we wanted to create a list where we can append the numbers to it to check the numerical order that the user is clicking the squares.\nsquare_group = pygame.sprite.Group()                        # Grouping of the squares\nnum_list = []                                               # List to append the square's numbers\nNext, we needed to create the game setting variables such as the level or the number of incorrect guesses the user makes which we can iterate in the event they get the round right or wrong.\n# Creating game settings\nlevel = 1                                                   # Initial level value\nincorrect_guesses = 0                                       # Initial incorrect guesses made\nThen, because one of our challenge mode was a timed memory phase, and since our third technical component was to use , we needed to incorporate time variables in order to help keep track of when to display the timer and as well as how long did the user take to complete the level. First, because pygame itself doesn’t have a timer, we needed to establish a stopwatch variable time_passed (starting at 0) as well as a max_timer where we will constantly find the difference of to keep track of how much time is left for memorizing. Second, since we want our game to only display the timer at certain points, we needed a boolean variable timer_on_or_off which we will assign 1 or 0 to let the game know when it should be on or off.\n# Time setting\ntime_passed = 0                                             # Stopwatch\nmax_timer = 100                                             # Initial timer for lvl 1\nstart_time = 0                                              # Initial level start time\ntimer_on_or_off = 1                                         # Boolean indicating if timer is on\nLastly, because we needed to take into account the user’s desired difficulty and or challenge mode chosen, we also needed to find some way to access the settings CSV. Thus, similar to the Flask section, we -1 indexing method to only get the last line and assigned difficulty to be the int version of the string on the left of the ‘,’ delimiter. However, this time, because in our CSV file, the string for the challenge mode is either 'timer\\n' or 'none\\n', we can’t just assign use this. So, instead, we created another boolean variable timer_chosen to indicate whether or not they selected this mode. For this, we used an if statement to check that if the entry is 'timer\\n', then we will set the boolean as 1, indicating True, and 0 otherwise.\ndifficulty_file = \"game_setting.csv\"                        \nwith open(difficulty_file, 'r') as file:                    # Accessing game_setting.csv\n    last_line = file.readlines()[-1]                        # Accessing the last entry row\n    difficulty = int(last_line.split(',')[0])               # Splitting last line with comma delimiters\n                                                            # Taking the 1st integer variable as difficulty\n    if last_line.split(',')[1] == 'timer\\n' :               # Checking if 2nd element says timer chosen\n        timer_chosen = 1\n    else :                                                  # Checking if 2nd element says timer NOT chosen\n        timer_chosen = 0\nAfter decalring our global variables, we will create our functions and classes.\nThe first function we need to write is game_init() which will sllow us to initialize all pygame modules. In this function, we want to set what our initial screen will look like:\ndef game_init():\n    \"\"\"\n    Initializing game\n    \"\"\"\n    global font, screen\n    pygame.init()                                             # Initializing pygame modules\n    screen = pygame.display.set_mode((800, 800))              # Setting screen size\n    pygame.display.set_caption(\"Chimp Test\")                  # Setting game name\n    font = pygame.font.SysFont(\"Times New Roman\", 50)         # Setting general text font and size\n    screen.fill((0, 0, 255))                                  # Setting background blue\nThe next thing we needed to do is create our class Square, the initial step our game. This class will help us create the square objects that we want to manipulate throughout the game. Thus, for the class, we will set pygame.sprite.Sprite as our parameter.\nclass Square(pygame.sprite.Sprite): \n    \"\"\"\n    Class representing Square object in the game\n    \"\"\"\n    def __init__(self, number, make_square=True):\n        \"\"\"\n        Initializing Square object with given number\n\n        Args:\n            number (int): Number associated with square\n            make_square (bool): Flag indicating whether to create the squares\n\n        Returns:\n            None\n        \"\"\"\n        super(Square, self).__init__()\n        self.number = number\n        if make_square:\n            self.make_square()\nNote that we have a number parameter which will be for the numbers we overlay and associate to each square as well as make_image parameter which will be explain later.\nWithin this class, we needed to define our methods. For the square objects, we needed to 1) assign its coordinates 2) generate the squares on the grids and 3) constantly change its appearance depending on the phase of the game.\nThe first one is update(self), which is to change the appearance of the square on the screen. This one is the simplest out of the other methods since we will this is just getting them to appear on the screen after the squares have already been assigned then their coordinates and given definition of what it should look like. So for this, we just needed to use the blit() function to overlay it on the screen.\ndef update(self): \n    \"\"\"\n    Updating the appearance of the square on the screen \n    by using the 'make_square' method and then blitting image on screen\n    \"\"\"\n    self.make_square()\n    screen.blit(self.image, (self.x, self.y))     # Blitting square onto screen at its allocated coordinate\nThe next method will be random_coord(self). Because we want our squares to be randomly positioned throughout the screen, we want to incorporate our random package to get the coordinates of each square. Also, because our x and y values only extend up to 9, we need to rescale these values to that none of the squares overlap each other.\ndef random_coord(self): \n    \"\"\"\n    Generating random coordinates for each square\n    \"\"\"\n    global coord\n    if not coord:                                      # Checking if coord is empty\n        coord.extend(original_coord)                   # Refilling the list\n    coordinates = random.choice(coord)                 # Choosing random coordinates\n    coord.remove(coordinates)                          # Removing the chosen coordinates to prevent repeats\n    x, y = coordinates \n    return x * 80, y * 80                              # Return scaled coordinates to get 80 x 80 square\nAfter we get the coordinates of the squares, we can then produce the square images on the screen. Thus, the last method will be make_square(self) which will use random_coord to first get the (x,y) coordinates and then say where the square should be created on the screen. In order to accomplish this, we used pygame.Surface which are just 2D rectangular images that also allow us to specify sizing as well as color through RGB values. In addition, for this function, we needed to be able to display the number associated to each squares, this, we first got the area of the square through the Pygame’s get_rect() function and then center() function to blit/render the number in string form on the squares.\ndef make_square(self):\n    \"\"\"\n    Creating the graphical representation of the squares \n    using 'random_coord' method to assign squares the coordinates\n    \"\"\"\n    global font\n    self.x, self.y = self.random_coord()                       # Assigning the square a coordinate\n    self.image = pygame.Surface((80, 80))                      # Creating a surface for the square\n    self.image.fill((255, 255, 255))                           # Filling in the square white\n    self.rect = self.image.get_rect()                          # Getting area occupied by square\n    self.rect.center = self.x, self.y                          # Setting (x,y) coordinates as center\n    self.number = str(self.number)                             # Converting number into string\n    self.text = font.render(self.number, 1, (0, 0, 0))         # Rendering number onto square's surface\n    text_rect = self.text.get_rect(center=(80 // 2, 80 // 2))  # Getting rectangle of rendered text\n    self.image.blit(self.text, text_rect)                      # Blitting text onto square's surface\nNotice earlier that because this function produces the squares, we set its default boolean as True so that when new squares are initialized, it immediately gets produced.\nNow that we have our Square class created, we need to create the squares for Level 1. For this level, we wanted to only have three squares to make the game easy at the beggining. To do this, we will create a squares_init() function which initializes the first group of squares for the game. We accomplished this by using the for loop from 1 to 4 and used the add() functions of sprite.Group to add to the batch the first three square. The reason we did 1 to 4 and not just range(3) is because when calling the class Square with parameter i representing the value, this will cause the make_square method display the set of numbers 0 to 2 rather than 1 to 3.\ndef squares_init():\n    \"\"\"\n    Initializes the first batch of squares (for Level 1)\n    \"\"\"\n    for i in range(1, 4):                     # Iterating over range 1 to 3 (creating three squares)\n        square_group.add(Square(i))           # Adding Square objects with its number to square_group\nAfter this initial phase, we want to create our functions for the next phase: memory recall. The first function we want to create is hide_cards() which will help us remove the numbers once the user starts clicking the squares. The method we did this is blitting another white Surface on top of our previous group of Surfaces so that the numbered squares are beneath a cover.\ndef hide_square():\n    \"\"\"\n    Hides the square's numbers by covering them with a white surface\n    \"\"\"\n    cover = pygame.Surface((80, 80))           # Creating a 80 x 80 white surface \n    for sprite in square_group:                # Iterating through each square in square_group\n        cover.fill((255, 255, 255))            # Filling the surface with white\n        sprite.image.blit(cover, (0, 0))       # Blitting the white surface onto the squares\nNext function we needed to make is reset_coord(). As mentioned at the beginning of this section, from random_coord(), positions are removed one at a time from coord when producing new squares. without replenishing the grid, our code would reach a point where there are no more available coordinates. Thus, we needed function is to help us replenish the coordinate grid after each level by setting it equal to original_coord, which is never changed throughout our code.\ndef reset_coord():\n    \"\"\"\n    Resets the coordinate grid so the squares can take any of the 81 positions after a new level\n    \"\"\"\n    global coord, original_coord            # Accessing the global variables\n    coord = original_coord[:]               # Resetting coord to original_coord by creating a shallow copy\nWith these functions, we can create our memory() function which will be the main function for the memory recall phase. This function will guide us through what will happen when they start clicking the squares as well as what to do once they finish recalling, whether they get the sequence right or wrong.\nFirst, we want to access the neccesary global variables:\nglobal num_list, level, timer_on_or_off, max_timer, incorrect_guesses, start_time, game_data, difficulty\nNext, we want to let the game know what to do if the user actually clicked on one of the squares so that nothing happens if they click the background only. So, we will use if sprite.rect.collidepoint(x, y):\nAfter, we want to let the game know what to do if the user makes any mistake throughout recall (i.e. if the clicked square is out of order. So we will use, if sprite.number != str(len(num_list)):\nThen, we want to let the game know what to do if the user manages to click all the squares. So we will use if len(num_list) == len(square_group):\nif len(num_list) == len(square_group):        # Checking if all squares were clicked\n        win = num_list == [str(squares.number) for squares in square_group] # Setting win condition\n        end_time = pygame.time.get_ticks()    # Getting end_time of level\n        time_spent = (end_time - start_time) / 1000.0  # Calculating level time spent in seconds\n    \n    if win:                                   # Checking if win condition met\n        level += 1                            # Increasing level by 1\n        reset_coord()                         \n        num_list = []\n        for i in range(difficulty):           # Adding number of squares equal to difficulty chosen\n            square_group.add(Square(len(square_group) + 1, make_square=False)) \n        timer_on_or_off = 1            \n        max_timer += 10                       # Increasing timer for next level\n        screen.fill((0, 255, 0))              # Updating display\n        pygame.time.wait(500)                 # Extending green screen for 1000 milliseconds\n        \n    else:                                     # If win condition is not met\n        reset_coord()                         \n        incorrect_guesses += 1\n        num_list = []\n        timer_on_or_off = 1\n        screen.fill((255, 0, 0))             # Filling screen with red\n        pygame.display.flip()  \n        pygame.time.wait(500) \nNote that else section will probably never be ran since the if sprite.number != str(len(num_list)): comes first and will always catch if the user ever makes a mistake (the else is just for good coding practice)\nWith all of these situational if statements established, our completed memory function will look something like this:\ndef memory(sprite):\n    \"\"\"Memory recal game logic\n    Args:\n        sprite (Square): Square object that was clicked\n\n    Returns:\n        None\n    \"\"\"\n    global num_list, level, timer_on_or_off, max_timer, incorrect_guesses, start_time, game_data, difficulty\n\n    x, y = pygame.mouse.get_pos()                      # Getting current mouse position \n    if sprite.rect.collidepoint(x, y):                 # Checking if mouse is within the square's boundary\n        num_list.append(sprite.number)                 # Appending square's number to num_list\n        sprite.rect = pygame.Rect(-80, -80, 80, 80)    # Moving square off the screen\n\n        if sprite.number != str(len(num_list)):    # Checking if the number clicked is out of order\n            end_time = pygame.time.get_ticks()     # Ending pygame clock\n            time_spent = (end_time - start_time) / 1000.0  # Calculating time spent in seconds\n            reset_coord()                          # Resetting coordinate grid\n            incorrect_guesses += 1                 # Increasing amount of incorrect guesses by 1\n            num_list = []                          # Emptying num_list for new round\n            timer_on_or_off = 1                    # Setting timer back on\n            square_group.update()                  # Updating square group\n            screen.fill((255, 0, 0))               # Filling screen with red\n            pygame.display.flip()                  # Updating display screen\n            pygame.time.wait(500)                  # Extending red screen for 500 milliseconds\n            return\n\n    if len(num_list) == len(square_group):        # Checking if all squares were clicked\n            win = num_list == [str(squares.number) for squares in square_group] # Setting win condition\n            end_time = pygame.time.get_ticks()    # Getting end_time of level\n            time_spent = (end_time - start_time) / 1000.0  # Calculating level time spent in seconds\n        \n        if win:                                   # Checking if win condition met\n            level += 1                            # Increasing level by 1\n            reset_coord()                         \n            num_list = []\n            for i in range(difficulty):           # Adding number of squares equal to difficulty chosen\n                square_group.add(Square(len(square_group) + 1, make_square=False)) \n            timer_on_or_off = 1            \n            max_timer += 10                       # Increasing timer for next level\n            screen.fill((0, 255, 0))              # Updating display\n            pygame.time.wait(500)                 # Extending green screen for 1000 milliseconds\n            \n        else:                                     # If win condition is not met\n            reset_coord()                         \n            incorrect_guesses += 1\n            num_list = []\n            timer_on_or_off = 1\n            screen.fill((255, 0, 0))              # Filling screen with red\n            pygame.display.flip()  \n            pygame.time.wait(500)\n\n        start_time = pygame.time.get_ticks()      # Resetting start time for next level/attempt\n        square_group.update()                     # Updating square group\nThe last thing we needed was the aftermath phase where once the player finishes the game. Again, because we are incorporating data visualization, we needed a function to help us save the game results to a data file. What we wanted in our data was that for each round, we wanted to save the level they were on, the result of that round, and lastly the time spend on it. Thus, we created the function save_game_data to take in a list of lists as a parameter and then flatten in to get a singular row representing the whole game stat. Similar to the Flask write_to_csv method, we needed to open our game data CSV using appending mode and initializing CSV file writer object to create a new row entry.\ndef save_game_data(game_data):\n    \"\"\"\n    Saving the game data results into a CSV file\n\n    Args:\n        game data(list of lists): The list of data to be saved, each sublist represents the results of a round\n\n    Results:\n        None\n    \"\"\"\n    headers = [\"Level\", \"Result\", \"Time Spent\"]                       # Defining headers for CSV file\n    flattened_data = [item for sublist in game_data for item in sublist]  # Flattening the list\nNow that we have all the pathways established for the game, we will then create the main() function which will have the actual game loop running in it.\nFor this function, we first need to call game_init() and squares_init() to initialize all Pygame modules and then create the starting squares for round 1. After, since we have a time aspect, we needed to incorporate pygame.time.Clock() which allowed us to limit the frames per second and then pygame.time.get_ticks() which lets us get the execution time in milliseconds (we will convert this to seconds later). Lastly, we needed to establish our loop boolean running so that we can change it to False when we want to loop to end and the game to stop.\nNext we needed to define our loop. This is the most crucial part since it is what how long to run the game as well as where to go or what functions to access in specific scenarios.\nThe first things is the ending situation. This we had to define at the beginning of the loop since we need to check after each round if we should terminate the game or not. Since we are incorporating a three lives aspect, we used the if statement if incorrect_guesses &gt;= 3: to check if they have made three mistakes yet. In this if statement, we had to hide the squares so that we can then fill the screen with white and blit a “GAME OVER! text. In addition, since this is the last time the loop will run, we needed to make sure to save the game data using save_game_data() function before the game terminates. Lastly, to prevent any other code from running in this loop, we set running = False and then continue to skip the rest of the lines.\nAfter the ending condition, we established the display screen by setting the background, rendering the level and timer. For the timer, because there is no actual countdown timer in pygame, we had to do str(timer - time_passed) which is constantly updated to get the remaining time counting down. However, an issue we faced during writing this was that we only want the time to display and the numbers to automatically disappear if the user actually chose the timer challenge mode. This is why we created the timer_chosen variable as a separate so that we can do a double condition if statement. By doing if time_passed &gt;= timer and timer_chosen == 1: we are able to check if 1) the alloted memory time has passed and 2) the user actually chose this setting. In here, we would use hide_square() as well as set timer_on_or_off to 0 so that we can tell the loop to stop displaying the timer as well.\nLastly, for this loop, we need to check if they made contact with any of the squares. We can do this by combining the MOUSEBUTTONDOWN with if swuares.rect.collidepoint() to check if 1) the clicked their mouse followed by 2) if the place clicked was where one of the squares were. In the if statement, again, we would set timer_on_or_off to 0 to stop displaying the timer (if user chooses the setting) and also call memory to help with the recall phase of the game.\nWith this our code looked like this.\ndef main():\n    \"\"\"\n    Main function for running the game, including initializing the game settings, \n    managing the game loop, and updating game state and display\n    \"\"\"\n\n    # Accessing global variables\n    global timer_on_or_off, counter, timer, start_time, time_passed, game_data\n    \n    game_init()                                               # Initializing the game (pygame.init() in here)\n    squares_init()                                            # Initializing lvl 1 squares\n    clock = pygame.time.Clock()                               # Getting clock to time user's levels\n    start_time = pygame.time.get_ticks()                      # Starting timer for each level\n\n    running = True                                            # Establishing initial boolean for loop running\n\n    # Initial Welcome Message\n    print(\"WELCOME TO THE CHIMP TEST GAME! HAVE FUN!\")\n    print(\"-----------------------------------------\")\n    \n    while running:\n        # Ending Situation\n        if incorrect_guesses &gt;= 3:              \n            # Ending Message\n            print(\"--------------------------------\")\n            print(\"GAME OVER! Your Score: \", level)           \n            \n            hide_square()                                     # Hiding cards in end screen \n            screen.fill((255, 255, 255))                      # Filling background with white\n            text = font.render(\"GAME OVER!\", 1, (0, 0, 0))    # Rendering black \"GAME OVER!\" text\n            text_rect = text.get_rect(center=(screen.get_width()//2, screen.get_height()//2)) # Centering text\n            screen.blit(text, text_rect)                      # Blitting text onto screen\n            pygame.display.update()                           # Updating display to show text\n            \n            save_game_data(game_data)                         # Saving game data\n            running = False                                   # Setting game loop boolean to false\n            continue                                          # Skipping rest of loop\n              \n        screen.fill((0, 0, 255))                              # Setting background blue\n        text = font.render(\"Level: \" + str(level), 1, (255, 255, 255)) # Displaying level text\n        screen.blit(text, (0, 0))                             # Blitting text at specific position\n\n        # User Chose Timer Mode Situation\n        if timer_on_or_off == 1 and timer_chosen == 1:        \n            text = font.render(\" Time: \" + str(timer - time_passed), 1, (255, 255, 255)) # Displaying timer\n            screen.blit(text, (200, 0))                       \n            time_passed += 1                                  # Incrementing stopwatch\n            \n        for event in pygame.event.get():                      \n            if event.type == pygame.QUIT:                     # Checking if user manually quits\n                running = False        \n            if event.type == pygame.KEYDOWN and event.key == pygame.K_s:  # Checking if user clicks keys\n                running = False                         \n            if event.type == pygame.MOUSEBUTTONDOWN:          # Checking if clicked mouse\n                for squares in square_group:           \n                    # Mouse Clicks Square Situation\n                    if squares.rect.collidepoint(event.pos):   \n                        hide_square()                         # Hiding cards\n                        timer_on_or_off = 0                   # Turning timer off\n                        time_passed = 0                       # Resetting stopwatch\n                        memory(squares)                       # Activating memory logic with clicked square\n\n        square_group.draw(screen)                             # Drawing the remaining squares on screen\n\n        # Time-is-up Situation\n        if time_passed &gt;= timer and timer_chosen == 1:        \n            hide_square()                                      \n            time_passed = 0\n            timer_on_or_off = 0\n            \n        pygame.display.update()                               \n        clock.tick(20)                                        # Limiting frame rate to 20 frames per second\n        \n    pygame.quit()\nThroughout the quarter we were looking at different ways to make our Pygame code compatible with Flask, which we ultimately did not find. The first way was using Pyglet where we could use a Flask-Bootstrap package that can act as a barebones flask web interface that would control the pyglet app. This meant that we had to convert our Pygame code into Pyglet which looked like this\nimport pyglet\nfrom pyglet.gl import glClearColor\nimport random\nimport os\nimport csv\n\n# all the possible positions for the numbers\ncoord = [(x, y) for x in range(1, 10) for y in range(1, 10)] \noriginal_coord = coord[:]\n\ndef game_init():\n    global window, font\n    \n    window = pyglet.window.Window(800, 800, caption=\"Chimp Memory Test\")\n    font = pyglet.font.load(\"Times New Roman\", 35)\n \n    # Changing surface color\n    glClearColor(0, 0, 1, 1)  # Set background color to blue (R=0, G=0, B=1)\n    window.clear()  # Clears the window with the specified background color\n\nall_sprites = []\n\nclass Square(pyglet.sprite.Sprite):\n     \n    def __init__(self, number, batch, make_image = True):\n        super(Square, self).__init__(img = pyglet.image.SolidColorImagePattern(color=(255, 255, 255, 255)).create_image(80, 80), batch = batch)\n        self.number = number\n        all_sprites.append(self)\n        if make_image:\n            self.make_image()\n\n        \n    def random_coord(self):\n        global coord  # Access the global coord list\n        if not coord:\n            # Regenerate coord list if it's empty\n            coord.extend(original_coord)\n        coordinates = random.choice(coord)\n        x, y = coordinates\n        coord.remove(coordinates)  # Remove the chosen position from the list\n        x = x * 80\n        y = y * 80\n        return x, y\n    \n    def make_image(self):\n        self.x, self.y = self.random_coord()\n        self.number = str(self.number)\n        self.text = pyglet.text.Label(self.number,\n                                      font_name = \"Times New Roman\",\n                                      font_size=35,\n                                      color=(0, 0, 0, 255),\n                                      x=self.x + 40,\n                                      y=self.y + 40,\n                                      anchor_x='center', \n                                      anchor_y='center'\n                                     )\n    \n    def draw(self):\n        super(Square, self).draw()\n        self.text.draw()        \n\n\nsquare_batch = pyglet.graphics.Batch()\nnum_list = []\nlevel = 1\nincorrect_guesses = 0\n\n\ndef hide_cards():\n    for sprite in all_sprites:\n        sprite.opacity = 0\n        \ndef reset_coord():\n    global coord, original_coord\n    coord = original_coord[:]  # Reset pos to its original state        \n\n\ndef memory(x, y):\n    global num_list, level, timer_on, max_timer, cards_visible, incorrect_guesses, loop\n    # Check the collision only when conter is off\n\n    for square in all_sprites:\n        if square.x &lt;= x &lt; square.x + 80 and square.y &lt;= y &lt; square.y + 80:\n            num_list.append(square.number)\n            square.x = -80\n            square.y = -80\n        # Check if you are wrong as you type\n        if square.number != str(len(num_list)):\n            print(str(level))\n            print(\"incorrect\")\n            reset_coord()\n            incorrect_guesses += 1\n            num_list = []\n            timer_on = 1\n            square_batch.draw()\n            glClearColor(1.0, 0.0, 0.0, 1.0)  # Clear with red color\n            window.clear()\n            if incorrect_guesses &gt;= 3:\n                loop = False  # Terminate the game loop\n            return\n\n\n    if len(num_list) == len(all_sprites):\n        win = num_list == [str(square.number) for square in all_sprites]\n        if win:\n            print(str(level))\n            level += 1\n            print(\"correct\")\n            reset_coord()\n    \n            num_list = []\n            # Add new squares with make_image=True\n            for i in range(len(all_sprites) + 1, len(all_sprites) + 4):\n                square = Square(i, batch = square_batch)\n                square.make_image()  # Call make_image for each new square\n            timer_on = 1\n            max_timer += 10\n            cards_visible = 1\n            glClearColor(0.0, 1.0, 0.0, 1.0)  # Clear with green color\n            window.clear()\n            \n        else:\n            print(str(level))\n            print(\"incorrect\")\n            reset_coord()\n            incorrect_guesses += 1\n            if incorrect_guesses &gt;= 3:\n                loop = False  # Terminate the game loop\n            num_list = []\n            timer_on = 1\n            cards_visible = 1\n            glClearColor(1.0, 0.0, 0.0, 1.0)  # Clear with red color\n            window.clear()\n           \n        square_batch.draw()\n        print(\"-\"*25)        \n    \ndef squares_init():\n    global square_batch\n    for i in range(1, 4):\n        square = Square(i, batch = square_batch)\n\n\n\ncounter = 0\ntimer_on = 1\nmax_timer = 200\ncards_visible = 1\n\ndef get_maxlevel() -&gt; int:\n    filename = \"maxlevel.txt\"\n    if filename in os.listdir():\n        with open(filename, \"r\") as file:\n            leader = file.read()\n            if leader == \"\":\n                maxlevel = 0\n            else:\n                maxlevel = int(leader)\n    else:\n        maxlevel = 0\n    return maxlevel\n\n\ndef update_maxlevel(level):\n    maxlevel = get_maxlevel()\n    if level &gt; maxlevel:\n        with open(\"maxlevel.txt\", \"w\") as file:\n            file.write(str(maxlevel))\n        \ndef save_game_data(game_data):\n    # Check if the file is empty\n    is_empty = os.path.getsize(\"game_data.csv\") == 0\n\n    with open(\"game_data.csv\", \"a\", newline=\"\") as csvfile:\n        writer = csv.writer(csvfile)\n        if is_empty:  # Write the header only if the file is empty\n            writer.writerow([\"Level\"])  # Write header\n        for data in game_data:\n            writer.writerow([data[\"level\"]])\n\n\ndef main():\n    global timer_on, counter, max_timer, cards_visible, loop, game_data\n\n    game_init()\n    squares_init()\n    clock = pyglet.clock.Clock()\n\n    loop = True\n    game_data = []\n\n\n    @window.event\n    def on_draw():\n        pyglet.gl.glClearColor(0, 0, 1, 1)  # Set clear color to blue\n        window.clear()  # Clear the window with the specified background color\n        level_label = pyglet.text.Label(\"Level: \" + str(level),\n                                        font_name=\"Times New Roman\",\n                                        font_size=35,\n                                        x=0,\n                                        y=750\n                                       )\n        level_label.draw()\n        if timer_on:\n            time_label = pyglet.text.Label(\" Time: \" + str(max_timer - counter),\n                                           font_name=\"Times New Roman\",\n                                           font_size=35,\n                                           x=200,\n                                           y=750\n                                          )\n            time_label.draw()\n        square_batch.draw()\n\n\n       \n    @window.event\n    def on_mouse_press(x, y, button, modifiers):\n        global counter, timer_on, loop\n        if button == pyglet.window.mouse.LEFT:\n            # Click mouse and stop the timer and hide the cards\n            hide_cards()\n            timer_on = False\n            counter = 0\n            memory(x, y)\n    \n    def update(dt):\n        global counter, timer_on, loop\n        if timer_on:\n            counter += 1\n            if counter &gt;= max_timer:\n                counter = 0\n                timer_on = False\n#                 loop = False\n    pyglet.clock.schedule_interval(update, 1.0 / 60)\n    pyglet.app.run()\n    \n    # Append current level data to game data\n    game_data.append({\"level\": level})\n\n    # Save game data to CSV\n    save_game_data(game_data)\n\n    update_maxlevel(level)\n\nmain()\nHowever, after looking into it, we realized some JavaScript which we were now knowledgable of.\nThe second method we used was Pygbag which was a package that would allow us to run python code directly in modern web browsers. To make the change in our Pygame code, all we had to do was make the main function defintion an asynchronous function definition (async def main():) and use await asyncio.sleep(0) to let other tasks run main by using asyncio.run(main()). However, while we were able to get the game to run in the website by redirecting the select page to the localhost (doing redirect({localhost URL}), we were not able to access the game setting or access the game data CSV files. We believe that this is because of the asynchronous coroutin nature of Pygbag which makes it not able to access local directoy files. Therefore, with the Professor’s permission, we made the two separate entities\nNow that we have finished our main.py, now all that’s left is to call main() at the bottom to have the actual game loop running once we call python main.py in the prompt. Our end product looked like this\nMemory Phase: \nRecall Phase: \n\n\nPlotly (Data Visualization Aspect)\nThe last focus of our code was generating the statistics of the general game performance. We used plotly so that we can generate interactive graphs that allowed us to look at values at specific points. Again, we created three different visualizations: 1) distribution of max level reached 2) average completion time of each level and 3) distribution success rate of each level\nThe first thing we needed to do was to establish a directory to save the plots for the stats page of the website.\nPLOTS_DIR = 'static/plots'\nif not os.path.exists(PLOTS_DIR):\n    os.makedirs(PLOTS_DIR)\nFrom here on out, everything else we be in the stats() function for the app routing to /stats.\nNext, we need to read the file line by line.\nwith open(file_path, 'r') as file:\n    lines = file.readlines()\nNote that the lines each row is are strings, with the different elements being separated by a ‘,’ delimeter, which means that we are essentially reading in the lines as list of strings. Thus, we needed to created a list to store the parsed CSV data, which we can then read the CSV file again to parse its content. Since each row is a single game represented by the level, result, and time spent, for each round, we needed want to incorporate a for loop to process every three elements, combining the level, result, and time spend (after converting them into the numbered-strings into ints and floats) into one element for our parsed CSV data.\n# Initialize a list to store parsed CSV data\nparsed_data = []\n# Read the CSV file again to parse its content\nwith open(file_path, 'r') as file:\n    for line in file:\n        elements = line.strip().split(',')\n        # Process every three elements (assuming level, result, time_spent format)\n        for i in range(0, len(elements), 3):\n            try:\n                level = int(elements[i])  # Convert the first of every three elements to an integer\n                result = elements[i+1]  # The second element is the result (e.g., win/lose)\n                time_spent = float(elements[i+2])  # The third element is time spent, converted to float\n                parsed_data.append([level, result, time_spent])\n            except ValueError:\n                print(f\"Skipping malformed line: {line.strip()}\")\nNow that we have prepped our data, we can the data visualization process\nThe first visual will be the distribution of max level achieved. From reading the file line by line as a list of strings earlier, we can now create a for loop where we will process each line to extract the maximum level. We can accomplish this by first spliting the strings at each ‘,’ to make them elements in a list, storing this in a variable we called fields. Because our list consist of the three separate data elements (level, result, and time), we want to only take into account first element for every 3 elements. We can accomplish this by doing a list comprehension where we iterate over each (index, fields) and check if the index is divisible by 3 since python indexing starts at 0 as well as checking if fields the element consists of only digits. Thus, our list comprehension will look like this: [int(field) for i, field in enumerate(fields) if i % 3 == 0 and field.isdigit()]. Once we do this, we can then get the max integer of that list and append it to another list to represent the cumulative max level achieved for all the games.\nfor line in lines:\n    fields = line.strip().split(',')\n    levels = [int(field) for i, field in enumerate(fields) if i % 3 == 0 and field.isdigit()]\n    if levels:\n        max_levels.append(max(levels))\nOnce we have our list, we want to convert it into a pandas dataframe so that plotly can interpret it.\nlevel_data_custom = pd.DataFrame({'Level': max_levels})\nTo create a distribution graph, we used px.histogram to read in the max level dataframe. Because we want our data can constantly expand in the range of values, we need our graph to constantly update the bin sizes and the range to cover the integer values properly.\nAfter, estblishing number of bins and the titles our code looks like this\nfig1 = px.histogram(level_data_custom, x='Level',\n                nbins=19,  # Sets the number of bins for the histogram, aligning with the range of levels.\n                color_discrete_sequence=['skyblue'])  # Defines the color of the histogram bars.\n# Updates the traces to configure the bin sizes and ranges to ensure they cover integer values of levels correctly.\nfig1.update_traces(xbins=dict(start=0.5, end=20.5, size=1))\n# Sets the layout of the plot, including title and axis labels. Also adjusts the gap between histogram bars for clarity.\nfig1.update_layout(title_text='Game Statistics - Custom Processing',\n              xaxis_title_text='Level',\n              yaxis_title_text='Count',\n              bargap=0.2)\n# fig1.write_image(f\"{PLOTS_DIR}/plot1_plotly.png\")  # Saves the histogram as an image file in a specified directory.\nfig1.show()  # Displays the plot in the Flask web application interface.\nWith this set of code, we were able to generate this graph\n\n\n\nMax-Level.png\n\n\nFrom this graph, we can see that it has somewhat of a bell curve distribution which makes sense since we expect there to be a point where people struggle the most, in our case 6, and the earlier levels or further levels are less likely to be achieved, making their count smaller (For smaller values, it is less likely since they are easier to pass. For larger values, they are harder to reach).\nThe second graph we wanted to created is the average completion time at different levels.\nBecause we have a parsed CSV data established earlier, we can convert this into a pandas dataframe for this ploylt graph, where we will set three separate columns rather than a singular column like in graph 1.\ndf = pd.DataFrame(parsed_data, columns=['Level', 'Result', 'Time Spent'])\nBecause the main focus of this is the average time, we wanted to group our data based on the Time Spent column so that we can use the mean() function to get the average.\navg_time_spent = df.groupby(['Level', 'Result'])['Time Spent'].mean().reset_index()  # Calculates the average time spent per level and result.\nFor this graph, we also though it be interesting to separate by both levels and results to see if there are also any differences in time of those who get the round right versus those who get the round wrong. Thus, applying the Pandas’s unique() function to the Result column, we are able to create two different dataframes to contain the time spent on correct or incorrect rounds separately. Lastly, we will create the figure with the x values being the level and the y values being the time. Here, it is best to use a bar chart since there are two separate entities for the independent variable: level and result. That way, we can use the add_trace function to distinguish different result bars.\nAfter defining layout, our code was this\navg_time_spent = df.groupby(['Level', 'Result'])['Time Spent'].mean().reset_index()  # Calculates the average time spent per level and result.\nfig2 = go.Figure()  # Initializes an empty figure for plotting.\n# Loops through each unique result (e.g., 'correct', 'incorrect') to add a bar for each level and result combination.\nfor result in avg_time_spent['Result'].unique():\n    df_filtered = avg_time_spent[avg_time_spent['Result'] == result]  # Filters the data for the current result.\n    # Adds a bar trace to the figure for each result with the average time spent per level.\n    fig2.add_trace(go.Bar(x=df_filtered['Level'], y=df_filtered['Time Spent'], name=result))\n# Configures the layout of the plot, including the title, axis labels, and setting the bar mode to group for comparison.\nfig2.update_layout(title_text='Average Time Spent per Level',\n               xaxis_title='Level',\n               yaxis_title='Average Time Spent (seconds)',\n               barmode='group')\nfig2.show()  # Displays the plot.\nfig2.write_image(f\"{PLOTS_DIR}/plot2_plotly.png\")  # Saves the plot as an image file.\nWith this set of code, we were able to generate this graph\n\n\n\nAverage-Time.png\n\n\nFrom this graph, we can see that the graph is exponentially increasing as we get more to the right, meaning the higher the level value is. This makes sense since at the beginning, there are less squares so there is less to remember and they are able to click on the squares easlier, spending less time. However, as they get further in the game, the levels get progressively harder since there is more that they need to memorizing, meaning they will most likely need to spend more time before actually starting the recall stage (which will also take longer since there are more squares to click and in case they forget during recall).\nThe last graph we needed to produce was the success rate for different levels.\nUsing the same parsed CSV data, we can use groupby() based on the Results column. Because we want to get a percentage, however, we want to also use the count function to count 1) how many of a certain level’s result was correct and 2) how many times has a certain level been achieved. After getting the quotient of these two, which will be a decimal value, we want to multiply it bu 100 to convert it into a percentage value.\nsuccess_rate = df[df['Result'] == 'correct'].groupby('Level')['Result'].count() / df.groupby('Level')['Result'].count() * 100\nWe can now use this to create our graph. Again, here it would be good to use a bar chart with our levels being the independent variable and the the success rate as the dependent variable.\nAgain, after defining layout, our code was this\n# Calculates the success rate per level by dividing the number of correct results by the total number of attempts.\nsuccess_rate = df[df['Result'] == 'correct'].groupby('Level')['Result'].count() / df.groupby('Level')['Result'].count() * 100\nsuccess_rate = success_rate.reset_index(name='Success Rate')  # Resets index to turn the Series into a DataFrame.\nfig3 = go.Figure(data=go.Bar(x=success_rate['Level'], y=success_rate['Success Rate'], marker_color='blue'))  # Creates a bar plot for success rate.\n# Sets the layout of the plot, including the title and axis labels.\nfig3.update_layout(title_text='Success Rate per Level',\n               xaxis_title='Level',\n               yaxis_title='Success Rate (%)')\nfig3.show()  # Displays the plot.\nfig3.write_image(f\"{PLOTS_DIR}/plot3_plotly.png\")  # Saves the plot as an image file.\nWith this set of code, we were able to generate this graph\n\n\n\nSuccess-Rate.png\n\n\nFrom this graph, we can see how there is a V-shaped distribution which does this make sense for two separate scenarios. For the lower levels, particularly levels 1-6 which is the left side of the max-level histogram, there is much more data since more games have reaches these level. Because it seems that the most amount of max level achieve is six, this should also mean that more game results for level 6 is “incorrect” rather than “correct”, and this can only increase the lower the levels from that point. For the higher levels, from 7-10 which is the right side of the max-level distribution, we can see an increase and that is because there are much less data entries, meaning the denominator is much lower when calculating percentage. For example, if there were two games that reached level 10, with one passing it to level 11, then there would only be a 50 percent success rate. However, in order to get to level 10, you have to pass level 9, 8, and 7, which means these have more data, meaning higher denominator with a small numerator still, resulting in a lower success rate.\nNow that we have all three graphs and have created a png file by using write_image for each plotly graph, we can render the stats.html with these three PNG files to produce the following stats page.\n\n\n\nStats.png\n\n\nAnd with all three components completed, we are done with our project!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Group Project: GUI Pygame and Web Browser\n\n\n\n\n\n\nWeek 10\n\n\nFinal Project\n\n\n\n\n\n\n\n\n\nMar 18, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 6: More TensorFlow Modeling (Fake News Classification)\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5: TensorFlow and Keras Modeling (Image Classification)\n\n\n\n\n\n\nweek 8\n\n\nHomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Numpy & Matrix-Vectors (Heat Diffusion)\n\n\n\n\n\n\nWeek 7\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: Web Development\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Web Scraping (Movie Recs)\n\n\n\n\n\n\nWeek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Database Visualization (Climate Change)\n\n\n\n\n\n\nWeek 2\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0: Data Visualization\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Homework 0: Data Visualization",
    "section": "",
    "text": "In this blog, I will use the Palmer Penguins dataset to show how to create interesting data visualization. There are many visuals that we can use for the data set like scatterplot, histogram, or boxplot, but for today, we will use scatterplots.\n\nPrepping Dataframe\nTo access the dataframe, we first have to read in the csv file using pandas. Thus, we need to import the pandas library in order to work with data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)     \n\nNext, we want to visualize the data structure to see what type of information we are working with. We can use the head function of pandas to access the first five entries.\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nFollowing standard pandas operations, we want to clean up our data a little bit, removing any “N/A” entries and shortening the names.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])  # Removing penguins with NaN entries in Sex or Body Mass\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)   # Only getting the first word of the Species entry (i.e. species type)\ncols = [\"Species\",                 # Specifying the columns we want to look further\n        \"Island\",                  # into from the penguins data set\n        \"Sex\", \n        \"Culmen Length (mm)\", \n        \"Culmen Depth (mm)\", \n        \"Flipper Length (mm)\", \n        \"Body Mass (g)\"]\npenguins = penguins[cols]          # Choosing the columns we want our data frame to consist of\n\nNow, let’s take a look at the new simplified data set.\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\nNote that there were additional column information which we are going to ignore for today.\nOnce the data set is cleaned, we can start creating our data visualizations.\n\n\nSeparating Dataframe\nBecause this data set consists of different types of penguin species, we need to figure out how many species types we are dealing with and what their names are so that we know what to call in our code. We can use the value_counts function in the pandas library to do the following:\n\npenguins[\"Species\"].value_counts()  # Counting the number of penguins be each Species type\n\nSpecies\nAdelie       146\nGentoo       120\nChinstrap     68\nName: count, dtype: int64\n\n\nFrom this, we know that the penguins in this data set are classified as one of the three following species type: Adelie, Gentoo, or Chinstrap.\nNow, we can start writing our code to create the scatterplot. First, we need to separate the penguin types and create individual data frames for each of the three species. To do so, we can use boolean indexing. We can use a == statement to return a True or False value indicating whether or not a specific entry is of the indicated penguin species\nFor example, to find only the Gentoo penguins, we will say penguins[\"Species\"]==\"Gentoo\" as our conditional statement for including an into the created data frame. This pattern will help us create our three separate data frames as follows:\n\n# Generating the separate dataframes based on Species\nAdelie_dataset = penguins[penguins[\"Species\"] == \"Adelie\"]\nGentoo_dataset = penguins[penguins[\"Species\"] == \"Gentoo\"]\nChinstrap_dataset = penguins[penguins[\"Species\"] == \"Chinstrap\"]\n\nLet’s check to see if this successfully separated the penguins by species.\n\nAdelie_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nAdelie\nDream\nFEMALE\n36.6\n18.4\n184.0\n3475.0\n\n\n148\nAdelie\nDream\nFEMALE\n36.0\n17.8\n195.0\n3450.0\n\n\n149\nAdelie\nDream\nMALE\n37.8\n18.1\n193.0\n3750.0\n\n\n150\nAdelie\nDream\nFEMALE\n36.0\n17.1\n187.0\n3700.0\n\n\n151\nAdelie\nDream\nMALE\n41.5\n18.5\n201.0\n4000.0\n\n\n\n\n146 rows × 7 columns\n\n\n\n\n\nGentoo_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n220\nGentoo\nBiscoe\nFEMALE\n46.1\n13.2\n211.0\n4500.0\n\n\n221\nGentoo\nBiscoe\nMALE\n50.0\n16.3\n230.0\n5700.0\n\n\n222\nGentoo\nBiscoe\nFEMALE\n48.7\n14.1\n210.0\n4450.0\n\n\n223\nGentoo\nBiscoe\nMALE\n50.0\n15.2\n218.0\n5700.0\n\n\n224\nGentoo\nBiscoe\nMALE\n47.6\n14.5\n215.0\n5400.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\nGentoo\nBiscoe\nFEMALE\n47.2\n13.7\n214.0\n4925.0\n\n\n340\nGentoo\nBiscoe\nFEMALE\n46.8\n14.3\n215.0\n4850.0\n\n\n341\nGentoo\nBiscoe\nMALE\n50.4\n15.7\n222.0\n5750.0\n\n\n342\nGentoo\nBiscoe\nFEMALE\n45.2\n14.8\n212.0\n5200.0\n\n\n343\nGentoo\nBiscoe\nMALE\n49.9\n16.1\n213.0\n5400.0\n\n\n\n\n120 rows × 7 columns\n\n\n\n\n\nChinstrap_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n152\nChinstrap\nDream\nFEMALE\n46.5\n17.9\n192.0\n3500.0\n\n\n153\nChinstrap\nDream\nMALE\n50.0\n19.5\n196.0\n3900.0\n\n\n154\nChinstrap\nDream\nMALE\n51.3\n19.2\n193.0\n3650.0\n\n\n155\nChinstrap\nDream\nFEMALE\n45.4\n18.7\n188.0\n3525.0\n\n\n156\nChinstrap\nDream\nMALE\n52.7\n19.8\n197.0\n3725.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n215\nChinstrap\nDream\nMALE\n55.8\n19.8\n207.0\n4000.0\n\n\n216\nChinstrap\nDream\nFEMALE\n43.5\n18.1\n202.0\n3400.0\n\n\n217\nChinstrap\nDream\nMALE\n49.6\n18.2\n193.0\n3775.0\n\n\n218\nChinstrap\nDream\nMALE\n50.8\n19.0\n210.0\n4100.0\n\n\n219\nChinstrap\nDream\nFEMALE\n50.2\n18.7\n198.0\n3775.0\n\n\n\n\n68 rows × 7 columns\n\n\n\n\nGreat! Now all we have left is the scatterplot.\n\n\nCreating Visualization\nIn our examples, we will be using the matplotlib, a common library used for data visualizaiton. So, we need to import the matplotlib library.\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nFor matplotlib, the syntax for creating a scatterplot using a dataset is plt.scatter(df[x],df[y]) where df is the dataset name and x,y are the independent and dependent columns, respectively, that we want to work with. For our example, we will use Culmen Length and Flipper Length. So using Adelie, we will use the following line of code:\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"])\nBecause we are also working with three different data frames, meaning we will have three separate scatterplots on one plot, we can also add a label in our line of code. This way, when we combine all of them into one figure, we can use a legend to help us differentiate each data set. Thus, our code for each species data set will look like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n\n\n\n\n\n\n\nNow that we have out scatterplot, we can add features to make the graph more clear. To add the main title, we can use plt.title(). For the the x and y titles, it will be plt.xlabel() and plt.ylabel(), respectively. And finally, to create our legend, we will use plt.legend(). Once we added all this, our figure may look something like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n# Adding the title, x and y axes labels, and legend\nplt.title(\"Culmen Length vs. Flipper Length\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\n\n\n\n\n\n\n\n\nWith this completed scatterplot, we can finally analyze the two data columns. From this, it appears that for all species, there is a moderate to strong positive correlation between Culmen Length and Flipper Length. Also, it appears that these two may be good candidate factors in differentiating the species type as there are three relatively isolated clusters. And with that, we have successfully created our data visualization."
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Homework 1: Database Visualization (Climate Change)",
    "section": "",
    "text": "In this blog, we will show how to work with databases in order to create interesting and interactive data graphics. We want to utilize databases when dealing with data sets that are incredibly large to the point of being impratical or inefficient storing the data.\n\nCreating Database\nBecause we are dealing with data, we will import the typical libraries involving data like pandas and numpy. In addition, we will import a library called sqlite3 which will let us worth with SQL as well as databases in Python.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have imported sqlite3, we will use the connect function to create the database in the curent directory.\n\n# Creating database in current directory we named data.db\nconn = sqlite3.connect(\"data.db\")\n\nAfter we have created our database, we now want to create its tables, which corresponds to the csv files that we are working with. For our example, we will create three tables that will be labeled temperatures, stations, countries.\nNext, we want to read in out data. Because we have a large data set, instead of reading it in all at once, we want to read in chunks of the data one at a time. We will do this by using the keyword chunksize, which returns an iterator that reads in the number of rows from the data equal to chucksize once we start querying the iterator. It should look something like follows:\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\nOnce we have our table, we want to create a function that will clean our data (as we will repeat this for the other two tables). In our data, each column is representative for each month of the year. Thus, we want to use the stack() function of pandas to create one column for the month data.\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nWith our function, we are ready to read in and populate the temperature table in our database. To do this, we want to use df.to_sql() which will write to the specified table (in our case, the conn object we created earlier). In addition to make sure each piece is added to the table and nothing is overwritten after each iteration, we will use if_exists.\n\n# Reading in data for the temperature table\nfor df in df_iter:\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"append\", index = False)\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\n\n\n1\nUSW00014924\n2016\n2\n-8.40\n\n\n2\nUSW00014924\n2016\n3\n-0.20\n\n\n3\nUSW00014924\n2016\n4\n3.21\n\n\n4\nUSW00014924\n2016\n5\n13.85\n\n\n\n\n\n\n\n\nAnd with that we have finished the temperature table of our database. Now, we want to repeat this for the station table.\n\n# stations table \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.head()\n\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\nNote with this current data set, there is no common column between our countries data and stations data. Thus, to match the two, we will add a new column to this data frame labeled CODE and represents the first two letters of the ID column.\n\nstations[\"CODE\"] = stations[\"ID\"].str[:2]\nstations.head()\n\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\nCODE\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\nAC\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\nAE\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\nAE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\nAE\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\nAE\n\n\n\n\n\n\n\n\nNow, we can create and fill the stations table of the database using the same to_sql() method.\n\n# Creating the stations table\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n27585\n\n\nWe will repeat this process one more time for our countries table now.\n\n# Creating the countries table\nurl = \"countries.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\n279\n\n\nNow we have a database containing three tables. Let’s just check that this is indeed the case.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nGreat! We officially have our three tables.\n\n\nWriting Query Function\nWe are now ready to construct the query function. First, let’s examine what type of data we are working with for each table.\n\ncursor = conn.cursor()\ncursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT,\n  \"CODE\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nNote that the temperatures and stations tables share the ID column while the stations and countries tables have matching entries under the CODE and FIPS 10-4, respectively\nWith this information, we want to create a function that will incorporate SQL in order to select the desired output data as well as matching up the tables and implement specified conditions. It should look something like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C ON SUBSTR(S.id, 1, 2) = C.[FIPS 10-4]\n        WHERE C.Name = ? AND T.Year BETWEEN ? AND ? AND T.Month = ?\n        \"\"\"\n\n        df = pd.read_sql_query(cmd, conn,params = (country, year_begin, year_end, month))\n        df = df.rename(columns = {\"Name\"  : \"Country\"})\n        return df\n\n\n\nNow we have our query function. Let’s see if it is outputting our desired data set using a randon test case. For our example, we will to retrieve data from India in the month of January from 1980 to 2020.\n\nquery_climate_database(db_file = \"data.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n47275\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n47276\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n47277\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n47278\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n47279\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n47280 rows × 7 columns\n\n\n\n\nGreat! Our query function is returnning exactly what we wanted.\n\n\nWriting Geographic Scatter Function\nWith our basic query function down, we will now diverge into different examples of them. The first example we will make is a scatter function which uses data to create scatterplot.\nFor this example, we will be using the plotly library which allows us to create interactive data visualization, so we will need to import that.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nFirst, we need to create a coeff() function that will perform linear linear regression on the data, using the years as the independent variable and the temperatures as the dependent variable. Because we are dealing with linear regression, we will also need to import it from the sklearn library. Also, in order to read the numbers as months, we will import calendar.\n\nfrom sklearn.linear_model import LinearRegression\nimport calendar\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # Creating a dataframe consisting of the years\n    y = data_group[\"Temp\"]   # Creating a string of the temperatures\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nWith this function, we can now create a function, which we will call temperature_plot() to create our plots. Because we already have a query function to create our desired dataframe, we are going to call it in our function to make it simpler.\n\ndegree_sign = u'\\N{DEGREE SIGN}' # Syntax for the degree symbol\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    This function creates a scatterplot figure for data on yearly temperature \n    increase for constraints specified by the user's parameter inputs.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_obs: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the different stations and their annual\n    temperature increase given a specified country, year range, and month of the year.\n    '''\n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n        \n    coeff = df.groupby([\"NAME\",\"LATITUDE\",\"LONGITUDE\",\"Country\",\"Month\", \"Station_Counts\"]).apply(coef) # Calculating annual change in temperature\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : f\"Estimated Yearly Increase ({degree_sign}C)\"}) # Renaming column\n    \n    coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"] = coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"].round(4) # Setting number of sig figs\n    \n    return px.scatter_mapbox(coeff,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_name = \"NAME\",\n                        color = f\"Estimated Yearly Increase ({degree_sign}C)\",\n                        title = f\"Estimates of Yearly Increase in {calendar.month_name[month]} for Stations in {country} ({year_begin} - {year_end})\",\n                        **kwargs)\n\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nLet’s also see how the United States looks like with the same time settings!\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"United States\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\nMore Figures / Data Visualisations\nTo end, we will create two more plots. Because our data involves temperature, we will create figures based on climate comparisons.\nBecause a common researched aspect of climate change is highly variable weather (meaning it is expected that weather of both extremes become more similar as time progresses), we will first create a plot looking at countries’ variance in temperature over time across its stations. Again, we can call the query function to create our dataframe.\n\ndef temperature_variance_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    This function creates a scatterplot figure for data on yearly temperature standard \n    deviation change for all the stations of a country specified by the user.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_occ: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the annal tempearture standard deviation change\n    given a specified country, year range, and month of the year.\n    \"\"\"\n    \n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    \n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n    \n    SD = df.groupby([\"NAME\",\"Year\"])[\"Temp\"].std() # Calculating annual standard deviation\n    SD = SD.reset_index() #Reformatting data frame\n    \n    coeff = df.groupby([\"NAME\"]).apply(coef)  # Calculating annual change in standard deviation\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : \"Correlation Coeff R\"}) # Renaming column\n    \n    \n    return px.scatter(data_frame = coeff,   \n                 y = \"Correlation Coeff R\", \n                 title = f\"Annual Standard Deviation Changes of Stations in {country}({year_begin} - {year_end})\",\n                 hover_name = \"NAME\",\n                 hover_data = [\"Correlation Coeff R\"])\n\nWith this function, let’s look into the variance of the US’s temperature in March from 1980 to 2020\n\nfig = temperature_variance_plot(\"United States\", 1980, 2020, 3, min_obs = 3)\nfig.show()\n\n\n\n\nFrom this figure, we can see that because the R-value, representing the average annual change, is mostly around 0, it means that there has been no positive or negative trend in terms of temperature variance. Note that this is for the US only, which means this trend should not be applied to other countries as well.\nFor our second one, let’s examine the climate/weather of two geographically similar and close countries. For this, we will need to use histograms to look at their temperature distributions. Unlike the other two, since we want our dataframe to incorporate two different countries, we will have to use a different cmd.\n\ndef temperature_histogram_plot(country1, country2, year_begin, year_end, **kwargs):\n    \"\"\"\n    This function generates two histograms of countries' \n    temperature distribution specified year range\n    \n    Parameters:\n    ----------\n    country1: string giving the name of the first country \n    country2: string giving the name of the second country\n    year_begin: integer representing the earliest year the output includes\n    year_end: integer representing the latest year the output includes\n    \n    Return:\n    ----------\n    A plotly normalized histograms\n    \"\"\"\n    \n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON S.code = C.\"FIPS 10-4\"\n    WHERE C.name = \"{country1}\" OR C.name = \"{country2}\" AND T.year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns = {\"Name\" : \"Country\"})\n    \n    return px.histogram(df,\n                  x = \"Temp\",\n                  opacity = 0.5,\n                  nbins = 30,\n                  histnorm = \"percent\", # Normalizing histograms in case the countries are largely different in counts\n                  barmode = \"stack\", \n                  facet_col = \"Country\",\n                  labels={\n                     \"Temp\": f\"Temperature ({degree_sign}C)\",\n                     \"count\": \"Normalized Counts\"\n                  },\n                  title = f\"Normalized Temperature Distributions of {country1} and {country2} from years {year_begin} - {year_end}\",\n                  **kwargs)\n\nNote that this is creating normalized histogram distibutions to prevent unfair visual comparison. Now we can test our function using two close countries, like India and Pakistan.\n\nfig = temperature_histogram_plot(\"India\", \"Pakistan\", 1980, 2020)\nfig.show()\n\n\n\n\nFrom this figure, we can see how India has a a larger temperature range, from -8 to 38 degrees Celsius, compared to Pakistan, from 0 to 38 degrees. In addition, it also has a higher peek than Pakistan. However, Pakistan’s peak is father away than India, peaking at 30 to 32 degrees while India peaked at 26-28 degrees. We can also not that both left-skewed. Thus, while there is a similarity between the two countries, there are also major differences in terms of climate. This makes sense since India is a much more geographically large countyry compared to Pakistan, which would mean it should hve more variance in temperature across different stations. In addition, India has more coastal regions and also takes up majority of the Himilayas mountains, resulting in colder temperatures than Pakistan which has less coastal regions and only takes up five percent of the mountains.\nNow that we have created all our figures, let’s make sure to close the database connection.\n\nconn.close()"
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Homework 3: Web Development",
    "section": "",
    "text": "In this blog post, we will learn how to create simple webapp via Flask, a web framework that provides useful tools and features that make creating web applications in Python a lot easier. For our example, we will creating a message bank which allows a user to either submit a message or view previous message entries.\n\nSetting up Flask\nThe first thing we need to do is setup flask. So, we will go to our virtual environment and then install flask. For Windows, the two line of code will be: conda activate {environment name} and pip install flask. Then to help with our website creation, we want to set our Flask environment to “development” through export FLASK_ENV=development which enables debug mode and gives more specific error messages.\nNow, we can start with the code files. First, in order to tell the Flask where our application is, we will create a python file called app.py. In this file we will need to import the flask python library as well as some of its tools. First, we need Flask class to run the application. We are also using render_template() function in order to generate output from a template file based on the Jinja2 engine that is found in the application’s templates folder. Next, we import request which creates a Request object that allows you to access the data passed into your Flask application. Lastly, we need g which acts as a namespace object to store common data during a request.\nfrom flask import Flask, render_template, request, g\nNext, we want to create a Flask application object and set up the Flask application to know where to look for templates and static files based on the location of the current Python module. We can do that with the code below:\napp = Flask(__name__)\nLastly, in order to run the the Flask application we want to use the line below:\nif __name__ == '__main__':\n    app.run(debug=True)\nBy saying if_name_ == '__main__', we are ensuring that the application only starts/runs when the script is executed directly and not when imported as a module in another script. This line will be the last in the python file\n\n\nCreating Website Functions\nNext up, depending on the goals of your website, we want to create some functions that we will call later when specifying the purpose of each website. For our purposes, because we want to have a website to submit and view messages, we will create three different functions.\nFor the first function, we want to create a function that should handle creating the database of messages (thus, we should also import sqlite3). This function should be able to 1) check whether there is a database in the g attribute of the app and connect to the database if there isn’t. Thus, our code should look something like this:\ndef get_message_db():\n    try:\n        return g.message_db                                       # Returning message database from attribute g                               \n    except AttributeError: \n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")      # Creating connection to database file\n        cursor = g.message_db.cursor()                            # Creating cursor object to execute SQL\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                          id INTEGER PRIMARY KEY,\n                          handle TEXT, \n                          message TEXT)''')                       # Creatng message table if it doesn't exist yet\n        g.message_db.commit()                                     # Commiting changes to database\n        return g.message_db\nSecond, we want to create a function for dealin with submitting message. This is where we will use the request object us to pass data through flask. In addition, because we are dealing with databases, we will want to write and SQL Query that will help us execute inserting the data into the messages table. Thus our code should look something like this:\ndef insert_message(request):\n    message = request.form['message']                             # Extracting messages from request form\n    handle = request.form['handle']                               # Extracting handle from request form\n    \n    db = get_message_db()                                         # Getting message database\n    cursor = db.cursor()                                          # Creating cursor object to execute SQL\n    \n    SQL_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"  # SQL query to insert data     \n                                                                        # into messages table \n        \n    cursor.execute(SQL_query, (handle, message))                  # Executing SQL query with handle \n                                                                  # and message as its parameters\n    \n    db.commit()                                                   # Commiting changes to database\n    db.close()                                                    # Closing database connection\nLastly, we need a function for the viewing aspect of our website. The function will have similar aspects of the insert_message function regarding getting the message database, using SQL query, and executing cursor. However, because we also want previous submitted messages, we will use the fetchall() method to retrieve all rows of a query result set (in our case, the selected messages). Again, our code should look something like this:\ndef random_messages(n):\n    db = get_message_db()                                         # Getting Message database\n    cursor = db.cursor()                                          # Creating cursor object to execute SQL\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))  # Select random messages \n                                                                                                # from the database\n    messages = cursor.fetchall()                                  # Fetching all selected messages\n    db.close()                                                    # Closing database connection\n    return messages   \n\n\nCreating HTML Templates\nBecause we are using render_template() function, we will also need to create HTML templates in order to specify what each page will do on the website.\nFirst, we want to create a base.html which will act as the initial structure/format of our website, and to do this we want to use &lt;!doctype html&gt; to declare HTML document type. This HTML file will send us to different links, so we will need to use &lt;nav&gt; which declares a navigation section to provide navigation links to other documents. Because we want to have two different ‘buttons’ to send to different links, one for submitting and one for viewing messages, we want to use a href which will denote a hyperlink from one web address to another and also denotes a hyperlink from one web address to another. In addition, because this html is acting as the starting page which will send us to the other pages, we will want to have additional html files that we can reference to in this base.html. Thus we will use the url_for() function to generate the URL to a view based on a name and arguments. It should look something like this:\n&lt;!doctype html&gt;   &lt;!-- HTML document type declaration --&gt;\n\n&lt;!-- Link to a stylesheet (style.css) in the 'static' directory --&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n\n&lt;!-- Title of the webpage, with a block for inheritance --&gt;\n&lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n\n&lt;nav&gt;    &lt;!-- Navigation section --&gt;\n  &lt;h1&gt;A Simple Message Bank&lt;/h1&gt; &lt;!-- Heading --&gt;\n  &lt;ul&gt;   &lt;!-- Unordered list of navigation links --&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;   &lt;!-- Link to the 'submit' route --&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;        &lt;!-- Link to the 'view' route --&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;    &lt;!-- Content section --&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}   &lt;!-- Block for inheritance to allow overriding of header content --&gt;\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nThere are two things that we want to take note of in this clock of code. The first is the line &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; which is used to include external CSS file (in ur case static) into the document. The second thing is the bottom chuck of code &lt;section class=\"content\"&gt;. By writing this line, we are allowing for template inheritance, meaning future extension templates (in our case, it will be the submit and view HTML files) will be able to override and header content here.\nNow that we have the base HTML down, we want to create our extension files, starting with the submit.html.\nFirst thing is that since this extends the base HTML , we will need to indicate by saying {% extends 'base.html' %}. For this page, because we want user input, &lt;form method=\"post\" action=\"/submit\"&gt;. form method indicates the starm of an HTML form element, ‘POST’ indicates we that the form will use the ‘POST’ method for submission, and action='/submit' indicates sending this data to the route called ‘/submit’.\nIn our form method, we want to create the input field as well as its specifying label which we can achieve through label type and label for, respective (since we want fields, we will use “text” as our input type. Make sure that for each id attribute, you put the variable name specified in your insert_message function as this is what the function will rely on.\nLastly, we want a submit button which will send the form data to the route by using &lt;input type=\"submit\" value=\"Submit\"&gt;. Putting this all together, our code will look something like this:\n{% extends 'base.html' %}    &lt;!-- Extension of base.html template --&gt;\n\n{% block title %}            &lt;!-- Override the title block --&gt;\nSubmit Message               &lt;!-- Title for this page --&gt;\n{% endblock %} \n\n{% block content %}          &lt;!-- Override the content block --&gt;\n&lt;h1&gt;Submit a Message&lt;/h1&gt;\n&lt;form method=\"post\" action=\"/submit\"&gt;    &lt;!-- Form for submitting a message, POST method to \"/submit\" route --&gt;     \n    &lt;label for=\"handle\"&gt;Your Name:&lt;/label&gt;               &lt;!-- Label for the input field \"handle\" --&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;    &lt;!-- Input field for entering the name --&gt;\n    &lt;label for=\"message\"&gt;Your Message:&lt;/label&gt;           &lt;!-- Label for the input field \"message\" --&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;  &lt;!-- Input field for entering the message --&gt;\n    &lt;input type=\"submit\" value=\"Submit\"&gt;                 &lt;!-- Submit button --&gt;\n&lt;/form&gt;\n{% endblock %}\nThe next file that we want to create now is the view.html for the viewing aspect of our website. Similar to submit.html we will need {% extends 'base.html' %} to indicate an extension. However, since this wont require user input, we will not using form method. Instead, since we are looking to output the messages and handle, we will be using a for loop that will iterate through the rows of the ‘message’ database and returning a list item for each ‘message’, first item being the handle and second item being the message content. Putting these two factors together, our code will look something like this.\n{% extends 'base.html' %}                        &lt;!-- Extend base.html template --&gt;\n\n{% block title %}                                &lt;!-- Override the title block --&gt;\nView Messages                                    &lt;!-- Title for this page --&gt;\n{% endblock %}\n        \n{% block content %}                              &lt;!-- Override the content block --&gt;\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n{% for message in messages %}                    &lt;!-- Loop through the messages --&gt;\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;  &lt;!-- List item displaying each message --&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\n\nApp Routing\nNow we will start working on the routing part of our website.\nThe first one we need will be for the homepage. To specify the routing of different pages on a website, we will be using @app.route() each time, and in the () will be the unique tag to identify that page. Because we are starting with the homepage, we will first put '/' inside. Next, since we have a HTML file that specifies the formatting of our homepage, we will create a function to call that file using render_template(). And so, our code will look like this:\n@app.route('/')\ndef main():\n    return render_template('base.html')                           # Rendering base.html template\nNext, we will specify the routing to the submit page. Similar, to homepage, we will need @app route(), but this time we will put '/submit' in the ().\nAdditionally, since our submit.html uses form method, we will add ‘methods’ parameters to tell to the application that this route handles both different requests. Thus, we will need an if-else statement in our function to specify different things. First, we want to check if the request is specifically a ‘POST’ request, and if so, the form on the submit.html was submitted, which means we will need to call the insert_message function with request to send the content into the database and then redirect to the same page with a fresh start. Our code will look something like this:\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':                                  \n        insert_message(request)                                   # Inserting message into database\n        return render_template('submit.html')                     # Rendering submit.html template\n    else:\n        return render_template('submit.html')\nLastly, we need to route to the viewing page. Again we do @app.route('/view') this time. The only aspects left we haven’t used is the view.html and the random_message function. So in our function, we will call our function with any quantity parameter (for this case, we will use 5) to retrieve random messages from our database and then assign it to a variable. With this variable, we will render the view.html while passing our variable to it, giving us the displayed messages. So, our code will look like this:\n@app.route('/view')\ndef view():\n    messages = random_messages(5)                                 # Retrieving 5 random messages from database\n    return render_template('view.html', messages=messages)        # Rendering view.html with fetched messages\nWith these three app routings, we have finished creating all the pages of our website!\n\n\nCustomizing App\nThere are many ways to customize your webapp. The way we will teach is by using CSS, a style sheet language that helps with the styling of a document written in a markup language (in our case, HTML). For the purpose of this blog, we will be changing the font and the color. In our base.html, we ran a line saying &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;, which means that it is looking for a folder caled static and in there a file named style.css, so we first need to create these two components (within the css file is where we will write our specifications).\nTo specify the font, we will use the line font-family:{font_name_1}, {font_name_2} to change from the default font (we added {font_name_1}, {font_name_2} to specify a different option in case our first font doesnt exists.\nTo Specify the text color, we say color: #{number} which tells the aplication that all texts will be set this color-associated number.\nThere are other ways to make changes to the website which can be seen by our code below:\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #f0f0f0;\n}\nh1 {\n    color: #333;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #fff;\n    padding: 10px;\n    margin-bottom: 5px;\n}\nOnce we have all this, we can go to our terminal, change directory to that specific folder and then use flask run to run our application and start our website. If successful, you should get your website to look something like this:\n\n\n\nPIC16B-HW3-PIC1-2.png\n\n\n\n\n\nPIC16B-HW3-PIC2-3.png\n\n\n\n\n\nPIC16B-HW3-PIC3-3.png\n\n\nNote that with each page, the URL changes with the unique tag we specified in @app.route(). For the codes and files to our website, you can go to https://github.com/Coding-Tom-1405/HW-3 for more details.\nAnd with that we are done with our website!"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Homework 5: TensorFlow and Keras Modeling (Image Classification)",
    "section": "",
    "text": "In this blog post, we will go over how keras layering and tensorflow to create different models for a dataset. For our example, we will be using image classification to determine the specific type of animal the image portrays: cat or dog.\n\nLoading Packages\nTo start, we need to import and load all our needed packages.\nFirst, because we will be utilizing Keras 3 in order to work on top of our TensorFlow backend, we need to upgrade the Colab’s default version of 2.15.0 to get version 3.0.5\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 8.8 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\nWith this, we can import all the packages and establish our backend:\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\nimport tensorflow as tf\nimport random\n\nNow that we imported the packages, let’s check the version of keras\n\nkeras.__version__\n\n'3.0.5'\n\n\nNext, since we are creating Keras backends and TensorFlow models, we will incorporate GPUs (or graphics processing units) to accelerate the computation.\nYou’ll need to enable GPUs for the notebook:\n\nNavigate to Edit→Notebook Settings\nselect GPU from the Hardware Accelerator drop-down\n\nAfter, we’ll confirm that we can connect the GPU with jax:\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\nLet’s check our GPU usage\n\n!nvidia-smi\n\nMon Mar 11 02:47:39 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0              25W /  70W |    105MiB / 15360MiB |      3%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\n\nObtaining Data\nNow that we have finished loading our packages, we can start creating our datasets using tensorflow.\nSimilar to when using numpy in the past, we will split our data into groups, except this time we will have three of them: train, test, and validation. The reason we have a validation dataset is because we are training multiple models that have different combinations of hyperameters. Thus, we need a common factor that will help evaluate and compare the performance of each model through the validation set.\nFor our code, we will split it we will do 40:10:10 (remaining unused), so our code should look something like this:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, to ensure consistent sizing, we want to resize and establish the expected dimension of all our images. We can accomplish this by using the layers object and Resizing() function in keras as follows:\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nLastly, we want to set up our data piplines for training, validation, and testing (in order to optimize efficiency of loading and preprocessing our data), which can be done as seen below:\n\n# Defining number of samples processed in each batch for training\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nOnce our batches of data are complete, let’s see what our images look like (checking to see if they are consistent, yet still clear). We can do this by creating a function that will take in our dataset and output the desired-dimensions subplot)\n\ndef ds_visual(dataset, num_rows, num_cols, n):\n\n    # Creating plot\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # Selecting a single batch\n    for i, (images, labels) in enumerate(dataset.skip(n).take(1)):\n        # Initializing counters for dog,cat appearances in plot\n        cat_count = 0\n        dog_count = 0\n\n    # Iterating through the batch of image,label pairings\n    for image, label in zip(images, labels):\n        # Selecting only the car images\n        if label == 0 and cat_count &lt; num_cols:\n            # Plotting cat image\n            axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[0, cat_count].set_title('Cat')\n            axes[0, cat_count].axis(\"off\")\n            cat_count += 1\n\n        # Selecting only the car images\n        elif label == 1 and dog_count &lt; num_cols:\n            # Plotting dog image\n            axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[1, dog_count].set_title('Dog')\n            axes[1, dog_count].axis(\"off\")\n            dog_count += 1\n\n        # Stopping looop once all subplots filled\n        if cat_count == num_cols and dog_count == num_cols:\n            break\n\n    plt.show()\n\nWith this function, we can see what our images look like.\n\nds_visual(train_ds, 2, 3, random.randint(1,10))\n\n\n\n\n\n\n\n\nPerfect! We have exactly one row of three cats and one row of three dogs, and all of these images are of the same dimension.\nBefore moving on to our training models, let’s first check that our dataset is fair. What we mean by that is because we split the dataset without knowing the distribution of the two species, we need to see if their frequencies are relatively even. Thus, we can check the distribution with the following two codes:\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (cat):\", cat_count)\nprint(\"Number of images with label 1 (dog):\", dog_count)\n\nNumber of images with label 0 (cat): 4637\nNumber of images with label 1 (dog): 4668\n\n\nGreat! It is even.\nTo have a starting comparing condition, we should first talk about the most basic predicting model: the baseline model.\nBecause the baseline model always predicts the most occurring label (in our case, dog), then the baseline model would only have an accuracy rate of 4668/(4637+4668)%, or 50.2%. Again, this makes sense in that because our dataset is basically proportional, then the baseline ultimately has a 1 in 2 chance of predicting the right label, dog or cat.\nWith that understanding and starting place, let’s jump into creating the models!\n\n\nFirst Model (Basic Model Structure)\nFor our first one, we will start of by introducing the most basic component when building a model.\nTo start off, we need to specify the expected shape of our data, which will require us to put layers.Input() at the beginning of our model.\nNext, with all of our models, we want to extract “features” (meaningful properties) from each images. We can do this by using Conv2D() which adds 2D convolutional layer in our neural network models to magnify the any specific textures or patterns to help with classification.\nWith a component helping magnify features, we also want a component that will help remove irrelavant data. This is done through MaxPooling2D which retains the max-values pixels in different regions while reducing the spatial dimensions of the image (Typically models will have this and Conv2D as alternating layers).\nAfter finishing our “2D” components, we now need to Flatten our data to 1D so that we can pass it through a Dense layer which will make the prediction for us. In addition, to prevent overfitting, we will also want to incorporate a Dropout layer (turning certain amount of input units to 0) at times\nWith all this info, our model should look something like this:\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nThe main thing we experimented with is 1) dropout value and 2) and maxpooling value as both of these have to do with reducing spatial dimension which impacts how much important feature remains.\nTo further understand what our model is doing, let’s take a look at what is happening to our image data at each step.\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAnd with that, let’s start configuring the training process for our neural network model. To do this, we need to specify what optimization algorithm, loss function (how well model’s predictions match the true labels), and evaluation metric during training. Thus, our code will look something like this:\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nLastly, all that is left to do is to train it. To ensure consistency and/or improvement of our model, we will use the epoch parameter to indicate how many times our model will go through the entire training dataset. Thus, our code will be as follows:\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 90ms/step - accuracy: 0.4883 - loss: 23.6040 - val_accuracy: 0.5537 - val_loss: 0.9793\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 33ms/step - accuracy: 0.5134 - loss: 1.4983 - val_accuracy: 0.5666 - val_loss: 0.7251\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 38ms/step - accuracy: 0.5507 - loss: 1.3817 - val_accuracy: 0.5997 - val_loss: 0.7705\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5766 - loss: 1.3752 - val_accuracy: 0.6028 - val_loss: 0.7648\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5961 - loss: 1.3353 - val_accuracy: 0.5993 - val_loss: 0.7467\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6193 - loss: 1.2841 - val_accuracy: 0.5903 - val_loss: 0.9021\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6491 - loss: 1.2248 - val_accuracy: 0.6165 - val_loss: 0.8251\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.6811 - loss: 1.1777 - val_accuracy: 0.6092 - val_loss: 0.8820\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7094 - loss: 1.1061 - val_accuracy: 0.6002 - val_loss: 1.0495\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7332 - loss: 1.0746 - val_accuracy: 0.6079 - val_loss: 1.0947\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7657 - loss: 1.0373 - val_accuracy: 0.6071 - val_loss: 1.0857\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7786 - loss: 1.0003 - val_accuracy: 0.5959 - val_loss: 1.2514\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.7966 - loss: 0.9854 - val_accuracy: 0.6066 - val_loss: 1.3315\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8142 - loss: 0.9666 - val_accuracy: 0.6109 - val_loss: 1.4496\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8063 - loss: 0.9892 - val_accuracy: 0.6032 - val_loss: 1.4400\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8015 - loss: 1.0174 - val_accuracy: 0.6028 - val_loss: 1.7445\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8122 - loss: 0.9848 - val_accuracy: 0.5980 - val_loss: 1.5714\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8057 - loss: 0.9930 - val_accuracy: 0.5954 - val_loss: 1.8500\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8095 - loss: 0.9867 - val_accuracy: 0.5989 - val_loss: 2.4330\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8218 - loss: 0.9689 - val_accuracy: 0.6088 - val_loss: 2.3539\n\n\nThere are two things that we can note from looking at this statistics. First, we can see that the accuracy of my model stabilized between 55.37% and 61.09% during training, meaning our model is 5.17-10.89% better at predicting the label. However, you will notice that there are two different accuracy values in each epoch. One of them is for the validation dataset while the other one accuracy is for the training dataset. Let’s graph to compare what these accuracies look like agains each other\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the the gap between the two variables are getting increasingly larger, with training accuracy values is being greater than the validation accuracy as we go through more iterations. This indicates an overfitting in our model as it is relying heavily on the features in the training dataset, resulting an underperformance in our validation dataset.\n\n\nModel with Data Augment\nWith the basic model structure out of the way, we can now add more specific components to enhance our model’s performance, starting with data augmentation.\nData augmentation refers to the practice of including modified copies of the same image in the training set, meaning that a dog image will always remain to be a dog image even if we decide to flip or rotate it.By adding the transformed “version” of the images, our model can learn invariant features of our data.\nTo achieve this step, all we need to add are the following to layers in our model: layers.RandomFlip() and layers.RandomRotation(). To see these layers in action, we will plot our original image along with a few copies where the layers were applied.\nFirst, the RandomFlip():\n\ndata_augmentation_random_flip = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n\nfor images, labels in train_ds.take(1):\n    # Taking the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# Adding the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plotting the original, unflipped image\nplt.subplot(1, 3, 1)\n# Accessing the first element of the batch dimension\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\n# Plotting the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_flip(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, through the RandomFlip layer, we were able to flip our dog image both horizontally and/or vertically.\nNext, the RandomRotation():\n\ndata_augmentation_random_rotate = tf.keras.Sequential([\n  layers.RandomRotation(0.2) # 0.2 specifies max rotation angle\n])\n\nfor images, labels in train_ds.take(1):\n    # Assuming you want the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# # Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plot the original, unflipped image\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))  # Accessing the first element of the batch dimension\nplt.axis(\"off\")\n\n# Plot the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_rotate(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nFor this, we can see how our dog has been rotated at various angles.\nWith these two new layers, we can add on to our starting model. Since we want our model to first be able to identify whether or not the image has been modulated, we want to make these two augmentation layers the first ones in our models, making our code look like this:\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),  # Flipping augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10),\n    layers.Dropout(0.5)\n])\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n\nAgain, let’s take a look at our summary and start training.\n\nmodel2.summary()\n\nModel: \"sequential_29\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_23 (RandomFlip)          │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_19 (RandomRotation)  │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_63 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_42 (MaxPooling2D)      │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_64 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_43 (MaxPooling2D)      │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_65 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_21 (Flatten)                 │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_42 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_43 (Dense)                     │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (Dropout)                  │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 95s 526ms/step - accuracy: 0.4902 - loss: 21.2286 - val_accuracy: 0.6144 - val_loss: 0.6761\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5317 - loss: 1.4244 - val_accuracy: 0.6432 - val_loss: 0.6785\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 218ms/step - accuracy: 0.5418 - loss: 1.4079 - val_accuracy: 0.6612 - val_loss: 0.6301\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 292ms/step - accuracy: 0.5442 - loss: 1.4034 - val_accuracy: 0.6797 - val_loss: 0.6092\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5717 - loss: 1.3642 - val_accuracy: 0.6922 - val_loss: 0.6192\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5531 - loss: 1.3825 - val_accuracy: 0.6823 - val_loss: 0.6084\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5540 - loss: 1.3716 - val_accuracy: 0.6660 - val_loss: 0.6346\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5635 - loss: 1.3650 - val_accuracy: 0.6930 - val_loss: 0.6010\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5497 - loss: 1.3600 - val_accuracy: 0.7072 - val_loss: 0.5903\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5622 - loss: 1.3492 - val_accuracy: 0.6763 - val_loss: 0.6072\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.5645 - loss: 1.3489 - val_accuracy: 0.6870 - val_loss: 0.5993\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5565 - loss: 1.3696 - val_accuracy: 0.6999 - val_loss: 0.6130\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5767 - loss: 1.3488 - val_accuracy: 0.6892 - val_loss: 0.5941\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 209ms/step - accuracy: 0.5696 - loss: 1.3566 - val_accuracy: 0.6926 - val_loss: 0.6353\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5674 - loss: 1.3447 - val_accuracy: 0.7283 - val_loss: 0.5621\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5713 - loss: 1.3509 - val_accuracy: 0.7356 - val_loss: 0.5911\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5638 - loss: 1.3601 - val_accuracy: 0.7210 - val_loss: 0.5733\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5623 - loss: 1.3656 - val_accuracy: 0.7270 - val_loss: 0.5433\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5904 - loss: 1.3121 - val_accuracy: 0.7395 - val_loss: 0.5454\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5647 - loss: 1.3410 - val_accuracy: 0.7399 - val_loss: 0.5652\n\n\nWow! The accuracy of my model stabilized between 61.44% and 73.99% during training, meaning it was 6.07-18.62% better at predicting the label than our model1 was. And, if we take a look at the chart comparing the training and validation accuracy,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nwe can see that the training accuracy was never higher than the validation accuracy, meaning there were no immediate observations of overfitting happening here, unlike model1.\n\n\nData Processing\nThe third model that we will create will continue building off of the previous models. Now, we will introduce the concept of data processing\nSometimes, itis helpful to make simple transformations to the input data to make training easier for neural network models. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0/-1 and 1 where we scale the weights. However, rather than scaling during training, we can do it prior which will allow the training time/energy more focused on the data content itself.\nThus, we can do that by writing the following block of code:\n\n# Define the preprocessing layer\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs=x)\n\nWith this preprocessor layer, we can slot it into our model pipeline, getting the following code:\n\n# Define the rest of the model architecture\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2),\n])\n\nNow, let’s get our summary and train the model.\n\nmodel3.summary()\n\nModel: \"sequential_4\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_13 (Functional)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_4 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_4 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_12 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_8 (MaxPooling2D)       │ (None, 49, 49, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_13 (Conv2D)                   │ (None, 47, 47, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_9 (MaxPooling2D)       │ (None, 15, 15, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_14 (Conv2D)                   │ (None, 13, 13, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_4 (Flatten)                  │ (None, 10816)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (Dense)                      │ (None, 64)                  │         692,288 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (Dense)                      │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,163,176 (8.25 MB)\n\n\n\n Trainable params: 721,058 (2.75 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 1,442,118 (5.50 MB)\n\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.7971 - loss: 0.4250 - val_accuracy: 0.7932 - val_loss: 0.4449\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8115 - loss: 0.4147 - val_accuracy: 0.8001 - val_loss: 0.4289\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8085 - loss: 0.4122 - val_accuracy: 0.8027 - val_loss: 0.4295\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8054 - loss: 0.4186 - val_accuracy: 0.8035 - val_loss: 0.4274\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8019 - loss: 0.4157 - val_accuracy: 0.7975 - val_loss: 0.4353\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8083 - loss: 0.4125 - val_accuracy: 0.8057 - val_loss: 0.4303\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8173 - loss: 0.4012 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8148 - loss: 0.3978 - val_accuracy: 0.8044 - val_loss: 0.4160\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8191 - loss: 0.3924 - val_accuracy: 0.7966 - val_loss: 0.4294\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8214 - loss: 0.3920 - val_accuracy: 0.8113 - val_loss: 0.4249\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8259 - loss: 0.3842 - val_accuracy: 0.8078 - val_loss: 0.4094\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8268 - loss: 0.3803 - val_accuracy: 0.8151 - val_loss: 0.4329\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8278 - loss: 0.3854 - val_accuracy: 0.8199 - val_loss: 0.4099\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8346 - loss: 0.3733 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8439 - loss: 0.3667 - val_accuracy: 0.8203 - val_loss: 0.3971\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8353 - loss: 0.3675 - val_accuracy: 0.8246 - val_loss: 0.4054\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8362 - loss: 0.3706 - val_accuracy: 0.8250 - val_loss: 0.3923\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8477 - loss: 0.3479 - val_accuracy: 0.8177 - val_loss: 0.4211\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8384 - loss: 0.3572 - val_accuracy: 0.8250 - val_loss: 0.3840\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8438 - loss: 0.3590 - val_accuracy: 0.8186 - val_loss: 0.3964\n\n\nThis one performed even better! The accuracy of my model stabilized between 79.32% an 82.50% during training, meaning it was better than model1 by 23.95-27.13%.\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nHowever, like model2 our training accuracy values are consistently higher than the validation accuracy values, indicating signs of overfitting for model3.\n\n\nTransfer Learning\nFor our last model, we will be focusing on tranfer learning. What does that mean?\nSo far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Thus, we can try to use a pre-existing model for our task.\nTo do this, we need to first access a pre-existing “base model” which we will then incorporate in a full model to train for our specific dataset. Thus, we will first use the following code to download MobileNetV2Large and then configure as a layer to be slotted in our model, similar to preprocessor:\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nFrom this “base model,” all we have to do is add in our audmentation layer as well as a Dense() layer for prediction.\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n    base_model_layer,\n    layers.GlobalMaxPool2D(),\n    layers.Dense(2)\n])\n\nFor one last time, let’s take a look at our summary and then start training.\n\nmodel4.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip (RandomFlip)             │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation (RandomRotation)     │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_1 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 2)                   │           1,922 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nTwo things to note from this summary. The first is that between the base_model_layer, denoted by functional_1, and Dense() layer, there is a major jump in output shape, which is why we added a GlobalMaxPooling2D layer to help the transformation from 2D to 1D (needed for dense layer). The second thing is that compared to the previous models which had hundreds of thousands and even millions of training parameters, this model only has close to 2,000 traininable parameters, making this more efficient.\nWith that, let’s start the training.\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 29s 134ms/step - accuracy: 0.6900 - loss: 3.2898 - val_accuracy: 0.9286 - val_loss: 0.4906\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.8854 - loss: 0.7964 - val_accuracy: 0.9471 - val_loss: 0.3098\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9019 - loss: 0.6175 - val_accuracy: 0.9480 - val_loss: 0.3026\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 51ms/step - accuracy: 0.9022 - loss: 0.5572 - val_accuracy: 0.9527 - val_loss: 0.2610\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9112 - loss: 0.4793 - val_accuracy: 0.9514 - val_loss: 0.2471\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9152 - loss: 0.4250 - val_accuracy: 0.9557 - val_loss: 0.2024\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9059 - loss: 0.4484 - val_accuracy: 0.9587 - val_loss: 0.1757\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 51ms/step - accuracy: 0.9163 - loss: 0.3734 - val_accuracy: 0.9493 - val_loss: 0.2260\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9233 - loss: 0.3127 - val_accuracy: 0.9617 - val_loss: 0.1646\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9254 - loss: 0.2912 - val_accuracy: 0.9488 - val_loss: 0.2356\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 59ms/step - accuracy: 0.9222 - loss: 0.3179 - val_accuracy: 0.9480 - val_loss: 0.2038\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 60ms/step - accuracy: 0.9171 - loss: 0.3419 - val_accuracy: 0.9583 - val_loss: 0.1724\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9225 - loss: 0.2911 - val_accuracy: 0.9626 - val_loss: 0.1437\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9256 - loss: 0.2803 - val_accuracy: 0.9531 - val_loss: 0.1747\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9210 - loss: 0.3063 - val_accuracy: 0.9609 - val_loss: 0.1632\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9290 - loss: 0.2569 - val_accuracy: 0.9536 - val_loss: 0.1656\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9184 - loss: 0.2861 - val_accuracy: 0.9557 - val_loss: 0.1811\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9259 - loss: 0.2598 - val_accuracy: 0.9381 - val_loss: 0.2751\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9207 - loss: 0.3125 - val_accuracy: 0.9463 - val_loss: 0.2146\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9286 - loss: 0.2523 - val_accuracy: 0.9600 - val_loss: 0.1502\n\n\nThis one did the best, constantly hitting over 90’s! The accuracy of my model stabilized between 92.86% and 96.26% during training, meaning our model is 37.29-41.28% better at predicting than model1. Comparing the accuracy values,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nvalidation again is always higher than training, indicating no immediae signs of overfitting for model4.\n\n\nTrying on Test Data\nAgain, out of all of these, it seems that model4 was the most performant model, given it has only 90’s values for validation accuracy and there are no indicators of overfitting.\nNow that we have our best model figured out, we will now run our model against unseen test data.\n\nmodel4.evaluate(test_ds, verbose = 1)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.9487 - loss: 0.1970\n\n\n[0.21008619666099548, 0.9471195340156555]\n\n\nSeems like the model did well! It got about 94.8% accuracy, which falls in the range of the validation accuracy for model4 when training it.\nAnd with that we are done!"
  }
]