[
  {
    "objectID": "posts/HW6/index.html",
    "href": "posts/HW6/index.html",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "",
    "text": "In this blog post, we will go how to create more tensorflow models. However, unlike the last blog post where we utilized a dataframe of images to do image classification, we are going to use a dataframe of articles and performing text classification to determine if an article would be considered “fake news”."
  },
  {
    "objectID": "posts/HW6/index.html#acquire-training-data",
    "href": "posts/HW6/index.html#acquire-training-data",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Acquire Training Data",
    "text": "Acquire Training Data\nFor this task we are going to use something called stopwords. These are widely used words such as “the”, “a”, “an”, etc. that we want to configure our search engine to disregard while indexing. Thus, we can do that by the following:\n\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\nNow let’s import the remaining libraries along with the data.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.0.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.7)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow import keras\nfrom keras import utils\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\n\n# Embedding visualizations\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"iframe\"\n\n# Importing data\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\nimported_data = pd.read_csv(train_url)\n\n# Importing English stopwords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nWith our data imported and read in, let’s see what we are dealing with.\n\nimported_data\n\n\n\n  \n    \n\n\n\n\n\n\nUnnamed: 0\ntitle\ntext\nfake\n\n\n\n\n0\n17366\nMerkel: Strong result for Austria's FPO 'big c...\nGerman Chancellor Angela Merkel said on Monday...\n0\n\n\n1\n5634\nTrump says Pence will lead voter fraud panel\nWEST PALM BEACH, Fla.President Donald Trump sa...\n0\n\n\n2\n17487\nJUST IN: SUSPECTED LEAKER and “Close Confidant...\nOn December 5, 2017, Circa s Sara Carter warne...\n1\n\n\n3\n12217\nThyssenkrupp has offered help to Argentina ove...\nGermany s Thyssenkrupp, has offered assistance...\n0\n\n\n4\n5535\nTrump say appeals court decision on travel ban...\nPresident Donald Trump on Thursday called the ...\n0\n\n\n...\n...\n...\n...\n...\n\n\n22444\n10709\nALARMING: NSA Refuses to Release Clinton-Lynch...\nIf Clinton and Lynch just talked about grandki...\n1\n\n\n22445\n8731\nCan Pence's vow not to sling mud survive a Tru...\n() - In 1990, during a close and bitter congre...\n0\n\n\n22446\n4733\nWatch Trump Campaign Try To Spin Their Way Ou...\nA new ad by the Hillary Clinton SuperPac Prior...\n1\n\n\n22447\n3993\nTrump celebrates first 100 days as president, ...\nHARRISBURG, Pa.U.S. President Donald Trump hit...\n0\n\n\n22448\n12896\nTRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...\nMELBOURNE, FL is a town with a population of 7...\n1\n\n\n\n\n22449 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\nFor this dataset, we can see that each row represents an article. Within each row, we are given the title and the full text of each article as well as a boolean value called fake that denotes whether the article contains fake news, as determined by the authors of the paper above."
  },
  {
    "objectID": "posts/HW6/index.html#make-a-dataset",
    "href": "posts/HW6/index.html#make-a-dataset",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Make a Dataset",
    "text": "Make a Dataset\nOnce we have imported everything we needed, we can move on to creating the dataset, which we will accomplish by creating a function that takes in the dataframe as the parameter. In order to do that, there are a three things we want our function to execute: 1) making all the texts lowercase to prevent misinterpretation, 2) removing all the stopwords to improve efficiency, 3) constructing and returning a tf.data.Dataset that has two inputs (title, text) and one output fake. Thus, we can accomplish all of these with the following code:\n\ndef make_dataset(df):\n    \"\"\"\n    Creating a prepped tensorflow dataset from a dataframe\n\n    Input:\n      df = dataframe\n    Return:\n      tensorflow dataset with two inputs and one outputs\n    \"\"\"\n    # Removing stopwords\n    df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n\n    # Creating dataset\n    ds = tf.data.Dataset.from_tensor_slices((\n      {\"title\" : df[[\"title\"]], \"text\" : df[[\"text\"]]},\n      {\"fake\" : df[\"fake\"]}\n      ))\n\n    # Batching dataset (improve efficiency)\n    ds = ds.batch(100)\n    return ds\n\nWith this function, let’s create our dataset using our imported dataframe. In addition, because the dataframe is in a specific order, let’s randomize the order before we split it into training and validation sets.\n\ndataset = make_dataset(imported_data)\ndataset = dataset.shuffle(buffer_size = len(dataset)) # Shuffling order of dataset\n\ntrain_size = int(0.8*len(dataset)) # Indicating size: 80% for training\nval_size   = int(0.2*len(dataset)) # Indicating size: 20% for validation\n\ntrain = dataset.take(train_size) # Training set\nval = dataset.skip(train_size).take(val_size) # Validation Set\n\nBefore we move on to our model components, let’s establish what our initial comparing condition is.\n\nimported_data['fake'].value_counts() # display counts of fake/real news\n\n1    11740\n0    10709\nName: fake, dtype: int64\n\n\nAs we mentioned in the last blog post, the base rate refers to the accuracy of a baseline model that always predicts the most occurring output. Thus, our baseline model would always predict 1, or fake news, making our base rate 11740/(11740+10709)%, or 52.3%.\nNext, let’s also implement text vectorization for our models later.\n\n# Preprating a text vectorization layer for tf model\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    no_punctuation = tf.strings.regex_replace(lowercase,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation\n\ntitle_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, # only consider this many words\n    output_mode='int',\n    output_sequence_length=500)\n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\ntext_vectorize_layer = layers.TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary,\n    output_mode='int',\n    output_sequence_length=500)\n\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))"
  },
  {
    "objectID": "posts/HW6/index.html#create-models",
    "href": "posts/HW6/index.html#create-models",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Create Models",
    "text": "Create Models\nAs mentioned above, the purpose of these models is to use the inputs title and text within the imported data and classify whether each article is considered either real or fake news, outputting a fake boolean of 0 or 1.\nSo first, we want to specify our model’s input shapes.\n\n# Title input shape\ntitle_input = keras.Input(\n    shape=(1,),\n    name = \"title\",\n    dtype = \"string\"\n)\n\n#Text input shape\ntext_input = keras.Input(\n    shape=(1,),\n    name = \"text\",\n    dtype = \"string\"\n)"
  },
  {
    "objectID": "posts/HW6/index.html#first-model",
    "href": "posts/HW6/index.html#first-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "First Model",
    "text": "First Model\nNow, we can start constructing the tensorflow models.\nFor our first model, we will use only the article title as an input.\nWe will begin by constructing the layers. For all of these models, we will be incorporating what’s called an embedding layer, which allows us to convert input info into a dense vector. In addition, because we aredealing with strings, we will want to incorporate a TextVectorization layer in order to map/transform text features into integer sequences. Thus, our code should look something like this\n\n# TextVectorization layer to input_title\ntitle_features = title_vectorize_layer(title_input)\n\n# Embeddings\ntitle_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\n\n# Binary classifications\ntitle_features = layers.Dense(2, activation='relu', name=\"fake\")(title_features)\n\n\n# only using title\nmodel1 = keras.Model(\n    inputs = [title_input],\n    outputs = title_features\n)\n\nLet’s visualize what our model looks like.\n\nutils.plot_model(model1, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel1.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory = model1.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 11s 10ms/step - accuracy: 0.5083 - loss: 0.6927 - val_accuracy: 0.4991 - val_loss: 0.6931\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.5156 - loss: 0.6920 - val_accuracy: 0.5200 - val_loss: 0.6861\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.5633 - loss: 0.6845 - val_accuracy: 0.5287 - val_loss: 0.6765\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.6161 - loss: 0.6768 - val_accuracy: 0.5356 - val_loss: 0.6657\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6592 - loss: 0.6653 - val_accuracy: 0.6278 - val_loss: 0.6515\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.7231 - loss: 0.6502 - val_accuracy: 0.8740 - val_loss: 0.6368\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 8ms/step - accuracy: 0.7261 - loss: 0.6352 - val_accuracy: 0.8244 - val_loss: 0.6180\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 7ms/step - accuracy: 0.7695 - loss: 0.6139 - val_accuracy: 0.8791 - val_loss: 0.5950\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 5ms/step - accuracy: 0.7824 - loss: 0.6004 - val_accuracy: 0.8829 - val_loss: 0.5719\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8152 - loss: 0.5767 - val_accuracy: 0.8789 - val_loss: 0.5545\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8255 - loss: 0.5575 - val_accuracy: 0.8964 - val_loss: 0.5303\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 7ms/step - accuracy: 0.8436 - loss: 0.5371 - val_accuracy: 0.8962 - val_loss: 0.5128\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8425 - loss: 0.5216 - val_accuracy: 0.8860 - val_loss: 0.4936\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 9ms/step - accuracy: 0.8469 - loss: 0.5032 - val_accuracy: 0.8873 - val_loss: 0.4769\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8682 - loss: 0.4827 - val_accuracy: 0.9018 - val_loss: 0.4556\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8638 - loss: 0.4675 - val_accuracy: 0.8582 - val_loss: 0.4510\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8583 - loss: 0.4544 - val_accuracy: 0.9124 - val_loss: 0.4233\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8596 - loss: 0.4434 - val_accuracy: 0.9058 - val_loss: 0.4086\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 6ms/step - accuracy: 0.8759 - loss: 0.4213 - val_accuracy: 0.9128 - val_loss: 0.3888\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 1s 5ms/step - accuracy: 0.8823 - loss: 0.4100 - val_accuracy: 0.9247 - val_loss: 0.3757\n\n\n\n# we visualize our training history\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy of model1 started at 49.9%% but hit peak of 92.5%, better than the baseline model by 40.2%."
  },
  {
    "objectID": "posts/HW6/index.html#second-model",
    "href": "posts/HW6/index.html#second-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Second Model",
    "text": "Second Model\nFor our second model, we will use only the article text as an input this time around.\nThe structure of our model will look the exact same as model1, except this time, for out text_vectorize_layer input parameter, we are going to use text_input rather than title_input. Thus our code will look like so:\n\n# We begin construct our model's layers\ntext_features = text_vectorize_layer(text_input)\ntext_features = layers.Embedding(size_vocabulary, output_dim = 3, name=\"embedding\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\ntext_features = layers.Dense(2, activation='relu', name=\"fake\")(text_features)\n\nmodel2 = keras.Model(\n    # only using text\n    inputs = [text_input],\n    outputs = text_features\n)\n\nOnce again, let’s visualize and train our model.\n\n# we visualize our model\nutils.plot_model(model2, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel2.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\n\nhistory2 = model2.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.5218 - loss: 0.6885 - val_accuracy: 0.5418 - val_loss: 0.6711\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 13ms/step - accuracy: 0.5843 - loss: 0.6687 - val_accuracy: 0.6849 - val_loss: 0.6439\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.6715 - loss: 0.6363 - val_accuracy: 0.8587 - val_loss: 0.5951\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.7953 - loss: 0.5863 - val_accuracy: 0.9193 - val_loss: 0.5277\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.8478 - loss: 0.5275 - val_accuracy: 0.9253 - val_loss: 0.4646\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 17ms/step - accuracy: 0.8666 - loss: 0.4667 - val_accuracy: 0.9398 - val_loss: 0.4078\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 21ms/step - accuracy: 0.8877 - loss: 0.4186 - val_accuracy: 0.9302 - val_loss: 0.3664\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8967 - loss: 0.3782 - val_accuracy: 0.9461 - val_loss: 0.3276\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9098 - loss: 0.3468 - val_accuracy: 0.9480 - val_loss: 0.3012\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9219 - loss: 0.3175 - val_accuracy: 0.9269 - val_loss: 0.2823\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9278 - loss: 0.2921 - val_accuracy: 0.9522 - val_loss: 0.2574\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 13ms/step - accuracy: 0.9315 - loss: 0.2717 - val_accuracy: 0.9576 - val_loss: 0.2328\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9386 - loss: 0.2576 - val_accuracy: 0.9616 - val_loss: 0.2175\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 11ms/step - accuracy: 0.9387 - loss: 0.2415 - val_accuracy: 0.9577 - val_loss: 0.2089\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9412 - loss: 0.2297 - val_accuracy: 0.9500 - val_loss: 0.2087\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9473 - loss: 0.2211 - val_accuracy: 0.9536 - val_loss: 0.1980\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9483 - loss: 0.2103 - val_accuracy: 0.9593 - val_loss: 0.1839\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.9545 - loss: 0.1974 - val_accuracy: 0.9664 - val_loss: 0.1704\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 2s 10ms/step - accuracy: 0.9555 - loss: 0.1873 - val_accuracy: 0.9420 - val_loss: 0.1790\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 10ms/step - accuracy: 0.9511 - loss: 0.1885 - val_accuracy: 0.9669 - val_loss: 0.1576\n\n\n\n# we visualize our training history\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy of model2 started at 54.2% but hit peak of 96.6%, which was better than the baseline model by 44.3%. In addition, model2 stabalized around 91% to 96% by the 4th epoch."
  },
  {
    "objectID": "posts/HW6/index.html#third-model",
    "href": "posts/HW6/index.html#third-model",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Third Model",
    "text": "Third Model\nFor our final model, we will use both the article title and the article text as input.\nFor this model, because we have two components, we are going to want to perform vectorization and embedding on them individually and then concatenating the output of the article title pipeline with the output of the article text pipeline. Once concatenation, we can perform use the same Dropout,FlobalAveragePooling1D, and Dense layers as the previous models, making our code look like so:\n\n# Vectorization\ntitle_features = title_vectorize_layer(title_input)\ntext_features = text_vectorize_layer(text_input)\n\n# Embedding\ntitle_embedding = layers.Embedding(size_vocabulary, 10)\ntext_embedding = layers.Embedding(size_vocabulary, 10)\ntitle_features = title_embedding(title_features)\ntext_features = text_embedding(text_features)\ntitle_features = layers.Dense(32, activation='relu')(title_features)\ntext_features = layers.Dense(32, activation='relu')(text_features)\n\n# main layers\nmain = layers.concatenate([title_features, text_features], axis = 1)\nmain = layers.Dropout(0.2)(main)\nmain = layers.GlobalAveragePooling1D()(main)\nmain = layers.Dropout(0.2)(main)\nmain = layers.Dense(2, activation='relu', name = 'fake')(main)\n\nFor the last time, let’s visualize and train:\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = main\n)\n\n\n# we visualize our model\nutils.plot_model(model3, \"output_filename.png\",\n                       show_shapes=True,\n                       show_layer_names=True)\n\n\n\n\n\n\n\n\n\nmodel3.compile(optimizer=\"adam\",\n              loss = losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[\"accuracy\"])\nhistory3 = model3.fit(train,\n                    validation_data=val,\n                    epochs = 20,\n                    verbose = True)\n\nEpoch 1/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 17ms/step - accuracy: 0.9344 - loss: 0.2223 - val_accuracy: 0.9618 - val_loss: 0.1554\nEpoch 2/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9498 - loss: 0.1773 - val_accuracy: 0.9736 - val_loss: 0.1209\nEpoch 3/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9571 - loss: 0.1462 - val_accuracy: 0.9778 - val_loss: 0.1043\nEpoch 4/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9675 - loss: 0.1208 - val_accuracy: 0.9789 - val_loss: 0.0886\nEpoch 5/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.9712 - loss: 0.1055 - val_accuracy: 0.9873 - val_loss: 0.0623\nEpoch 6/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9762 - loss: 0.0910 - val_accuracy: 0.9881 - val_loss: 0.0552\nEpoch 7/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9782 - loss: 0.0767 - val_accuracy: 0.9903 - val_loss: 0.0462\nEpoch 8/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 16ms/step - accuracy: 0.9831 - loss: 0.0634 - val_accuracy: 0.9920 - val_loss: 0.0414\nEpoch 9/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9859 - loss: 0.0560 - val_accuracy: 0.9929 - val_loss: 0.0350\nEpoch 10/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9872 - loss: 0.0497 - val_accuracy: 0.9924 - val_loss: 0.0355\nEpoch 11/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9861 - loss: 0.0464 - val_accuracy: 0.9933 - val_loss: 0.0247\nEpoch 12/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 19ms/step - accuracy: 0.9875 - loss: 0.0432 - val_accuracy: 0.9940 - val_loss: 0.0259\nEpoch 13/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9910 - loss: 0.0351 - val_accuracy: 0.9958 - val_loss: 0.0202\nEpoch 14/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 19ms/step - accuracy: 0.9913 - loss: 0.0326 - val_accuracy: 0.9953 - val_loss: 0.0207\nEpoch 15/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 4s 15ms/step - accuracy: 0.9908 - loss: 0.0308 - val_accuracy: 0.9964 - val_loss: 0.0190\nEpoch 16/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9929 - loss: 0.0308 - val_accuracy: 0.9964 - val_loss: 0.0162\nEpoch 17/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 15ms/step - accuracy: 0.9935 - loss: 0.0257 - val_accuracy: 0.9976 - val_loss: 0.0150\nEpoch 18/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 6s 17ms/step - accuracy: 0.9931 - loss: 0.0268 - val_accuracy: 0.9960 - val_loss: 0.0147\nEpoch 19/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 5s 15ms/step - accuracy: 0.9948 - loss: 0.0215 - val_accuracy: 0.9973 - val_loss: 0.0138\nEpoch 20/20\n180/180 ━━━━━━━━━━━━━━━━━━━━ 3s 16ms/step - accuracy: 0.9936 - loss: 0.0225 - val_accuracy: 0.9967 - val_loss: 0.0136\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nUnlike the previous model which started off low and then climbed into the 90s value, the accuracy of model3 stabilized between 96.2% and 99.7% during training right from the first epoch, meaning it was better than the baseline model by 43.5% to 47.3% for all epochs."
  },
  {
    "objectID": "posts/HW6/index.html#model-evaluation",
    "href": "posts/HW6/index.html#model-evaluation",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nOut of all three of our models, it is obvious that model3 using both article title and text was the best performing. Thus, we will now use model3 to see how well it performs on our unseen test data. Similar to before, we will use make_dataset() to import and read in the test data, creating a new dataset, which we will then evaluate our model with.\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\nimported_test_data = pd.read_csv(test_url)\ntest = make_dataset(imported_test_data)\n\nmodel3.evaluate(test, verbose=1)\n\n225/225 ━━━━━━━━━━━━━━━━━━━━ 2s 9ms/step - accuracy: 0.9946 - loss: 0.0186\n\n\n[0.017387792468070984, 0.9952336549758911]\n\n\nGreat! model3 was able to get 99.4% accuracy on the new test dataset, meaning eat would be correct at detecting fake news above 99% of the time."
  },
  {
    "objectID": "posts/HW6/index.html#embedding-visualization",
    "href": "posts/HW6/index.html#embedding-visualization",
    "title": "Homework 6: More TensorFlow Modeling (Fake News Classification)",
    "section": "Embedding Visualization",
    "text": "Embedding Visualization\nBecause we used embedding layers in our models, we can visualize the embedding our model learned to see if there are any patterns in the words that our model found useful when distinguising news. To do this we will use a 2d embedding plot and then principal component analysis (PCA) to reduce dimensionality.\n\nweights = model3.get_layer('embedding_1').get_weights()[0] # get weights from embedding layer\nvocab = title_vectorize_layer.get_vocabulary() # get vocabulary from data prep\n\npca = PCA(n_components=2) # Initializing PCA\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = [2]*len(embedding_df),\n                 hover_name = \"word\")\n\nfig.show()\n\n\n\n\nAs you can see, “funds”, “concerns”, “financial”, and “debt” are clustered close by, which make sense in the topic of economics. We also we words like “socialism” and “corruption” next to each other on the right side of the scatterplot, which could be highlighting more right-leaning articles."
  },
  {
    "objectID": "posts/HW4/index.html",
    "href": "posts/HW4/index.html",
    "title": "Homework 4: Numpy & Matrix-Vectors (Heat Diffusion)",
    "section": "",
    "text": "In this blog post, we will learn how to use numpy and other tools to work with linear algebra in Python as well as increase computational speed. For our example, we will conduct a simulation of two-dimensional heat diffusion.\n\nMatrix Multiplication\nFor this example, we will need the following block of code.\n\nN = 101\nepsilon = 0.2\n\nHere, N is representative of the number of x,y grid values while epsilon is representative of how “heavy” we want each update to be.\nFirst, we want to create our initial condition: an empty grid with one unit of heat at the midpoint called u0. To do that, we will need the following block of code:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)\n\n\n\n\n\n\n\n\n\nimport jax\n\nModuleNotFoundError: No module named 'jax'\n\n\nTesting"
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Homework 2: Web Scraping (Movie Recs)",
    "section": "",
    "text": "In this blog, we will learn how to extract data from HTML sites using webscraping. For our example, we will use movie sites.\n\nCreating Scrapy Project\nWe will first start by running these two commands in the terminal:\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nRunning these lines will allow us to create a folder labeled “TBDB_scraper” which we will then for this webscraping tutorial.\nInside the inner “TMBD_scraper” will exists a python file called settings.py which we will want to add these two lines:\nUSER_AGENT = \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"\nDOWNLOAD_DELAY = 2\nAdding this line will be crucial for the 403 Forbidden errors, which commonly arises since these websites have detectors that see that we are scraping the website, so they want to block us from this action.\nThe first line USER_AGENT helps implement a fake user agent for all the requests that we will send, which makes it harder for the website to know if the requests are from a scraper (which it will want to block) or an actual user.\nThe second line DOWNLOAD_DELAY helps implement a randomization of request delays, which spaces out the requests over longer, patternless intervals to make the website flag us less frequently for scraping.\n\n\nImplementing Scraping Methods\nNow that we have our scrapy project, we want to create a tmdb_spider.py file in the spiders folder.\nWe first start by importing scrapy, which is the webscraping library which will allow us to do all of this.\n\nimport scrapy\n\nNext, we need to create the spider class TmbdSpider which contains its name (tmbd_spider) that we will need to perform the webscraping along with the url for the movie from the website we are working with and its three parse methods. For this example, we will use The Dark Knight with the data from The Movie Dtabase. So, our code will look something like this:\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir = None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nAnd for the three parsing methods:\ndef parse(self, response):\n      \"\"\"\n      This method starts with the start_url which is a movie page on the website,\n      and proceeds by sending a request to the cast page of the movie from the start_url.\n      This has no outputs but sets up the request to be used in the parse_full_credits method.\n      \"\"\"\n        page = response.url + \"/cast\" # Adding on /cast to get the cast page of movie's url\n        yield scrapy.Request(page, callback = self.parse_full_credits)\nThe parse method starts with the start_url and then sends a request for the movie’s cast page from the start_url, which will be used in the next defined method, parse_full_credits\ndef parse_full_credits(self, response):\n      \"\"\"\n      This method starts with the cast page of a movie given through the start_url \n      from parse function. It then iterates through all the actors's, actresses's links \n      on this page and sends a request for each actor's, actress's page. This\n      produces no outputs but sets up the requests to be used in the parse_actor_page method.\n      \"\"\"\n        for entry in response.css(\"ol\")[0].css(\"li\"):\n            actor_tag = entry.css(\"a::attr(href)\").get() # Gets unique tag for each actor, actress\n            actor_link = \"https://www.themoviedb.org\" + actor_tag # Adding on tag to main website \n                                                                  # address to get the link to\n                                                                  # each actor's, actress's page  \n            yield scrapy.Request(actor_link, callback = self.parse_actor_page)\n                                           \nSimilar to the last method, this method starts with the cast page of the desired movie and then iterates through all the actor’s, actress’s links on this page and sends a request for each actor’s actress’s page, which will be used in the last defined method, parse_actor_page.\ndef parse_actor_page(self, response):\n        \"\"\"\n        This method starts with the actor/actress's page given by the \n        parse_full_credits function. It then iterates through all the movies that the \n        actor/actress has made an appearance. For each movie, the output will be\n        a dictionary where the key is thename of the actor, actress, and the value\n        is the name of the movie.\n        \"\"\" \n        actor_name = response.css(\"h2\").css(\"::text\").get() # Gets the name of actor,actress \n        for entry in response.css(\"div.credits_list bdi::text\"):\n            movie_or_TV_name = entry.get() # Iterates through each movie appearance\n            yield {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name} # Returns dictionary\nThis method starts with the page of a certain actor, actress and iterates through all movies the this actor, actress has made an appearance and for each one movie we will return a dictionary with the their name and the movie’s name.\nGreat, now we can use our defined spider! So in terminal, navigate to the TMDB_scraper folder by running\nscrapy crawl tmdb_spider -o results.csv\nwhich will return us a csv file with the completed list of all the movies that all the actors, actresses in our chosen movie has made an appearance in.\n\n\nCreating Movie Recommender\nNow that we have our completed data set, we will want to create an algorith to help us with movie recommendations.\nWe first start off by reading in our data, so we need to import the pandas and numpy libraries.\n\nimport pandas as pd\nimport numpy as np\nresults = pd.read_csv(\"results.csv\")\nresults\n\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nHeath Ledger\nHeath Ledger: A Tragic Tale\n\n\n1\nHeath Ledger\nJoker: Put on a Happy Face\n\n\n2\nHeath Ledger\nI Am Heath Ledger\n\n\n3\nHeath Ledger\nThe Fire Rises: The Creation and Impact of The...\n\n\n4\nHeath Ledger\nToo Young to Die\n\n\n...\n...\n...\n\n\n4144\nMichael Caine\nThe Double\n\n\n4145\nMichael Caine\nBlue Ice\n\n\n4146\nMichael Caine\nThe Fourth Protocol\n\n\n4147\nMichael Caine\nPulp\n\n\n4148\nMichael Caine\nGet Carter\n\n\n\n\n4149 rows × 2 columns\n\n\n\n\nAs verification, we want to make sure that the number of actors and actresses listed in our data equals the size of the movie cast. So, we would run the code below:\n\nlen(results[\"actor\"].unique())\n\n136\n\n\nNow, we want to create a 2-dimensional list with one column consists of all the movie names the actors or actresses have appeared in while the other column consists of the number of actor and actresses from The Dark Knight that was also in a certain movie.\n\nmovies_or_TV_shows = results[\"movie_or_TV_name\"].unique() # Creating df containing all \n                                                          # movie names with 0 repeats\n\nrows, cols = (len(movies_or_TV_shows), 2) \nrec_list = [[0 for i in range(cols)] for j in range(rows)] # Creating 2D lists, each unique movie \n                                                           # gets in own row\n\nindex = 0\nfor movie_or_TV_show in movies_or_TV_shows:\n    \"\"\"\n    Iterating through each movie, creating a dataframe of all actors and actreses from \n    The Dark Knight movie that also appeared in the specific movie, and then counting\n    the number of actors and actresses\n    \"\"\"\n    \n    panda = results[results[\"movie_or_TV_name\"] == movie_or_TV_show]\n    num_shared_actors = len(panda)\n    \n    rec_list[index][0] = movie_or_TV_show   # Movie name list\n    rec_list[index][1] = num_shared_actors  # Counting list\n    \n    index += 1 # Updating list\n\nWith this list, we now need to sort it, which we can do using the sorted() function and lambda functions. Because we want the top movie recommendations first, we will use the reverse=True statement. Lastly, we want to convert this to a pandas dataframe to use later.\n\nrec_list_sorted = sorted(rec_list,key=lambda l:l[1], reverse=True)\nrec_list_sorted = pd.DataFrame(rec_list_sorted)\nrec_list_panda = rec_list_sorted.rename(columns = {0: \"Movie\", \n                                                   1: \"No. Actors Appearing in Dark Knight\"})\nrec_list_panda\n\n\n\n\n\n\n\n\n\nMovie\nNo. Actors Appearing in Dark Knight\n\n\n\n\n0\nThe Dark Knight\n138\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n...\n...\n...\n\n\n3383\nTony Awards\n1\n\n\n3384\nNavy Log\n1\n\n\n3385\nMark Saber\n1\n\n\n3386\nWhat's My Line?\n1\n\n\n3387\nMorning Departure\n1\n\n\n\n\n3388 rows × 2 columns\n\n\n\n\nNote that for the first entry, “The Dark Knight” as a count of 138 when we were expecting to only get 136. This is because two actors, Heath Ledger and Tom McComas, are actually listed twice on their cast page, which accounts for the increase of two counts.\nGreat! Now we have our ordered list! Ignoring “The Dark Knight” with the highest count (which is a given), we can see that the next highest is “The Fire Rises: The Creation and Impact of the Dark Knight Trilogy,” a documentary about “The Dark Knight” trilogy (which also makes sense), so we shold ignore that one as well. The third highest is “The Dark Knight Rises,” followed by “CSI: Crime Scene Investigation” and then “Doctor Who.” These are our top three recommendations\n\n\nCreating Data Visualization\nTo end, let’s create a data visualization showing a bar chart of number of shared actors for the movies.\nBecause there over 3,000 movies, let’s restrict the amount we want to include by imposing a minimum count. For this example, we will make it 6. Also, because we only want recommendations, we will exclude The Dark Knight in the visualization as well.\nFirst we want to create our considered movie rec dataframe.\n\nimport matplotlib.pyplot as plt \nbar_data = pd.DataFrame(rec_list_sorted)             # Creating dataframe of 2D sorted list\nbar_data = bar_data[bar_data[1] &lt; max(bar_data[1])]  # Removing The Dark Knight entry\nbar_data = bar_data[bar_data[1] &gt;= 6]                # Restricting minimum score to be 6\nbar_data\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n5\nGotham Tonight\n8\n\n\n6\nPrison Break\n7\n\n\n7\nWaking the Dead\n7\n\n\n8\nHeath Ledger: A Tribute\n6\n\n\n9\nE! True Hollywood Story\n6\n\n\n10\nEnding the Knight\n6\n\n\n11\nBatman Begins\n6\n\n\n12\nCasualty\n6\n\n\n13\nThe View\n6\n\n\n14\nLIVE with Kelly and Mark\n6\n\n\n\n\n\n\n\n\nFinally, we can create our bar chart figure. Because we have titles as our independent variable, which can be long, we want to make a horizontal bar chart instead of a vertical one.\n\nfig = plt.figure(figsize = (10, 5))\n \n# creating the bar plot\nplt.barh(bar_data[0], bar_data[1], color ='Blue')\n \nplt.xlabel(\"Movie\")\nplt.ylabel(\"No. of Actor/Actress Appearances Who Were Also in The Dark Knight\")\nplt.title(\"Bar Plot of the Top Recs for The Dark Knight\")\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Homework 0: Data Visualization",
    "section": "",
    "text": "In this blog, I will use the Palmer Penguins dataset to show how to create interesting data visualization. There are many visuals that we can use for the data set like scatterplot, histogram, or boxplot, but for today, we will use scatterplots.\n\nPrepping Dataframe\nTo access the dataframe, we first have to read in the csv file using pandas. Thus, we need to import the pandas library in order to work with data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)     \n\nNext, we want to visualize the data structure to see what type of information we are working with. We can use the head function of pandas to access the first five entries.\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nFollowing standard pandas operations, we want to clean up our data a little bit, removing any “N/A” entries and shortening the names.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])  # Removing penguins with NaN entries in Sex or Body Mass\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)   # Only getting the first word of the Species entry (i.e. species type)\ncols = [\"Species\",                 # Specifying the columns we want to look further\n        \"Island\",                  # into from the penguins data set\n        \"Sex\", \n        \"Culmen Length (mm)\", \n        \"Culmen Depth (mm)\", \n        \"Flipper Length (mm)\", \n        \"Body Mass (g)\"]\npenguins = penguins[cols]          # Choosing the columns we want our data frame to consist of\n\nNow, let’s take a look at the new simplified data set.\n\npenguins.head()\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\n\nNote that there were additional column information which we are going to ignore for today.\nOnce the data set is cleaned, we can start creating our data visualizations.\n\n\nSeparating Dataframe\nBecause this data set consists of different types of penguin species, we need to figure out how many species types we are dealing with and what their names are so that we know what to call in our code. We can use the value_counts function in the pandas library to do the following:\n\npenguins[\"Species\"].value_counts()  # Counting the number of penguins be each Species type\n\nSpecies\nAdelie       146\nGentoo       120\nChinstrap     68\nName: count, dtype: int64\n\n\nFrom this, we know that the penguins in this data set are classified as one of the three following species type: Adelie, Gentoo, or Chinstrap.\nNow, we can start writing our code to create the scatterplot. First, we need to separate the penguin types and create individual data frames for each of the three species. To do so, we can use boolean indexing. We can use a == statement to return a True or False value indicating whether or not a specific entry is of the indicated penguin species\nFor example, to find only the Gentoo penguins, we will say penguins[\"Species\"]==\"Gentoo\" as our conditional statement for including an into the created data frame. This pattern will help us create our three separate data frames as follows:\n\n# Generating the separate dataframes based on Species\nAdelie_dataset = penguins[penguins[\"Species\"] == \"Adelie\"]\nGentoo_dataset = penguins[penguins[\"Species\"] == \"Gentoo\"]\nChinstrap_dataset = penguins[penguins[\"Species\"] == \"Chinstrap\"]\n\nLet’s check to see if this successfully separated the penguins by species.\n\nAdelie_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nAdelie\nDream\nFEMALE\n36.6\n18.4\n184.0\n3475.0\n\n\n148\nAdelie\nDream\nFEMALE\n36.0\n17.8\n195.0\n3450.0\n\n\n149\nAdelie\nDream\nMALE\n37.8\n18.1\n193.0\n3750.0\n\n\n150\nAdelie\nDream\nFEMALE\n36.0\n17.1\n187.0\n3700.0\n\n\n151\nAdelie\nDream\nMALE\n41.5\n18.5\n201.0\n4000.0\n\n\n\n\n146 rows × 7 columns\n\n\n\n\n\nGentoo_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n220\nGentoo\nBiscoe\nFEMALE\n46.1\n13.2\n211.0\n4500.0\n\n\n221\nGentoo\nBiscoe\nMALE\n50.0\n16.3\n230.0\n5700.0\n\n\n222\nGentoo\nBiscoe\nFEMALE\n48.7\n14.1\n210.0\n4450.0\n\n\n223\nGentoo\nBiscoe\nMALE\n50.0\n15.2\n218.0\n5700.0\n\n\n224\nGentoo\nBiscoe\nMALE\n47.6\n14.5\n215.0\n5400.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\nGentoo\nBiscoe\nFEMALE\n47.2\n13.7\n214.0\n4925.0\n\n\n340\nGentoo\nBiscoe\nFEMALE\n46.8\n14.3\n215.0\n4850.0\n\n\n341\nGentoo\nBiscoe\nMALE\n50.4\n15.7\n222.0\n5750.0\n\n\n342\nGentoo\nBiscoe\nFEMALE\n45.2\n14.8\n212.0\n5200.0\n\n\n343\nGentoo\nBiscoe\nMALE\n49.9\n16.1\n213.0\n5400.0\n\n\n\n\n120 rows × 7 columns\n\n\n\n\n\nChinstrap_dataset\n\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n152\nChinstrap\nDream\nFEMALE\n46.5\n17.9\n192.0\n3500.0\n\n\n153\nChinstrap\nDream\nMALE\n50.0\n19.5\n196.0\n3900.0\n\n\n154\nChinstrap\nDream\nMALE\n51.3\n19.2\n193.0\n3650.0\n\n\n155\nChinstrap\nDream\nFEMALE\n45.4\n18.7\n188.0\n3525.0\n\n\n156\nChinstrap\nDream\nMALE\n52.7\n19.8\n197.0\n3725.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n215\nChinstrap\nDream\nMALE\n55.8\n19.8\n207.0\n4000.0\n\n\n216\nChinstrap\nDream\nFEMALE\n43.5\n18.1\n202.0\n3400.0\n\n\n217\nChinstrap\nDream\nMALE\n49.6\n18.2\n193.0\n3775.0\n\n\n218\nChinstrap\nDream\nMALE\n50.8\n19.0\n210.0\n4100.0\n\n\n219\nChinstrap\nDream\nFEMALE\n50.2\n18.7\n198.0\n3775.0\n\n\n\n\n68 rows × 7 columns\n\n\n\n\nGreat! Now all we have left is the scatterplot.\n\n\nCreating Visualization\nIn our examples, we will be using the matplotlib, a common library used for data visualizaiton. So, we need to import the matplotlib library.\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nFor matplotlib, the syntax for creating a scatterplot using a dataset is plt.scatter(df[x],df[y]) where df is the dataset name and x,y are the independent and dependent columns, respectively, that we want to work with. For our example, we will use Culmen Length and Flipper Length. So using Adelie, we will use the following line of code:\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"])\nBecause we are also working with three different data frames, meaning we will have three separate scatterplots on one plot, we can also add a label in our line of code. This way, when we combine all of them into one figure, we can use a legend to help us differentiate each data set. Thus, our code for each species data set will look like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n\n\n\n\n\n\n\nNow that we have out scatterplot, we can add features to make the graph more clear. To add the main title, we can use plt.title(). For the the x and y titles, it will be plt.xlabel() and plt.ylabel(), respectively. And finally, to create our legend, we will use plt.legend(). Once we added all this, our figure may look something like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n# Adding the title, x and y axes labels, and legend\nplt.title(\"Culmen Length vs. Flipper Length\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\n\n\n\n\n\n\n\n\nWith this completed scatterplot, we can finally analyze the two data columns. From this, it appears that for all species, there is a moderate to strong positive correlation between Culmen Length and Flipper Length. Also, it appears that these two may be good candidate factors in differentiating the species type as there are three relatively isolated clusters. And with that, we have successfully created our data visualization."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Homework 6: More TensorFlow Modeling (Fake News Classification)\n\n\n\n\n\n\nweek 9\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 5: TensorFlow and Keras Modeling (Image Classification)\n\n\n\n\n\n\nweek 8\n\n\nHomework\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 4: Numpy & Matrix-Vectors (Heat Diffusion)\n\n\n\n\n\n\nWeek 7\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 3: Web Development\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Web Scraping (Movie Recs)\n\n\n\n\n\n\nWeek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Database Visualization (Climate Change)\n\n\n\n\n\n\nWeek 2\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0: Data Visualization\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 18, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Homework 1: Database Visualization (Climate Change)",
    "section": "",
    "text": "In this blog, we will show how to work with databases in order to create interesting and interactive data graphics. We want to utilize databases when dealing with data sets that are incredibly large to the point of being impratical or inefficient storing the data.\n\nCreating Database\nBecause we are dealing with data, we will import the typical libraries involving data like pandas and numpy. In addition, we will import a library called sqlite3 which will let us worth with SQL as well as databases in Python.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have imported sqlite3, we will use the connect function to create the database in the curent directory.\n\n# Creating database in current directory we named data.db\nconn = sqlite3.connect(\"data.db\")\n\nAfter we have created our database, we now want to create its tables, which corresponds to the csv files that we are working with. For our example, we will create three tables that will be labeled temperatures, stations, countries.\nNext, we want to read in out data. Because we have a large data set, instead of reading it in all at once, we want to read in chunks of the data one at a time. We will do this by using the keyword chunksize, which returns an iterator that reads in the number of rows from the data equal to chucksize once we start querying the iterator. It should look something like follows:\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\n\nOnce we have our table, we want to create a function that will clean our data (as we will repeat this for the other two tables). In our data, each column is representative for each month of the year. Thus, we want to use the stack() function of pandas to create one column for the month data.\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nWith our function, we are ready to read in and populate the temperature table in our database. To do this, we want to use df.to_sql() which will write to the specified table (in our case, the conn object we created earlier). In addition to make sure each piece is added to the table and nothing is overwritten after each iteration, we will use if_exists.\n\n# Reading in data for the temperature table\nfor df in df_iter:\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"append\", index = False)\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\n\n\n1\nUSW00014924\n2016\n2\n-8.40\n\n\n2\nUSW00014924\n2016\n3\n-0.20\n\n\n3\nUSW00014924\n2016\n4\n3.21\n\n\n4\nUSW00014924\n2016\n5\n13.85\n\n\n\n\n\n\n\n\nAnd with that we have finished the temperature table of our database. Now, we want to repeat this for the station table.\n\n# stations table \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.head()\n\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\nNote with this current data set, there is no common column between our countries data and stations data. Thus, to match the two, we will add a new column to this data frame labeled CODE and represents the first two letters of the ID column.\n\nstations[\"CODE\"] = stations[\"ID\"].str[:2]\nstations.head()\n\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\nCODE\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\nAC\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\nAE\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\nAE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\nAE\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\nAE\n\n\n\n\n\n\n\n\nNow, we can create and fill the stations table of the database using the same to_sql() method.\n\n# Creating the stations table\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n27585\n\n\nWe will repeat this process one more time for our countries table now.\n\n# Creating the countries table\nurl = \"countries.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\n279\n\n\nNow we have a database containing three tables. Let’s just check that this is indeed the case.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nGreat! We officially have our three tables.\n\n\nWriting Query Function\nWe are now ready to construct the query function. First, let’s examine what type of data we are working with for each table.\n\ncursor = conn.cursor()\ncursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT,\n  \"CODE\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nNote that the temperatures and stations tables share the ID column while the stations and countries tables have matching entries under the CODE and FIPS 10-4, respectively\nWith this information, we want to create a function that will incorporate SQL in order to select the desired output data as well as matching up the tables and implement specified conditions. It should look something like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C ON SUBSTR(S.id, 1, 2) = C.[FIPS 10-4]\n        WHERE C.Name = ? AND T.Year BETWEEN ? AND ? AND T.Month = ?\n        \"\"\"\n\n        df = pd.read_sql_query(cmd, conn,params = (country, year_begin, year_end, month))\n        df = df.rename(columns = {\"Name\"  : \"Country\"})\n        return df\n\n\n\nNow we have our query function. Let’s see if it is outputting our desired data set using a randon test case. For our example, we will to retrieve data from India in the month of January from 1980 to 2020.\n\nquery_climate_database(db_file = \"data.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n47275\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n47276\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n47277\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n47278\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n47279\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n47280 rows × 7 columns\n\n\n\n\nGreat! Our query function is returnning exactly what we wanted.\n\n\nWriting Geographic Scatter Function\nWith our basic query function down, we will now diverge into different examples of them. The first example we will make is a scatter function which uses data to create scatterplot.\nFor this example, we will be using the plotly library which allows us to create interactive data visualization, so we will need to import that.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nFirst, we need to create a coeff() function that will perform linear linear regression on the data, using the years as the independent variable and the temperatures as the dependent variable. Because we are dealing with linear regression, we will also need to import it from the sklearn library. Also, in order to read the numbers as months, we will import calendar.\n\nfrom sklearn.linear_model import LinearRegression\nimport calendar\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # Creating a dataframe consisting of the years\n    y = data_group[\"Temp\"]   # Creating a string of the temperatures\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nWith this function, we can now create a function, which we will call temperature_plot() to create our plots. Because we already have a query function to create our desired dataframe, we are going to call it in our function to make it simpler.\n\ndegree_sign = u'\\N{DEGREE SIGN}' # Syntax for the degree symbol\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    This function creates a scatterplot figure for data on yearly temperature \n    increase for constraints specified by the user's parameter inputs.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_obs: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the different stations and their annual\n    temperature increase given a specified country, year range, and month of the year.\n    '''\n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n        \n    coeff = df.groupby([\"NAME\",\"LATITUDE\",\"LONGITUDE\",\"Country\",\"Month\", \"Station_Counts\"]).apply(coef) # Calculating annual change in temperature\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : f\"Estimated Yearly Increase ({degree_sign}C)\"}) # Renaming column\n    \n    coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"] = coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"].round(4) # Setting number of sig figs\n    \n    return px.scatter_mapbox(coeff,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_name = \"NAME\",\n                        color = f\"Estimated Yearly Increase ({degree_sign}C)\",\n                        title = f\"Estimates of Yearly Increase in {calendar.month_name[month]} for Stations in {country} ({year_begin} - {year_end})\",\n                        **kwargs)\n\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nLet’s also see how the United States looks like with the same time settings!\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"United States\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\nMore Figures / Data Visualisations\nTo end, we will create two more plots. Because our data involves temperature, we will create figures based on climate comparisons.\nBecause a common researched aspect of climate change is highly variable weather (meaning it is expected that weather of both extremes become more similar as time progresses), we will first create a plot looking at countries’ variance in temperature over time across its stations. Again, we can call the query function to create our dataframe.\n\ndef temperature_variance_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    This function creates a scatterplot figure for data on yearly temperature standard \n    deviation change for all the stations of a country specified by the user.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_occ: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the annal tempearture standard deviation change\n    given a specified country, year range, and month of the year.\n    \"\"\"\n    \n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    \n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n    \n    SD = df.groupby([\"NAME\",\"Year\"])[\"Temp\"].std() # Calculating annual standard deviation\n    SD = SD.reset_index() #Reformatting data frame\n    \n    coeff = df.groupby([\"NAME\"]).apply(coef)  # Calculating annual change in standard deviation\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : \"Correlation Coeff R\"}) # Renaming column\n    \n    \n    return px.scatter(data_frame = coeff,   \n                 y = \"Correlation Coeff R\", \n                 title = f\"Annual Standard Deviation Changes of Stations in {country}({year_begin} - {year_end})\",\n                 hover_name = \"NAME\",\n                 hover_data = [\"Correlation Coeff R\"])\n\nWith this function, let’s look into the variance of the US’s temperature in March from 1980 to 2020\n\nfig = temperature_variance_plot(\"United States\", 1980, 2020, 3, min_obs = 3)\nfig.show()\n\n\n\n\nFrom this figure, we can see that because the R-value, representing the average annual change, is mostly around 0, it means that there has been no positive or negative trend in terms of temperature variance. Note that this is for the US only, which means this trend should not be applied to other countries as well.\nFor our second one, let’s examine the climate/weather of two geographically similar and close countries. For this, we will need to use histograms to look at their temperature distributions. Unlike the other two, since we want our dataframe to incorporate two different countries, we will have to use a different cmd.\n\ndef temperature_histogram_plot(country1, country2, year_begin, year_end, **kwargs):\n    \"\"\"\n    This function generates two histograms of countries' \n    temperature distribution specified year range\n    \n    Parameters:\n    ----------\n    country1: string giving the name of the first country \n    country2: string giving the name of the second country\n    year_begin: integer representing the earliest year the output includes\n    year_end: integer representing the latest year the output includes\n    \n    Return:\n    ----------\n    A plotly normalized histograms\n    \"\"\"\n    \n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON S.code = C.\"FIPS 10-4\"\n    WHERE C.name = \"{country1}\" OR C.name = \"{country2}\" AND T.year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns = {\"Name\" : \"Country\"})\n    \n    return px.histogram(df,\n                  x = \"Temp\",\n                  opacity = 0.5,\n                  nbins = 30,\n                  histnorm = \"percent\", # Normalizing histograms in case the countries are largely different in counts\n                  barmode = \"stack\", \n                  facet_col = \"Country\",\n                  labels={\n                     \"Temp\": f\"Temperature ({degree_sign}C)\",\n                     \"count\": \"Normalized Counts\"\n                  },\n                  title = f\"Normalized Temperature Distributions of {country1} and {country2} from years {year_begin} - {year_end}\",\n                  **kwargs)\n\nNote that this is creating normalized histogram distibutions to prevent unfair visual comparison. Now we can test our function using two close countries, like India and Pakistan.\n\nfig = temperature_histogram_plot(\"India\", \"Pakistan\", 1980, 2020)\nfig.show()\n\n\n\n\nFrom this figure, we can see how India has a a larger temperature range, from -8 to 38 degrees Celsius, compared to Pakistan, from 0 to 38 degrees. In addition, it also has a higher peek than Pakistan. However, Pakistan’s peak is father away than India, peaking at 30 to 32 degrees while India peaked at 26-28 degrees. We can also not that both left-skewed. Thus, while there is a similarity between the two countries, there are also major differences in terms of climate. This makes sense since India is a much more geographically large countyry compared to Pakistan, which would mean it should hve more variance in temperature across different stations. In addition, India has more coastal regions and also takes up majority of the Himilayas mountains, resulting in colder temperatures than Pakistan which has less coastal regions and only takes up five percent of the mountains.\nNow that we have created all our figures, let’s make sure to close the database connection.\n\nconn.close()"
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Homework 3: Web Development",
    "section": "",
    "text": "In this blog post, we will learn how to create simple webapp via Flask, a web framework that provides useful tools and features that make creating web applications in Python a lot easier. For our example, we will creating a message bank which allows a user to either submit a message or view previous message entries.\n\nSetting up Flask\nThe first thing we need to do is setup flask. So, we will go to our virtual environment and then install flask. For Windows, the two line of code will be: conda activate {environment name} and pip install flask. Then to help with our website creation, we want to set our Flask environment to “development” through export FLASK_ENV=development which enables debug mode and gives more specific error messages.\nNow, we can start with the code files. First, in order to tell the Flask where our application is, we will create a python file called app.py. In this file we will need to import the flask python library as well as some of its tools. First, we need Flask class to run the application. We are also using render_template() function in order to generate output from a template file based on the Jinja2 engine that is found in the application’s templates folder. Next, we import request which creates a Request object that allows you to access the data passed into your Flask application. Lastly, we need g which acts as a namespace object to store common data during a request.\nfrom flask import Flask, render_template, request, g\nNext, we want to create a Flask application object and set up the Flask application to know where to look for templates and static files based on the location of the current Python module. We can do that with the code below:\napp = Flask(__name__)\nLastly, in order to run the the Flask application we want to use the line below:\nif __name__ == '__main__':\n    app.run(debug=True)\nBy saying if_name_ == '__main__', we are ensuring that the application only starts/runs when the script is executed directly and not when imported as a module in another script. This line will be the last in the python file\n\n\nCreating Website Functions\nNext up, depending on the goals of your website, we want to create some functions that we will call later when specifying the purpose of each website. For our purposes, because we want to have a website to submit and view messages, we will create three different functions.\nFor the first function, we want to create a function that should handle creating the database of messages (thus, we should also import sqlite3). This function should be able to 1) check whether there is a database in the g attribute of the app and connect to the database if there isn’t. Thus, our code should look something like this:\ndef get_message_db():\n    try:\n        return g.message_db                                       # Returning message database from attribute g                               \n    except AttributeError: \n        g.message_db = sqlite3.connect(\"messages_db.sqlite\")      # Creating connection to database file\n        cursor = g.message_db.cursor()                            # Creating cursor object to execute SQL\n        cursor.execute('''CREATE TABLE IF NOT EXISTS messages (\n                          id INTEGER PRIMARY KEY,\n                          handle TEXT, \n                          message TEXT)''')                       # Creatng message table if it doesn't exist yet\n        g.message_db.commit()                                     # Commiting changes to database\n        return g.message_db\nSecond, we want to create a function for dealin with submitting message. This is where we will use the request object us to pass data through flask. In addition, because we are dealing with databases, we will want to write and SQL Query that will help us execute inserting the data into the messages table. Thus our code should look something like this:\ndef insert_message(request):\n    message = request.form['message']                             # Extracting messages from request form\n    handle = request.form['handle']                               # Extracting handle from request form\n    \n    db = get_message_db()                                         # Getting message database\n    cursor = db.cursor()                                          # Creating cursor object to execute SQL\n    \n    SQL_query = \"INSERT INTO messages (handle, message) VALUES (?, ?)\"  # SQL query to insert data     \n                                                                        # into messages table \n        \n    cursor.execute(SQL_query, (handle, message))                  # Executing SQL query with handle \n                                                                  # and message as its parameters\n    \n    db.commit()                                                   # Commiting changes to database\n    db.close()                                                    # Closing database connection\nLastly, we need a function for the viewing aspect of our website. The function will have similar aspects of the insert_message function regarding getting the message database, using SQL query, and executing cursor. However, because we also want previous submitted messages, we will use the fetchall() method to retrieve all rows of a query result set (in our case, the selected messages). Again, our code should look something like this:\ndef random_messages(n):\n    db = get_message_db()                                         # Getting Message database\n    cursor = db.cursor()                                          # Creating cursor object to execute SQL\n    cursor.execute('''SELECT handle, message FROM messages ORDER BY RANDOM() LIMIT ?''', (n,))  # Select random messages \n                                                                                                # from the database\n    messages = cursor.fetchall()                                  # Fetching all selected messages\n    db.close()                                                    # Closing database connection\n    return messages   \n\n\nCreating HTML Templates\nBecause we are using render_template() function, we will also need to create HTML templates in order to specify what each page will do on the website.\nFirst, we want to create a base.html which will act as the initial structure/format of our website, and to do this we want to use &lt;!doctype html&gt; to declare HTML document type. This HTML file will send us to different links, so we will need to use &lt;nav&gt; which declares a navigation section to provide navigation links to other documents. Because we want to have two different ‘buttons’ to send to different links, one for submitting and one for viewing messages, we want to use a href which will denote a hyperlink from one web address to another and also denotes a hyperlink from one web address to another. In addition, because this html is acting as the starting page which will send us to the other pages, we will want to have additional html files that we can reference to in this base.html. Thus we will use the url_for() function to generate the URL to a view based on a name and arguments. It should look something like this:\n&lt;!doctype html&gt;   &lt;!-- HTML document type declaration --&gt;\n\n&lt;!-- Link to a stylesheet (style.css) in the 'static' directory --&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; \n\n&lt;!-- Title of the webpage, with a block for inheritance --&gt;\n&lt;title&gt;{% block title %}{% endblock %} - PIC16B Website&lt;/title&gt;\n\n&lt;nav&gt;    &lt;!-- Navigation section --&gt;\n  &lt;h1&gt;A Simple Message Bank&lt;/h1&gt; &lt;!-- Heading --&gt;\n  &lt;ul&gt;   &lt;!-- Unordered list of navigation links --&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Submit a Message&lt;/a&gt;&lt;/li&gt;   &lt;!-- Link to the 'submit' route --&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;        &lt;!-- Link to the 'view' route --&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n&lt;section class=\"content\"&gt;    &lt;!-- Content section --&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}   &lt;!-- Block for inheritance to allow overriding of header content --&gt;\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nThere are two things that we want to take note of in this clock of code. The first is the line &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt; which is used to include external CSS file (in ur case static) into the document. The second thing is the bottom chuck of code &lt;section class=\"content\"&gt;. By writing this line, we are allowing for template inheritance, meaning future extension templates (in our case, it will be the submit and view HTML files) will be able to override and header content here.\nNow that we have the base HTML down, we want to create our extension files, starting with the submit.html.\nFirst thing is that since this extends the base HTML , we will need to indicate by saying {% extends 'base.html' %}. For this page, because we want user input, &lt;form method=\"post\" action=\"/submit\"&gt;. form method indicates the starm of an HTML form element, ‘POST’ indicates we that the form will use the ‘POST’ method for submission, and action='/submit' indicates sending this data to the route called ‘/submit’.\nIn our form method, we want to create the input field as well as its specifying label which we can achieve through label type and label for, respective (since we want fields, we will use “text” as our input type. Make sure that for each id attribute, you put the variable name specified in your insert_message function as this is what the function will rely on.\nLastly, we want a submit button which will send the form data to the route by using &lt;input type=\"submit\" value=\"Submit\"&gt;. Putting this all together, our code will look something like this:\n{% extends 'base.html' %}    &lt;!-- Extension of base.html template --&gt;\n\n{% block title %}            &lt;!-- Override the title block --&gt;\nSubmit Message               &lt;!-- Title for this page --&gt;\n{% endblock %} \n\n{% block content %}          &lt;!-- Override the content block --&gt;\n&lt;h1&gt;Submit a Message&lt;/h1&gt;\n&lt;form method=\"post\" action=\"/submit\"&gt;    &lt;!-- Form for submitting a message, POST method to \"/submit\" route --&gt;     \n    &lt;label for=\"handle\"&gt;Your Name:&lt;/label&gt;               &lt;!-- Label for the input field \"handle\" --&gt;\n    &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;&lt;br&gt;    &lt;!-- Input field for entering the name --&gt;\n    &lt;label for=\"message\"&gt;Your Message:&lt;/label&gt;           &lt;!-- Label for the input field \"message\" --&gt;\n    &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;&lt;br&gt;  &lt;!-- Input field for entering the message --&gt;\n    &lt;input type=\"submit\" value=\"Submit\"&gt;                 &lt;!-- Submit button --&gt;\n&lt;/form&gt;\n{% endblock %}\nThe next file that we want to create now is the view.html for the viewing aspect of our website. Similar to submit.html we will need {% extends 'base.html' %} to indicate an extension. However, since this wont require user input, we will not using form method. Instead, since we are looking to output the messages and handle, we will be using a for loop that will iterate through the rows of the ‘message’ database and returning a list item for each ‘message’, first item being the handle and second item being the message content. Putting these two factors together, our code will look something like this.\n{% extends 'base.html' %}                        &lt;!-- Extend base.html template --&gt;\n\n{% block title %}                                &lt;!-- Override the title block --&gt;\nView Messages                                    &lt;!-- Title for this page --&gt;\n{% endblock %}\n        \n{% block content %}                              &lt;!-- Override the content block --&gt;\n&lt;h1&gt;Some Cool Messages&lt;/h1&gt;\n&lt;ul&gt;\n{% for message in messages %}                    &lt;!-- Loop through the messages --&gt;\n    &lt;li&gt;{{ message[0] }}: {{ message[1] }}&lt;/li&gt;  &lt;!-- List item displaying each message --&gt;\n{% endfor %}\n&lt;/ul&gt;\n{% endblock %}\n\n\nApp Routing\nNow we will start working on the routing part of our website.\nThe first one we need will be for the homepage. To specify the routing of different pages on a website, we will be using @app.route() each time, and in the () will be the unique tag to identify that page. Because we are starting with the homepage, we will first put '/' inside. Next, since we have a HTML file that specifies the formatting of our homepage, we will create a function to call that file using render_template(). And so, our code will look like this:\n@app.route('/')\ndef main():\n    return render_template('base.html')                           # Rendering base.html template\nNext, we will specify the routing to the submit page. Similar, to homepage, we will need @app route(), but this time we will put '/submit' in the ().\nAdditionally, since our submit.html uses form method, we will add ‘methods’ parameters to tell to the application that this route handles both different requests. Thus, we will need an if-else statement in our function to specify different things. First, we want to check if the request is specifically a ‘POST’ request, and if so, the form on the submit.html was submitted, which means we will need to call the insert_message function with request to send the content into the database and then redirect to the same page with a fresh start. Our code will look something like this:\n@app.route('/submit', methods=['GET', 'POST'])\ndef submit():\n    if request.method == 'POST':                                  \n        insert_message(request)                                   # Inserting message into database\n        return render_template('submit.html')                     # Rendering submit.html template\n    else:\n        return render_template('submit.html')\nLastly, we need to route to the viewing page. Again we do @app.route('/view') this time. The only aspects left we haven’t used is the view.html and the random_message function. So in our function, we will call our function with any quantity parameter (for this case, we will use 5) to retrieve random messages from our database and then assign it to a variable. With this variable, we will render the view.html while passing our variable to it, giving us the displayed messages. So, our code will look like this:\n@app.route('/view')\ndef view():\n    messages = random_messages(5)                                 # Retrieving 5 random messages from database\n    return render_template('view.html', messages=messages)        # Rendering view.html with fetched messages\nWith these three app routings, we have finished creating all the pages of our website!\n\n\nCustomizing App\nThere are many ways to customize your webapp. The way we will teach is by using CSS, a style sheet language that helps with the styling of a document written in a markup language (in our case, HTML). For the purpose of this blog, we will be changing the font and the color. In our base.html, we ran a line saying &lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;, which means that it is looking for a folder caled static and in there a file named style.css, so we first need to create these two components (within the css file is where we will write our specifications).\nTo specify the font, we will use the line font-family:{font_name_1}, {font_name_2} to change from the default font (we added {font_name_1}, {font_name_2} to specify a different option in case our first font doesnt exists.\nTo Specify the text color, we say color: #{number} which tells the aplication that all texts will be set this color-associated number.\nThere are other ways to make changes to the website which can be seen by our code below:\nbody {\n    font-family: Arial, sans-serif;\n    background-color: #f0f0f0;\n}\nh1 {\n    color: #333;\n}\nul {\n    list-style-type: none;\n}\nli {\n    background-color: #fff;\n    padding: 10px;\n    margin-bottom: 5px;\n}\nOnce we have all this, we can go to our terminal, change directory to that specific folder and then use flask run to run our application and start our website. If successful, you should get your website to look something like this:\n\n\n\nPIC16B-HW3-PIC1-2.png\n\n\n\n\n\nPIC16B-HW3-PIC2-3.png\n\n\n\n\n\nPIC16B-HW3-PIC3-3.png\n\n\nNote that with each page, the URL changes with the unique tag we specified in @app.route(). For the codes and files to our website, you can go to https://github.com/Coding-Tom-1405/HW-3 for more details.\nAnd with that we are done with our website!"
  },
  {
    "objectID": "posts/HW5/index.html",
    "href": "posts/HW5/index.html",
    "title": "Homework 5: TensorFlow and Keras Modeling (Image Classification)",
    "section": "",
    "text": "In this blog post, we will go over how keras layering and tensorflow to create different models for a dataset. For our example, we will be using image classification to determine the specific type of animal the image portrays: cat or dog.\n\nLoading Packages\nTo start, we need to import and load all our needed packages.\nFirst, because we will be utilizing Keras 3 in order to work on top of our TensorFlow backend, we need to upgrade the Colab’s default version of 2.15.0 to get version 3.0.5\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 8.8 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\nWith this, we can import all the packages and establish our backend:\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import utils, datasets, layers, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\nimport tensorflow as tf\nimport random\n\nNow that we imported the packages, let’s check the version of keras\n\nkeras.__version__\n\n'3.0.5'\n\n\nNext, since we are creating Keras backends and TensorFlow models, we will incorporate GPUs (or graphics processing units) to accelerate the computation.\nYou’ll need to enable GPUs for the notebook:\n\nNavigate to Edit→Notebook Settings\nselect GPU from the Hardware Accelerator drop-down\n\nAfter, we’ll confirm that we can connect the GPU with jax:\n\nimport jax\njax.devices()\n\n[cuda(id=0)]\n\n\nLet’s check our GPU usage\n\n!nvidia-smi\n\nMon Mar 11 02:47:39 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0              25W /  70W |    105MiB / 15360MiB |      3%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n\n\n\n\nObtaining Data\nNow that we have finished loading our packages, we can start creating our datasets using tensorflow.\nSimilar to when using numpy in the past, we will split our data into groups, except this time we will have three of them: train, test, and validation. The reason we have a validation dataset is because we are training multiple models that have different combinations of hyperameters. Thus, we need a common factor that will help evaluate and compare the performance of each model through the validation set.\nFor our code, we will split it we will do 40:10:10 (remaining unused), so our code should look something like this:\n\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nNext, to ensure consistent sizing, we want to resize and establish the expected dimension of all our images. We can accomplish this by using the layers object and Resizing() function in keras as follows:\n\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\nLastly, we want to set up our data piplines for training, validation, and testing (in order to optimize efficiency of loading and preprocessing our data), which can be done as seen below:\n\n# Defining number of samples processed in each batch for training\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nOnce our batches of data are complete, let’s see what our images look like (checking to see if they are consistent, yet still clear). We can do this by creating a function that will take in our dataset and output the desired-dimensions subplot)\n\ndef ds_visual(dataset, num_rows, num_cols, n):\n\n    # Creating plot\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 7))\n\n    # Selecting a single batch\n    for i, (images, labels) in enumerate(dataset.skip(n).take(1)):\n        # Initializing counters for dog,cat appearances in plot\n        cat_count = 0\n        dog_count = 0\n\n    # Iterating through the batch of image,label pairings\n    for image, label in zip(images, labels):\n        # Selecting only the car images\n        if label == 0 and cat_count &lt; num_cols:\n            # Plotting cat image\n            axes[0, cat_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[0, cat_count].set_title('Cat')\n            axes[0, cat_count].axis(\"off\")\n            cat_count += 1\n\n        # Selecting only the car images\n        elif label == 1 and dog_count &lt; num_cols:\n            # Plotting dog image\n            axes[1, dog_count].imshow(image.numpy().astype(\"uint8\"))\n            axes[1, dog_count].set_title('Dog')\n            axes[1, dog_count].axis(\"off\")\n            dog_count += 1\n\n        # Stopping looop once all subplots filled\n        if cat_count == num_cols and dog_count == num_cols:\n            break\n\n    plt.show()\n\nWith this function, we can see what our images look like.\n\nds_visual(train_ds, 2, 3, random.randint(1,10))\n\n\n\n\n\n\n\n\nPerfect! We have exactly one row of three cats and one row of three dogs, and all of these images are of the same dimension.\nBefore moving on to our training models, let’s first check that our dataset is fair. What we mean by that is because we split the dataset without knowing the distribution of the two species, we need to see if their frequencies are relatively even. Thus, we can check the distribution with the following two codes:\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\n\ncat_count = 0\ndog_count = 0\n\nfor label in labels_iterator:\n    if label == 0:  # Cat label\n        cat_count += 1\n    elif label == 1:  # Dog label\n        dog_count += 1\n\nprint(\"Number of images with label 0 (cat):\", cat_count)\nprint(\"Number of images with label 1 (dog):\", dog_count)\n\nNumber of images with label 0 (cat): 4637\nNumber of images with label 1 (dog): 4668\n\n\nGreat! It is even.\nTo have a starting comparing condition, we should first talk about the most basic predicting model: the baseline model.\nBecause the baseline model always predicts the most occurring label (in our case, dog), then the baseline model would only have an accuracy rate of 4668/(4637+4668)%, or 50.2%. Again, this makes sense in that because our dataset is basically proportional, then the baseline ultimately has a 1 in 2 chance of predicting the right label, dog or cat.\nWith that understanding and starting place, let’s jump into creating the models!\n\n\nFirst Model (Basic Model Structure)\nFor our first one, we will start of by introducing the most basic component when building a model.\nTo start off, we need to specify the expected shape of our data, which will require us to put layers.Input() at the beginning of our model.\nNext, with all of our models, we want to extract “features” (meaningful properties) from each images. We can do this by using Conv2D() which adds 2D convolutional layer in our neural network models to magnify the any specific textures or patterns to help with classification.\nWith a component helping magnify features, we also want a component that will help remove irrelavant data. This is done through MaxPooling2D which retains the max-values pixels in different regions while reducing the spatial dimensions of the image (Typically models will have this and Conv2D as alternating layers).\nAfter finishing our “2D” components, we now need to Flatten our data to 1D so that we can pass it through a Dense layer which will make the prediction for us. In addition, to prevent overfitting, we will also want to incorporate a Dropout layer (turning certain amount of input units to 0) at times\nWith all this info, our model should look something like this:\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10), # number of classes\n    layers.Dropout(0.5)\n])\n\nThe main thing we experimented with is 1) dropout value and 2) and maxpooling value as both of these have to do with reducing spatial dimension which impacts how much important feature remains.\nTo further understand what our model is doing, let’s take a look at what is happening to our image data at each step.\n\nmodel1.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ conv2d (Conv2D)                      │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d (MaxPooling2D)         │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_1 (Conv2D)                    │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_1 (MaxPooling2D)       │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_2 (Conv2D)                    │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten (Flatten)                    │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout (Dropout)                    │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\nAnd with that, let’s start configuring the training process for our neural network model. To do this, we need to specify what optimization algorithm, loss function (how well model’s predictions match the true labels), and evaluation metric during training. Thus, our code will look something like this:\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nLastly, all that is left to do is to train it. To ensure consistency and/or improvement of our model, we will use the epoch parameter to indicate how many times our model will go through the entire training dataset. Thus, our code will be as follows:\n\nhistory = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 20s 90ms/step - accuracy: 0.4883 - loss: 23.6040 - val_accuracy: 0.5537 - val_loss: 0.9793\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 33ms/step - accuracy: 0.5134 - loss: 1.4983 - val_accuracy: 0.5666 - val_loss: 0.7251\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 6s 38ms/step - accuracy: 0.5507 - loss: 1.3817 - val_accuracy: 0.5997 - val_loss: 0.7705\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5766 - loss: 1.3752 - val_accuracy: 0.6028 - val_loss: 0.7648\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.5961 - loss: 1.3353 - val_accuracy: 0.5993 - val_loss: 0.7467\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6193 - loss: 1.2841 - val_accuracy: 0.5903 - val_loss: 0.9021\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.6491 - loss: 1.2248 - val_accuracy: 0.6165 - val_loss: 0.8251\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.6811 - loss: 1.1777 - val_accuracy: 0.6092 - val_loss: 0.8820\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7094 - loss: 1.1061 - val_accuracy: 0.6002 - val_loss: 1.0495\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7332 - loss: 1.0746 - val_accuracy: 0.6079 - val_loss: 1.0947\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7657 - loss: 1.0373 - val_accuracy: 0.6071 - val_loss: 1.0857\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.7786 - loss: 1.0003 - val_accuracy: 0.5959 - val_loss: 1.2514\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.7966 - loss: 0.9854 - val_accuracy: 0.6066 - val_loss: 1.3315\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8142 - loss: 0.9666 - val_accuracy: 0.6109 - val_loss: 1.4496\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8063 - loss: 0.9892 - val_accuracy: 0.6032 - val_loss: 1.4400\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8015 - loss: 1.0174 - val_accuracy: 0.6028 - val_loss: 1.7445\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8122 - loss: 0.9848 - val_accuracy: 0.5980 - val_loss: 1.5714\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8057 - loss: 0.9930 - val_accuracy: 0.5954 - val_loss: 1.8500\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 37ms/step - accuracy: 0.8095 - loss: 0.9867 - val_accuracy: 0.5989 - val_loss: 2.4330\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8218 - loss: 0.9689 - val_accuracy: 0.6088 - val_loss: 2.3539\n\n\nThere are two things that we can note from looking at this statistics. First, we can see that the accuracy of my model stabilized between 55.37% and 61.09% during training, meaning our model is 5.17-10.89% better at predicting the label. However, you will notice that there are two different accuracy values in each epoch. One of them is for the validation dataset while the other one accuracy is for the training dataset. Let’s graph to compare what these accuracies look like agains each other\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nFrom this graph, we can see that the the gap between the two variables are getting increasingly larger, with training accuracy values is being greater than the validation accuracy as we go through more iterations. This indicates an overfitting in our model as it is relying heavily on the features in the training dataset, resulting an underperformance in our validation dataset.\n\n\nModel with Data Augment\nWith the basic model structure out of the way, we can now add more specific components to enhance our model’s performance, starting with data augmentation.\nData augmentation refers to the practice of including modified copies of the same image in the training set, meaning that a dog image will always remain to be a dog image even if we decide to flip or rotate it.By adding the transformed “version” of the images, our model can learn invariant features of our data.\nTo achieve this step, all we need to add are the following to layers in our model: layers.RandomFlip() and layers.RandomRotation(). To see these layers in action, we will plot our original image along with a few copies where the layers were applied.\nFirst, the RandomFlip():\n\ndata_augmentation_random_flip = tf.keras.Sequential([\n  layers.RandomFlip(\"horizontal_and_vertical\")\n])\n\nfor images, labels in train_ds.take(1):\n    # Taking the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# Adding the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plotting the original, unflipped image\nplt.subplot(1, 3, 1)\n# Accessing the first element of the batch dimension\nplt.imshow(image[0].numpy().astype(\"uint8\"))\nplt.axis(\"off\")\n\n# Plotting the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_flip(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nAs you can see, through the RandomFlip layer, we were able to flip our dog image both horizontally and/or vertically.\nNext, the RandomRotation():\n\ndata_augmentation_random_rotate = tf.keras.Sequential([\n  layers.RandomRotation(0.2) # 0.2 specifies max rotation angle\n])\n\nfor images, labels in train_ds.take(1):\n    # Assuming you want the first image from the batch\n    image = images[0]\n    label = labels[0]\n\n# # Add the image to a batch.\nimage = tf.cast(tf.expand_dims(image, 0), tf.float32)\n\nplt.figure(figsize=(10, 10))\n\n# Plot the original, unflipped image\nplt.subplot(1, 3, 1)\nplt.imshow(image[0].numpy().astype(\"uint8\"))  # Accessing the first element of the batch dimension\nplt.axis(\"off\")\n\n# Plot the flipped versions\nfor i in range(2):\n    augmented_image = data_augmentation_random_rotate(image)\n    ax = plt.subplot(1, 3, i + 2)\n    plt.imshow(augmented_image[0].astype(\"uint8\"))\n    plt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\nFor this, we can see how our dog has been rotated at various angles.\nWith these two new layers, we can add on to our starting model. Since we want our model to first be able to identify whether or not the image has been modulated, we want to make these two augmentation layers the first ones in our models, making our code look like this:\n\nmodel2 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),  # Flipping augmentation\n    layers.RandomRotation(0.2),  # Random rotation augmentation\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10),\n    layers.Dropout(0.5)\n])\n\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/preprocessing/tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n\nAgain, let’s take a look at our summary and start training.\n\nmodel2.summary()\n\nModel: \"sequential_29\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_23 (RandomFlip)          │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_19 (RandomRotation)  │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_63 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_42 (MaxPooling2D)      │ (None, 74, 74, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_64 (Conv2D)                   │ (None, 72, 72, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_43 (MaxPooling2D)      │ (None, 36, 36, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_65 (Conv2D)                   │ (None, 34, 34, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_21 (Flatten)                 │ (None, 73984)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_42 (Dense)                     │ (None, 64)                  │       4,735,040 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_43 (Dense)                     │ (None, 10)                  │             650 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_5 (Dropout)                  │ (None, 10)                  │               0 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 4,764,330 (18.17 MB)\n\n\n\n Trainable params: 4,764,330 (18.17 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 95s 526ms/step - accuracy: 0.4902 - loss: 21.2286 - val_accuracy: 0.6144 - val_loss: 0.6761\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5317 - loss: 1.4244 - val_accuracy: 0.6432 - val_loss: 0.6785\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 32s 218ms/step - accuracy: 0.5418 - loss: 1.4079 - val_accuracy: 0.6612 - val_loss: 0.6301\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 292ms/step - accuracy: 0.5442 - loss: 1.4034 - val_accuracy: 0.6797 - val_loss: 0.6092\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5717 - loss: 1.3642 - val_accuracy: 0.6922 - val_loss: 0.6192\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5531 - loss: 1.3825 - val_accuracy: 0.6823 - val_loss: 0.6084\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5540 - loss: 1.3716 - val_accuracy: 0.6660 - val_loss: 0.6346\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5635 - loss: 1.3650 - val_accuracy: 0.6930 - val_loss: 0.6010\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 206ms/step - accuracy: 0.5497 - loss: 1.3600 - val_accuracy: 0.7072 - val_loss: 0.5903\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5622 - loss: 1.3492 - val_accuracy: 0.6763 - val_loss: 0.6072\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 289ms/step - accuracy: 0.5645 - loss: 1.3489 - val_accuracy: 0.6870 - val_loss: 0.5993\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5565 - loss: 1.3696 - val_accuracy: 0.6999 - val_loss: 0.6130\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5767 - loss: 1.3488 - val_accuracy: 0.6892 - val_loss: 0.5941\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 30s 209ms/step - accuracy: 0.5696 - loss: 1.3566 - val_accuracy: 0.6926 - val_loss: 0.6353\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 210ms/step - accuracy: 0.5674 - loss: 1.3447 - val_accuracy: 0.7283 - val_loss: 0.5621\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 42s 287ms/step - accuracy: 0.5713 - loss: 1.3509 - val_accuracy: 0.7356 - val_loss: 0.5911\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5638 - loss: 1.3601 - val_accuracy: 0.7210 - val_loss: 0.5733\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5623 - loss: 1.3656 - val_accuracy: 0.7270 - val_loss: 0.5433\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 211ms/step - accuracy: 0.5904 - loss: 1.3121 - val_accuracy: 0.7395 - val_loss: 0.5454\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 31s 212ms/step - accuracy: 0.5647 - loss: 1.3410 - val_accuracy: 0.7399 - val_loss: 0.5652\n\n\nWow! The accuracy of my model stabilized between 61.44% and 73.99% during training, meaning it was 6.07-18.62% better at predicting the label than our model1 was. And, if we take a look at the chart comparing the training and validation accuracy,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nwe can see that the training accuracy was never higher than the validation accuracy, meaning there were no immediate observations of overfitting happening here, unlike model1.\n\n\nData Processing\nThe third model that we will create will continue building off of the previous models. Now, we will introduce the concept of data processing\nSometimes, itis helpful to make simple transformations to the input data to make training easier for neural network models. For example, in this case, the original data has pixels with RGB values between 0 and 255, but many models will train faster with RGB values normalized between 0/-1 and 1 where we scale the weights. However, rather than scaling during training, we can do it prior which will allow the training time/energy more focused on the data content itself.\nThus, we can do that by writing the following block of code:\n\n# Define the preprocessing layer\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs=x)\n\nWith this preprocessor layer, we can slot it into our model pipeline, getting the following code:\n\n# Define the rest of the model architecture\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(2),\n])\n\nNow, let’s get our summary and train the model.\n\nmodel3.summary()\n\nModel: \"sequential_4\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ functional_13 (Functional)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_flip_4 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_4 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_12 (Conv2D)                   │ (None, 148, 148, 32)        │             896 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_8 (MaxPooling2D)       │ (None, 49, 49, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_13 (Conv2D)                   │ (None, 47, 47, 32)          │           9,248 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ max_pooling2d_9 (MaxPooling2D)       │ (None, 15, 15, 32)          │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ conv2d_14 (Conv2D)                   │ (None, 13, 13, 64)          │          18,496 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ flatten_4 (Flatten)                  │ (None, 10816)               │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_8 (Dense)                      │ (None, 64)                  │         692,288 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_4 (Dropout)                  │ (None, 64)                  │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_9 (Dense)                      │ (None, 2)                   │             130 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,163,176 (8.25 MB)\n\n\n\n Trainable params: 721,058 (2.75 MB)\n\n\n\n Non-trainable params: 0 (0.00 B)\n\n\n\n Optimizer params: 1,442,118 (5.50 MB)\n\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model3.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.7971 - loss: 0.4250 - val_accuracy: 0.7932 - val_loss: 0.4449\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8115 - loss: 0.4147 - val_accuracy: 0.8001 - val_loss: 0.4289\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8085 - loss: 0.4122 - val_accuracy: 0.8027 - val_loss: 0.4295\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8054 - loss: 0.4186 - val_accuracy: 0.8035 - val_loss: 0.4274\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8019 - loss: 0.4157 - val_accuracy: 0.7975 - val_loss: 0.4353\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8083 - loss: 0.4125 - val_accuracy: 0.8057 - val_loss: 0.4303\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 30ms/step - accuracy: 0.8173 - loss: 0.4012 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8148 - loss: 0.3978 - val_accuracy: 0.8044 - val_loss: 0.4160\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8191 - loss: 0.3924 - val_accuracy: 0.7966 - val_loss: 0.4294\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8214 - loss: 0.3920 - val_accuracy: 0.8113 - val_loss: 0.4249\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8259 - loss: 0.3842 - val_accuracy: 0.8078 - val_loss: 0.4094\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8268 - loss: 0.3803 - val_accuracy: 0.8151 - val_loss: 0.4329\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8278 - loss: 0.3854 - val_accuracy: 0.8199 - val_loss: 0.4099\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 27ms/step - accuracy: 0.8346 - loss: 0.3733 - val_accuracy: 0.8108 - val_loss: 0.4125\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8439 - loss: 0.3667 - val_accuracy: 0.8203 - val_loss: 0.3971\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 29ms/step - accuracy: 0.8353 - loss: 0.3675 - val_accuracy: 0.8246 - val_loss: 0.4054\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8362 - loss: 0.3706 - val_accuracy: 0.8250 - val_loss: 0.3923\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8477 - loss: 0.3479 - val_accuracy: 0.8177 - val_loss: 0.4211\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 27ms/step - accuracy: 0.8384 - loss: 0.3572 - val_accuracy: 0.8250 - val_loss: 0.3840\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 5s 28ms/step - accuracy: 0.8438 - loss: 0.3590 - val_accuracy: 0.8186 - val_loss: 0.3964\n\n\nThis one performed even better! The accuracy of my model stabilized between 79.32% an 82.50% during training, meaning it was better than model1 by 23.95-27.13%.\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nHowever, like model2 our training accuracy values are consistently higher than the validation accuracy values, indicating signs of overfitting for model3.\n\n\nTransfer Learning\nFor our last model, we will be focusing on tranfer learning. What does that mean?\nSo far, we’ve been training models for distinguishing between cats and dogs from scratch. In some cases, however, someone might already have trained a model that does a related task, and might have learned some relevant patterns. For example, folks train machine learning models for a variety of image recognition tasks. Thus, we can try to use a pre-existing model for our task.\nTo do this, we need to first access a pre-existing “base model” which we will then incorporate in a full model to train for our specific dataset. Thus, we will first use the following code to download MobileNetV2Large and then configure as a layer to be slotted in our model, similar to preprocessor:\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\nFrom this “base model,” all we have to do is add in our audmentation layer as well as a Dense() layer for prediction.\n\nmodel4 = models.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\", input_shape=(150, 150, 3)),\n    layers.RandomRotation(0.2),\n    base_model_layer,\n    layers.GlobalMaxPool2D(),\n    layers.Dense(2)\n])\n\nFor one last time, let’s take a look at our summary and then start training.\n\nmodel4.summary()\n\nModel: \"sequential\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip (RandomFlip)             │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation (RandomRotation)     │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_1 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d                 │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (Dense)                        │ (None, 2)                   │           1,922 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,998,274 (11.44 MB)\n\n\n\n Trainable params: 1,922 (7.51 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nTwo things to note from this summary. The first is that between the base_model_layer, denoted by functional_1, and Dense() layer, there is a major jump in output shape, which is why we added a GlobalMaxPooling2D layer to help the transformation from 2D to 1D (needed for dense layer). The second thing is that compared to the previous models which had hundreds of thousands and even millions of training parameters, this model only has close to 2,000 traininable parameters, making this more efficient.\nWith that, let’s start the training.\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 29s 134ms/step - accuracy: 0.6900 - loss: 3.2898 - val_accuracy: 0.9286 - val_loss: 0.4906\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.8854 - loss: 0.7964 - val_accuracy: 0.9471 - val_loss: 0.3098\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9019 - loss: 0.6175 - val_accuracy: 0.9480 - val_loss: 0.3026\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 51ms/step - accuracy: 0.9022 - loss: 0.5572 - val_accuracy: 0.9527 - val_loss: 0.2610\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9112 - loss: 0.4793 - val_accuracy: 0.9514 - val_loss: 0.2471\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9152 - loss: 0.4250 - val_accuracy: 0.9557 - val_loss: 0.2024\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9059 - loss: 0.4484 - val_accuracy: 0.9587 - val_loss: 0.1757\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 51ms/step - accuracy: 0.9163 - loss: 0.3734 - val_accuracy: 0.9493 - val_loss: 0.2260\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9233 - loss: 0.3127 - val_accuracy: 0.9617 - val_loss: 0.1646\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9254 - loss: 0.2912 - val_accuracy: 0.9488 - val_loss: 0.2356\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 59ms/step - accuracy: 0.9222 - loss: 0.3179 - val_accuracy: 0.9480 - val_loss: 0.2038\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 60ms/step - accuracy: 0.9171 - loss: 0.3419 - val_accuracy: 0.9583 - val_loss: 0.1724\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9225 - loss: 0.2911 - val_accuracy: 0.9626 - val_loss: 0.1437\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 54ms/step - accuracy: 0.9256 - loss: 0.2803 - val_accuracy: 0.9531 - val_loss: 0.1747\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9210 - loss: 0.3063 - val_accuracy: 0.9609 - val_loss: 0.1632\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9290 - loss: 0.2569 - val_accuracy: 0.9536 - val_loss: 0.1656\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 53ms/step - accuracy: 0.9184 - loss: 0.2861 - val_accuracy: 0.9557 - val_loss: 0.1811\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9259 - loss: 0.2598 - val_accuracy: 0.9381 - val_loss: 0.2751\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 8s 52ms/step - accuracy: 0.9207 - loss: 0.3125 - val_accuracy: 0.9463 - val_loss: 0.2146\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 10s 52ms/step - accuracy: 0.9286 - loss: 0.2523 - val_accuracy: 0.9600 - val_loss: 0.1502\n\n\nThis one did the best, constantly hitting over 90’s! The accuracy of my model stabilized between 92.86% and 96.26% during training, meaning our model is 37.29-41.28% better at predicting than model1. Comparing the accuracy values,\n\nplt.plot(history.history[\"accuracy\"], label = \"training\")\nplt.plot(history.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\nplt.legend()\n\n\n\n\n\n\n\n\nvalidation again is always higher than training, indicating no immediae signs of overfitting for model4.\n\n\nTrying on Test Data\nAgain, out of all of these, it seems that model4 was the most performant model, given it has only 90’s values for validation accuracy and there are no indicators of overfitting.\nNow that we have our best model figured out, we will now run our model against unseen test data.\n\nmodel4.evaluate(test_ds, verbose = 1)\n\n37/37 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.9487 - loss: 0.1970\n\n\n[0.21008619666099548, 0.9471195340156555]\n\n\nSeems like the model did well! It got about 94.8% accuracy, which falls in the range of the validation accuracy for model4 when training it.\nAnd with that we are done!"
  }
]